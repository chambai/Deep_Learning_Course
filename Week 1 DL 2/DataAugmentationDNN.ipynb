{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAugmentationDNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnzl6hh1u/M2ipuaG9eNTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%202/DataAugmentationDNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqjWYb8Gm_qX"
      },
      "source": [
        "#Data Augmentation with DNN Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b_sHIg-hmbQ"
      },
      "source": [
        "##Get standard MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fx336_OhBt6"
      },
      "source": [
        "# load the mnist dataset from keras\r\n",
        "import keras\r\n",
        "from keras import datasets\r\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\r\n",
        "\r\n",
        "# Normalize pixel values between 0 and 1\r\n",
        "x_train = x_train.astype('float32') / 255.0\r\n",
        "x_test = x_test.astype('float32') / 255.0\r\n",
        "\r\n",
        "# One hot encode the output data\r\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\r\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juKvSmMMhxvo"
      },
      "source": [
        "## Create, compile and fit model with original MNIST data only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7djlySPcKO2u"
      },
      "source": [
        "# Create, compile and fit the model with the original MNIST data\r\n",
        "from keras import layers\r\n",
        "from keras import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "model = keras.Sequential()\r\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28)))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, epochs = 20, validation_data = (x_test, y_test))\r\n",
        "\r\n",
        "# plot the history of the training\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLKKSL3LfXBB"
      },
      "source": [
        "##Augment the MNIST dataset with rotated, zoomed, width and height shifts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLAcxRkJGEo8"
      },
      "source": [
        "# use the image data generator from keras to augment the data\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "# setup the generator that will augment the images\r\n",
        "train_gen = ImageDataGenerator(rotation_range=8,   # rotate\r\n",
        "                               width_shift_range=0.08,  # width shiift\r\n",
        "                               shear_range=0.3,   # shear\r\n",
        "                               height_shift_range=0.08, # height shift\r\n",
        "                               zoom_range=0.08 )   # zoom\r\n",
        "\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "# Generate aungemnted images for the training data\r\n",
        "x_train = np.reshape(x_train, (60000,28,28,1))\r\n",
        "training_set = train_gen.flow(x_train, y_train, batch_size=x_train.shape[0])\r\n",
        "\r\n",
        "# Generate aungemnted images for the test data\r\n",
        "x_test = np.reshape(x_test, (10000,28,28,1))\r\n",
        "test_set= train_gen.flow(x_test, y_test, batch_size=x_test.shape[0])\r\n",
        "\r\n",
        "# extract the augmented images from the output flow of the generator\r\n",
        "# add the augmented images generated from the training data to the original training data\r\n",
        "i = 0\r\n",
        "for x, y in training_set:\r\n",
        "  if i < 2:\r\n",
        "    print(i)\r\n",
        "    x_train = np.concatenate((x_train, x))\r\n",
        "    y_train = np.concatenate((y_train, y))\r\n",
        "    print(x_train.shape)\r\n",
        "    print(y_train.shape)\r\n",
        "  else:\r\n",
        "    break\r\n",
        "  i += 1\r\n",
        "\r\n",
        "# add the augmented images generated from the test data to the original test data\r\n",
        "i = 0\r\n",
        "for x, y in test_set:\r\n",
        "  if i < 2:\r\n",
        "    print(i)\r\n",
        "    x_test_aug = np.concatenate((x_test, x))\r\n",
        "    y_test_aug = np.concatenate((y_test, y))\r\n",
        "    print(x_test.shape)\r\n",
        "    print(y_test.shape)\r\n",
        "  else:\r\n",
        "    break\r\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi1sdqrFiOEy"
      },
      "source": [
        "##Create, compile and fit the model with the augmented MNIST data\r\n",
        "(this is the same code as used above to create, compile and fit the model)\r\n",
        "\r\n",
        "Compare this accuracy/loss with the accuracy/loss achieved with the original MNIST dataset above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbDx0kNzg0tq"
      },
      "source": [
        "# same as code above to create, compile and fit the model but now using the new augmented data\r\n",
        "from keras import layers\r\n",
        "from keras import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "model = keras.Sequential()\r\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28)))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, epochs = 20, validation_data = (x_test, y_test))\r\n",
        "\r\n",
        "# plot the history of the training\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm_xVOZ2_jQ9"
      },
      "source": [
        "The validation accuracy may be roughly the same but it was overfitting before and now the validation accuracy is much better than the training accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POzu6pzf-O38"
      },
      "source": [
        "## Investigate the Keras `ImageDataGenerator` and see what other augmentation you could apply"
      ]
    }
  ]
}