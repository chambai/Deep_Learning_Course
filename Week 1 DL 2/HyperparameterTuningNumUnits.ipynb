{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperparameterTuningNumUnits.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxN4ItILVs4M+R4bt+vGbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%202/HyperparameterTuningNumUnits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bENOuzpTPZ"
      },
      "source": [
        "# Keras Tuner on MNIST - Tuning the number of units in two layers\r\n",
        "In this tutorial, you will use the Keras Tuner to perform hypertuning for the MNIST handwritten digit image dataset with a feed-forward neural network.\r\n",
        "It will try the number of units from 32 to 512 in the first hidden layer and try\r\n",
        "the number of units in the second layer from 64 to 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aH_56b1o6Jn"
      },
      "source": [
        "# install the Keras-tuner library\r\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zA_6vjOpeKa"
      },
      "source": [
        "# import the library and refer to it as kt\r\n",
        "import kerastuner as kt\r\n",
        "# load the mnist dataset from keras\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets\r\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\r\n",
        "\r\n",
        "# Normalize pixel values between 0 and 1\r\n",
        "x_train = x_train.astype('float32') / 255.0\r\n",
        "x_test = x_test.astype('float32') / 255.0\r\n",
        "\r\n",
        "# apply one-hot-encoding to the output data\r\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\r\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9biK42RbrRQv"
      },
      "source": [
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Flatten\r\n",
        "\r\n",
        "# build the model\r\n",
        "def model_builder(hp):\r\n",
        "  model = tf.keras.Sequential()\r\n",
        "  # specify the input layer separately to the first layer\r\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\r\n",
        "\r\n",
        "  # Tune the number of units in the first Dense layer\r\n",
        "  # Specify a change in the number of units in this layer \r\n",
        "  # from 32 to 512 in steps of 32\r\n",
        "  # We have specified the name of this hyperparameter as 'hidden_layer_1_units'\r\n",
        "  # this name will appear in the output text when running tuner.search function later on\r\n",
        "  hp_units = hp.Int('hidden_layer_1_units', min_value = 32, max_value = 512, step = 32)\r\n",
        "  # specify the range of units (hp_units) as the units input parameter to this layer of the model\r\n",
        "  model.add(Dense(units=hp_units, activation='relu'))\r\n",
        "\r\n",
        "  hp_units = hp.Int('hidden_layer_2_units', min_value = 64, max_value = 128, step = 32)\r\n",
        "  # specify the range of units (hp_units) as the units input parameter to this layer of the model\r\n",
        "  model.add(Dense(units=hp_units, activation='relu'))\r\n",
        "\r\n",
        "  # Specify the output layer\r\n",
        "  model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "  # set the optimiser in the compile method\r\n",
        "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnG505-D6mY"
      },
      "source": [
        "Setup the tuner by calling `kt.Hyperband`  and specifying the model_builder function as the first parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu89Y7DAreLO"
      },
      "source": [
        "# instatiate the tuner and perform hypertuning\r\n",
        "tuner = kt.Hyperband(model_builder,\r\n",
        "                     objective = 'val_accuracy', \r\n",
        "                     max_epochs = 10,\r\n",
        "                     factor = 3,            # factor is a number that determines how many models are created to run in parallel whaen testing the hyperparameters\r\n",
        "                     directory = 'my_dir', # directory that the tuned hyperparameter results are stored in (change the name of this directory if you get the message INFO:tensorflow:Oracle triggered exit)\r\n",
        "                     project_name = 'intro_to_kt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707Db3DGEVkg"
      },
      "source": [
        "A Callback is defined to clear the screen during the search for the hyperparameters.  A callback is just a way for the tuner to signal that it has reached the end of training for each hyperparameter setup.  The callback code below then clears the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LXCKGX1rpd0"
      },
      "source": [
        "# define a callback to clear the training outputs at the end of every training step\r\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\r\n",
        "  def on_train_end(*args, **kwargs):\r\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H2o3qRTE_KN"
      },
      "source": [
        "The hyperparameter search is performed using `tuner.search`.  The arguments are the same as for the fit method for the model.\r\n",
        "When the search is complete, it returns the best hyperparameters.\r\n",
        "\r\n",
        "**Note:** if you do not get a display of updating training epochs when running the next cell and you only see this message:   **INFO:tensorflow:Oracle triggered exit**, the keras tuner has not worked correctly.  To fix this, \r\n",
        "update `directory='my_dir'` (which is two code cells above) to have a different directory name. Keras stores the tuning results in this directory and can sometimes error if you are changing the model alot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9iMc7vLr2U0"
      },
      "source": [
        "import IPython\r\n",
        "# run the hyperparameter search\r\n",
        "tuner.search(x_train, y_train, epochs = 10, validation_data = (x_test, y_test), callbacks = [ClearTrainingOutput()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_r5a1zcvaKc"
      },
      "source": [
        "# print out the hyperparameters keras tuner has determined to be the best values\r\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\r\n",
        "print(best_hps.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxtnqhdeFiCR"
      },
      "source": [
        "Now we can train the model with the optimum parameters found in the previous step which are stored in `best_hps`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdeQqtsitpzZ"
      },
      "source": [
        "# set the model to have the optimum hyperparameters and print the summary so we can see the \r\n",
        "model = tuner.hypermodel.build(best_hps)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQvvjSrVsIi3"
      },
      "source": [
        "# train the model as usual\r\n",
        "history = model.fit(x_train, y_train, epochs = 10, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-x9IjHKF4o6"
      },
      "source": [
        "Plot the accuracy and loss of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWez9rhbxMPT"
      },
      "source": [
        "# plot the history of the training\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}