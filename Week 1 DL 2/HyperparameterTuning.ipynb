{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperparameterTuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPacsuDebjs4YN0lPVenDJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%202/HyperparameterTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bENOuzpTPZ"
      },
      "source": [
        "Adapted from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\r\n",
        "## Overview\r\n",
        "\r\n",
        "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*. \r\n",
        "\r\n",
        "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\r\n",
        "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\r\n",
        "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for i.e. the Adam optimiser.\r\n",
        "\r\n",
        "In this tutorial, you will use the Keras Tuner to perform hypertuning for the MNIST handwritten digit image dataset with a feed-forward neural network.\r\n",
        "It will try the number of units from 32 to 512 in the first hidden layer and try\r\n",
        "learning rates of 0.01, 0.001, and 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aH_56b1o6Jn"
      },
      "source": [
        "# install the Keras-tuner library\r\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWGWeVbTpGYw"
      },
      "source": [
        "# import the library and refer to it as kt\r\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNrhNX6vNQ4I"
      },
      "source": [
        "MNIST dataset of handwritten digits has images of digits from 0 to 9, has a training set of 60000 and a test set of 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zA_6vjOpeKa"
      },
      "source": [
        "# load the mnist dataset from keras\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets\r\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vza_GVXQRng"
      },
      "source": [
        "Investigate the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBOC27p9QQT6"
      },
      "source": [
        "print(x_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "print(x_test.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgKtjvwOA5fV"
      },
      "source": [
        "Each pixel of the picture is represented as a number between 0 and 255. Therefore we can normalize this data by dividing by 255 to ensure all input values to the deep neural network will be between 0 and 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9_-o83mppXi"
      },
      "source": [
        "# Normalize pixel values between 0 and 1\r\n",
        "x_train = x_train.astype('float32') / 255.0\r\n",
        "x_test = x_test.astype('float32') / 255.0\r\n",
        "\r\n",
        "# apply one-hot-encoding to the output data\r\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\r\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dy_oKhJBgd0"
      },
      "source": [
        "The model is built the same as we have done so far, using keras.Sequential().  The only difference is that this code is now within the function `def model_builder(hp)`.\r\n",
        "\r\n",
        "There is an extra step of `hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)` specifying the range by which the numner of units in the first layer will be changed by.\r\n",
        "\r\n",
        "There is an extra step of `hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])` specifying what the learning rate of the optimiser is to be varied by.  This is then passed to the optimiser as the learning_rate parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9biK42RbrRQv"
      },
      "source": [
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Flatten\r\n",
        "\r\n",
        "# build the model\r\n",
        "def model_builder(hp):\r\n",
        "  model = tf.keras.Sequential()\r\n",
        "  # specify the input layer separately to the first layer\r\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\r\n",
        "\r\n",
        "  # Tune the number of units in the first Dense layer\r\n",
        "  # Specify a change in the number of units in this layer \r\n",
        "  # from 32 to 512 in steps of 32\r\n",
        "  # We have specified the name of this hyperparameter as 'hidden_layer_1_units'\r\n",
        "  # this name will appear in the output text when running tuner.search function later on\r\n",
        "  hp_units = hp.Int('hidden_layer_1_units', min_value = 32, max_value = 512, step = 32)\r\n",
        "  # specify the range of units (hp_units) as the units input parameter to this layer of the model\r\n",
        "  model.add(Dense(units=hp_units, activation='relu'))\r\n",
        "\r\n",
        "  # Specify the output layer\r\n",
        "  model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "  # Tune the learning rate for the optimizer \r\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\r\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \r\n",
        "  # create the Adam optimiser, specifying the variable learning rate\r\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate)\r\n",
        "  # set the optimiser in the compile method\r\n",
        "  model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnG505-D6mY"
      },
      "source": [
        "Setup the tuner by calling `kt.Hyperband`  and specifying the model_builder function as the first parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ls7LEAnGVdl"
      },
      "source": [
        "The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket using the `factor` and `max_epochs` parameters.  A bracket is a tree diagram that represents the series of games played during a sports knockout tournament)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu89Y7DAreLO"
      },
      "source": [
        "# instatiate the tuner and perform hypertuning\r\n",
        "tuner = kt.Hyperband(model_builder,\r\n",
        "                     objective = 'val_accuracy', \r\n",
        "                     max_epochs = 10,\r\n",
        "                     factor = 3,            # factor is a number that determines how many models are created to run in parallel whaen testing the hyperparameters\r\n",
        "                     directory = 'my_dir', # directory that the tuned hyperparameter results are stored in (change the name of this directory if you get the message INFO:tensorflow:Oracle triggered exit)\r\n",
        "                     project_name = 'intro_to_kt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707Db3DGEVkg"
      },
      "source": [
        "A Callback is defined to clear the screen during the search for the hyperparameters.  A callback is just a way for the tuner to signal that it has reached the end of training for each hyperparameter setup.  The callback code below then clears the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LXCKGX1rpd0"
      },
      "source": [
        "# define a callback to clear the training outputs at the end of every training step\r\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\r\n",
        "  def on_train_end(*args, **kwargs):\r\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H2o3qRTE_KN"
      },
      "source": [
        "The hyperparameter search is performed using `tuner.search`.  The arguments are the same as for the fit method for the model.\r\n",
        "When the search is complete, it returns the best hyperparameters.\r\n",
        "\r\n",
        "**Note:** if you do not get a display of updating training epochs when running the next cell and you only see this message:   **INFO:tensorflow:Oracle triggered exit**, the keras tuner has not worked correctly.  To fix this, \r\n",
        "update `directory='my_dir'` (which is two code cells above) to have a different directory name. Keras stores the tuning results in this directory and can sometimes error if you are changing the model alot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9iMc7vLr2U0"
      },
      "source": [
        "import IPython\r\n",
        "# run the hyperparameter search\r\n",
        "tuner.search(x_train, y_train, epochs = 10, validation_data = (x_test, y_test), callbacks = [ClearTrainingOutput()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benStVfeGLGI"
      },
      "source": [
        "The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_r5a1zcvaKc"
      },
      "source": [
        "# print out the hyperparameters keras tuner has determined to be the best values\r\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\r\n",
        "print(best_hps.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxtnqhdeFiCR"
      },
      "source": [
        "Now we can train the model with the optimum parameters found in the previous step which are stored in `best_hps`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdeQqtsitpzZ"
      },
      "source": [
        "# set the model to have the optimum hyperparameters and print the summary so we can see the \r\n",
        "model = tuner.hypermodel.build(best_hps)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQvvjSrVsIi3"
      },
      "source": [
        "# train the model as usual\r\n",
        "history = model.fit(x_train, y_train, epochs = 10, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-x9IjHKF4o6"
      },
      "source": [
        "Plot the accuracy and loss of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWez9rhbxMPT"
      },
      "source": [
        "# plot the history of the training\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}