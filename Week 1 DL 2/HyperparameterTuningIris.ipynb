{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperparameterTuningIris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcJ4cedVuRpV8LFdfnZ8Bg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%202/HyperparameterTuningIris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWeV4Bkj4kLa"
      },
      "source": [
        "# Hyperparameter Tuning Iris\r\n",
        "\r\n",
        "Add hyperparameter tuning to the Iris DNN so that the number of units in the second layer is selected via automated hyperparameter tuning using Keras Tuner.\r\n",
        "Use the selected number of units to train the DNN and plot the accuracy/loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edV8wRk_4ihz"
      },
      "source": [
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras import utils\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# get the data\r\n",
        "data = load_iris()\r\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\r\n",
        "\r\n",
        "# Normalise the data\r\n",
        "df_norm = normalize(df)\r\n",
        "\r\n",
        "# split the data into train and test\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df_norm, data.target, random_state=0)\r\n",
        "\r\n",
        "# one-hot encode the data\r\n",
        "Y_train = utils.to_categorical(Y_train)\r\n",
        "Y_test = utils.to_categorical(Y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k131SVTwDnw9"
      },
      "source": [
        "# create the model\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Compile the model (replace the question marks)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    X_train,    # input training data\r\n",
        "    Y_train,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(X_test, Y_test))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwPTlgr05Z5G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}