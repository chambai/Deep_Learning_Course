{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperparameterTuningIris_Sln1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyME2MTMVnRxuPurmTvk7dnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%202/HyperparameterTuningIris_Sln1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWeV4Bkj4kLa"
      },
      "source": [
        "# Hyperparameter Tuning Iris\r\n",
        "\r\n",
        "Add hyperparemter tuning to the Iris DNN.\r\n",
        "Adds hyperparameter tuning to the second layer of the Iris neural network.\r\n",
        "Tries number of units from 10 to 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L_bPAXiD33G"
      },
      "source": [
        "# install the Keras-tuner library\r\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXrRNmG3DxtM"
      },
      "source": [
        "# import the library and refer to it as kt\r\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edV8wRk_4ihz"
      },
      "source": [
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import utils\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# get the data\r\n",
        "data = load_iris()\r\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\r\n",
        "\r\n",
        "# Normalise the data\r\n",
        "df_norm = normalize(df)\r\n",
        "\r\n",
        "# split the data into train and test\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_norm, data.target, random_state=0)\r\n",
        "\r\n",
        "# one-hot encode the data\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDEzyR6sEBtf"
      },
      "source": [
        "# build the model within a function\r\n",
        "# Apply hyperparmeter tuning to the number of units in the second hidden layer in the model\r\n",
        "def model_builder(hp):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(tf.keras.layers.Dense(input_dim=4, units=10, activation='relu'))\r\n",
        "  hp_units = hp.Int('units', min_value = 10, max_value = 200, step = 10)\r\n",
        "  model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\r\n",
        "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\r\n",
        "  #model.add(tf.keras.layers.Dense(units=hp.Int('units', min_value = 15, max_value = 350, step = 10), activation='softmax'))\r\n",
        "  model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rysjDFR0E7uG"
      },
      "source": [
        "# instatiate the tuner and perform hypertuning\r\n",
        "tuner = kt.Hyperband(model_builder,\r\n",
        "                     objective = 'val_accuracy', \r\n",
        "                     max_epochs = 125,\r\n",
        "                     factor = 3,        # factor is a number that determines how many models are created to run in parallel whaen testing the hyperparameters\r\n",
        "                     directory = 'my_dir',  # directory that the tuned hyperparameter results are stored in (change the name of this directory if you get the message INFO:tensorflow:Oracle triggered exit)\r\n",
        "                     project_name = 'intro_to_kt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AVJLQOJFiEz"
      },
      "source": [
        "# define a callback to clear the training outputs at the end of every training step\r\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\r\n",
        "  def on_train_end(*args, **kwargs):\r\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCVPCUQG0_Uw"
      },
      "source": [
        "The hyperparameter search is performed using `tuner.search`.  The arguments are the same as for the fit method for the model.\r\n",
        "When the search is complete, it returns the best hyperparameters.\r\n",
        "\r\n",
        "**Note:** if you do not get a display of updating training epochs when running the next cell and you only see this message:   **INFO:tensorflow:Oracle triggered exit**, the keras tuner has not worked correctly.  To fix this, \r\n",
        "update `directory='my_dir'` (which is two code cells above) to have a different directory name. Keras stores the tuning results in this directory and can sometimes error if you are changing the model alot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62HDptqx0YmR"
      },
      "source": [
        "import IPython\r\n",
        "# run the hyperparameter search\r\n",
        "tuner.search(x_train, y_train, epochs = 125, validation_data = (x_test, y_test), callbacks = [ClearTrainingOutput()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yammasbfFobR"
      },
      "source": [
        "# Get the optimal hyperparameters\r\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\r\n",
        "print(best_hps.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYSKqRDUgaFb"
      },
      "source": [
        "# apply the optimal hyperparameters to the model and print the summary\r\n",
        "model = tuner.hypermodel.build(best_hps)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUE5OcLsnMRY"
      },
      "source": [
        "# retrain the model with the optimum hyperparameters and train it on the data\r\n",
        "model = tuner.hypermodel.build(best_hps)\r\n",
        "model.summary()\r\n",
        "history = model.fit(x_train, y_train, epochs = 125, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwPTlgr05Z5G"
      },
      "source": [
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}