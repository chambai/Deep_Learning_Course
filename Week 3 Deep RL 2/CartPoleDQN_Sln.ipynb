{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpoR8UrGOHddVsWtySYgbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\r\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\r\n",
        "!pip install keras-rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0"
      },
      "source": [
        "# load the gym module\r\n",
        "import gym\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# import the usual Keras modules for creating deep neural networks\r\n",
        "from keras import Sequential\r\n",
        "from keras.layers import Input, Flatten, Dense\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "ENV_NAME = 'CartPole-v0'\r\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYR0I_3MYit1"
      },
      "source": [
        "import rl\r\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\r\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R8ZiiRbxlH2D",
        "outputId": "820f4b6f-060f-41d6-bd7d-c7e43d672e0d"
      },
      "source": [
        "import rl\r\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\r\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\r\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\r\n",
        "# memory buffer\r\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# define the policy (how we select the actions)\r\n",
        "policy = BoltzmannQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_10 (Flatten)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   22/10000: episode: 1, duration: 1.851s, episode steps:  22, steps per second:  12, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.508501, mae: 0.646089, mean_q: -0.064522\n",
            "   37/10000: episode: 2, duration: 0.113s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.498503, mae: 0.660585, mean_q: -0.007458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   58/10000: episode: 3, duration: 0.161s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.410333, mae: 0.579068, mean_q: 0.100719\n",
            "   75/10000: episode: 4, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.370710, mae: 0.547997, mean_q: 0.205263\n",
            "   89/10000: episode: 5, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.368100, mae: 0.531072, mean_q: 0.280399\n",
            "  104/10000: episode: 6, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.333233, mae: 0.513013, mean_q: 0.351245\n",
            "  123/10000: episode: 7, duration: 0.147s, episode steps:  19, steps per second: 129, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.323169, mae: 0.515359, mean_q: 0.447336\n",
            "  144/10000: episode: 8, duration: 0.167s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.316689, mae: 0.518928, mean_q: 0.560259\n",
            "  172/10000: episode: 9, duration: 0.216s, episode steps:  28, steps per second: 129, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.292529, mae: 0.527105, mean_q: 0.672699\n",
            "  194/10000: episode: 10, duration: 0.170s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.273669, mae: 0.573323, mean_q: 0.793990\n",
            "  225/10000: episode: 11, duration: 0.230s, episode steps:  31, steps per second: 135, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.266837, mae: 0.640621, mean_q: 0.927818\n",
            "  241/10000: episode: 12, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.268655, mae: 0.717526, mean_q: 1.018091\n",
            "  263/10000: episode: 13, duration: 0.166s, episode steps:  22, steps per second: 133, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.254848, mae: 0.771645, mean_q: 1.115115\n",
            "  299/10000: episode: 14, duration: 0.275s, episode steps:  36, steps per second: 131, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.246815, mae: 0.859577, mean_q: 1.242482\n",
            "  330/10000: episode: 15, duration: 0.231s, episode steps:  31, steps per second: 134, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.222842, mae: 0.955700, mean_q: 1.431129\n",
            "  347/10000: episode: 16, duration: 0.128s, episode steps:  17, steps per second: 133, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.213939, mae: 1.032858, mean_q: 1.571707\n",
            "  367/10000: episode: 17, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.216675, mae: 1.093443, mean_q: 1.683133\n",
            "  394/10000: episode: 18, duration: 0.200s, episode steps:  27, steps per second: 135, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.218635, mae: 1.172994, mean_q: 1.818554\n",
            "  435/10000: episode: 19, duration: 0.309s, episode steps:  41, steps per second: 133, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 0.188725, mae: 1.248572, mean_q: 2.028365\n",
            "  470/10000: episode: 20, duration: 0.259s, episode steps:  35, steps per second: 135, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.371 [0.000, 1.000],  loss: 0.162420, mae: 1.346350, mean_q: 2.259375\n",
            "  481/10000: episode: 21, duration: 0.085s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.207978, mae: 1.432360, mean_q: 2.436875\n",
            "  527/10000: episode: 22, duration: 0.340s, episode steps:  46, steps per second: 135, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.206582, mae: 1.532801, mean_q: 2.627635\n",
            "  542/10000: episode: 23, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.208497, mae: 1.628144, mean_q: 2.834962\n",
            "  559/10000: episode: 24, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.255792, mae: 1.702837, mean_q: 2.969635\n",
            "  578/10000: episode: 25, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.354291, mae: 1.779328, mean_q: 3.100113\n",
            "  600/10000: episode: 26, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.257831, mae: 1.807766, mean_q: 3.142645\n",
            "  617/10000: episode: 27, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.284330, mae: 1.885296, mean_q: 3.312763\n",
            "  633/10000: episode: 28, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.367602, mae: 1.965689, mean_q: 3.423780\n",
            "  654/10000: episode: 29, duration: 0.163s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.254665, mae: 1.984867, mean_q: 3.519702\n",
            "  680/10000: episode: 30, duration: 0.201s, episode steps:  26, steps per second: 130, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.252696, mae: 2.066099, mean_q: 3.731875\n",
            "  696/10000: episode: 31, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.267169, mae: 2.133030, mean_q: 3.872840\n",
            "  722/10000: episode: 32, duration: 0.195s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  loss: 0.372055, mae: 2.236763, mean_q: 4.007638\n",
            "  753/10000: episode: 33, duration: 0.231s, episode steps:  31, steps per second: 134, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.300466, mae: 2.296423, mean_q: 4.187561\n",
            "  768/10000: episode: 34, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.326161, mae: 2.396761, mean_q: 4.372255\n",
            "  784/10000: episode: 35, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.319653, mae: 2.452989, mean_q: 4.494237\n",
            "  796/10000: episode: 36, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.472457, mae: 2.514923, mean_q: 4.540391\n",
            "  823/10000: episode: 37, duration: 0.206s, episode steps:  27, steps per second: 131, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.414010, mae: 2.559191, mean_q: 4.641614\n",
            "  841/10000: episode: 38, duration: 0.135s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.499542, mae: 2.654519, mean_q: 4.780584\n",
            "  860/10000: episode: 39, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.346462, mae: 2.664108, mean_q: 4.904086\n",
            "  875/10000: episode: 40, duration: 0.116s, episode steps:  15, steps per second: 130, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.385682, mae: 2.730059, mean_q: 5.045058\n",
            "  901/10000: episode: 41, duration: 0.196s, episode steps:  26, steps per second: 133, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.456547, mae: 2.824466, mean_q: 5.204388\n",
            "  930/10000: episode: 42, duration: 0.220s, episode steps:  29, steps per second: 132, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 0.462414, mae: 2.911489, mean_q: 5.374575\n",
            "  944/10000: episode: 43, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.485186, mae: 2.984976, mean_q: 5.504426\n",
            "  997/10000: episode: 44, duration: 0.387s, episode steps:  53, steps per second: 137, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 0.555911, mae: 3.099150, mean_q: 5.728105\n",
            " 1009/10000: episode: 45, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.531837, mae: 3.204339, mean_q: 5.959504\n",
            " 1040/10000: episode: 46, duration: 0.229s, episode steps:  31, steps per second: 135, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.483678, mae: 3.262211, mean_q: 6.132030\n",
            " 1053/10000: episode: 47, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.514135, mae: 3.339942, mean_q: 6.306354\n",
            " 1073/10000: episode: 48, duration: 0.157s, episode steps:  20, steps per second: 127, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.654321, mae: 3.430485, mean_q: 6.376106\n",
            " 1088/10000: episode: 49, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.560346, mae: 3.454463, mean_q: 6.433708\n",
            " 1112/10000: episode: 50, duration: 0.184s, episode steps:  24, steps per second: 130, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.732265, mae: 3.556504, mean_q: 6.587464\n",
            " 1123/10000: episode: 51, duration: 0.084s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.609016, mae: 3.577734, mean_q: 6.654916\n",
            " 1150/10000: episode: 52, duration: 0.209s, episode steps:  27, steps per second: 129, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.695282, mae: 3.656200, mean_q: 6.793960\n",
            " 1165/10000: episode: 53, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.720437, mae: 3.721900, mean_q: 6.934325\n",
            " 1178/10000: episode: 54, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.687348, mae: 3.768436, mean_q: 7.030030\n",
            " 1216/10000: episode: 55, duration: 0.288s, episode steps:  38, steps per second: 132, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.841496, mae: 3.852995, mean_q: 7.163356\n",
            " 1247/10000: episode: 56, duration: 0.233s, episode steps:  31, steps per second: 133, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.748986, mae: 3.948507, mean_q: 7.397298\n",
            " 1268/10000: episode: 57, duration: 0.155s, episode steps:  21, steps per second: 136, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.890637, mae: 4.053576, mean_q: 7.578134\n",
            " 1321/10000: episode: 58, duration: 0.391s, episode steps:  53, steps per second: 136, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.803535, mae: 4.148571, mean_q: 7.790680\n",
            " 1337/10000: episode: 59, duration: 0.127s, episode steps:  16, steps per second: 126, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.060237, mae: 4.272782, mean_q: 7.948458\n",
            " 1381/10000: episode: 60, duration: 0.342s, episode steps:  44, steps per second: 129, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.889851, mae: 4.337294, mean_q: 8.142386\n",
            " 1398/10000: episode: 61, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.916962, mae: 4.424407, mean_q: 8.316051\n",
            " 1427/10000: episode: 62, duration: 0.216s, episode steps:  29, steps per second: 134, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 0.892792, mae: 4.508296, mean_q: 8.524322\n",
            " 1447/10000: episode: 63, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.946510, mae: 4.585598, mean_q: 8.651464\n",
            " 1468/10000: episode: 64, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.775785, mae: 4.623472, mean_q: 8.814414\n",
            " 1525/10000: episode: 65, duration: 0.413s, episode steps:  57, steps per second: 138, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.037754, mae: 4.770061, mean_q: 9.053280\n",
            " 1542/10000: episode: 66, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.578059, mae: 4.837263, mean_q: 9.310462\n",
            " 1572/10000: episode: 67, duration: 0.218s, episode steps:  30, steps per second: 138, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 1.230165, mae: 4.983823, mean_q: 9.464460\n",
            " 1596/10000: episode: 68, duration: 0.178s, episode steps:  24, steps per second: 135, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.255553, mae: 5.077672, mean_q: 9.558380\n",
            " 1610/10000: episode: 69, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.193576, mae: 5.107379, mean_q: 9.612975\n",
            " 1630/10000: episode: 70, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.216238, mae: 5.145823, mean_q: 9.782406\n",
            " 1724/10000: episode: 71, duration: 0.668s, episode steps:  94, steps per second: 141, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.241963, mae: 5.316371, mean_q: 10.108365\n",
            " 1742/10000: episode: 72, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.673020, mae: 5.523752, mean_q: 10.358516\n",
            " 1762/10000: episode: 73, duration: 0.152s, episode steps:  20, steps per second: 132, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.443633, mae: 5.558936, mean_q: 10.501528\n",
            " 1775/10000: episode: 74, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.258616, mae: 5.613019, mean_q: 10.647127\n",
            " 1789/10000: episode: 75, duration: 0.118s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.356083, mae: 5.611801, mean_q: 10.619943\n",
            " 1848/10000: episode: 76, duration: 0.434s, episode steps:  59, steps per second: 136, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.273971, mae: 5.735149, mean_q: 10.967677\n",
            " 1867/10000: episode: 77, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 1.229224, mae: 5.834498, mean_q: 11.173711\n",
            " 1897/10000: episode: 78, duration: 0.230s, episode steps:  30, steps per second: 130, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.449888, mae: 5.907081, mean_q: 11.239291\n",
            " 1916/10000: episode: 79, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.128430, mae: 6.059317, mean_q: 11.436753\n",
            " 1933/10000: episode: 80, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.617727, mae: 6.046908, mean_q: 11.466831\n",
            " 2023/10000: episode: 81, duration: 0.660s, episode steps:  90, steps per second: 136, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.683632, mae: 6.189556, mean_q: 11.821957\n",
            " 2110/10000: episode: 82, duration: 0.622s, episode steps:  87, steps per second: 140, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.446828, mae: 6.422643, mean_q: 12.344864\n",
            " 2149/10000: episode: 83, duration: 0.292s, episode steps:  39, steps per second: 134, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.691250, mae: 6.629374, mean_q: 12.772400\n",
            " 2197/10000: episode: 84, duration: 0.346s, episode steps:  48, steps per second: 139, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.556235, mae: 6.709662, mean_q: 12.932982\n",
            " 2230/10000: episode: 85, duration: 0.245s, episode steps:  33, steps per second: 135, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.781662, mae: 6.848420, mean_q: 13.265645\n",
            " 2246/10000: episode: 86, duration: 0.118s, episode steps:  16, steps per second: 135, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 2.757251, mae: 6.971078, mean_q: 13.202219\n",
            " 2258/10000: episode: 87, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.351386, mae: 6.997982, mean_q: 13.248555\n",
            " 2281/10000: episode: 88, duration: 0.174s, episode steps:  23, steps per second: 132, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.745462, mae: 6.971483, mean_q: 13.381815\n",
            " 2295/10000: episode: 89, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.355255, mae: 7.146564, mean_q: 13.682472\n",
            " 2437/10000: episode: 90, duration: 1.003s, episode steps: 142, steps per second: 142, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.267909, mae: 7.274820, mean_q: 13.992502\n",
            " 2516/10000: episode: 91, duration: 0.573s, episode steps:  79, steps per second: 138, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.902617, mae: 7.586818, mean_q: 14.753941\n",
            " 2533/10000: episode: 92, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.446306, mae: 7.765341, mean_q: 15.034230\n",
            " 2566/10000: episode: 93, duration: 0.251s, episode steps:  33, steps per second: 131, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.337986, mae: 7.829691, mean_q: 14.890620\n",
            " 2599/10000: episode: 94, duration: 0.254s, episode steps:  33, steps per second: 130, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.829916, mae: 7.846541, mean_q: 14.976490\n",
            " 2669/10000: episode: 95, duration: 0.511s, episode steps:  70, steps per second: 137, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.077409, mae: 8.007494, mean_q: 15.580368\n",
            " 2716/10000: episode: 96, duration: 0.355s, episode steps:  47, steps per second: 132, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.553516, mae: 8.164778, mean_q: 15.832682\n",
            " 2757/10000: episode: 97, duration: 0.310s, episode steps:  41, steps per second: 132, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 2.321608, mae: 8.277522, mean_q: 16.084805\n",
            " 2769/10000: episode: 98, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.964289, mae: 8.375942, mean_q: 16.355377\n",
            " 2816/10000: episode: 99, duration: 0.349s, episode steps:  47, steps per second: 135, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.038407, mae: 8.463293, mean_q: 16.345947\n",
            " 2855/10000: episode: 100, duration: 0.292s, episode steps:  39, steps per second: 134, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.588390, mae: 8.594885, mean_q: 16.700226\n",
            " 2902/10000: episode: 101, duration: 0.350s, episode steps:  47, steps per second: 134, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.862739, mae: 8.635313, mean_q: 16.730455\n",
            " 2939/10000: episode: 102, duration: 0.275s, episode steps:  37, steps per second: 134, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 3.366046, mae: 8.810304, mean_q: 16.987186\n",
            " 2968/10000: episode: 103, duration: 0.235s, episode steps:  29, steps per second: 123, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.678881, mae: 8.825837, mean_q: 17.101517\n",
            " 3015/10000: episode: 104, duration: 0.343s, episode steps:  47, steps per second: 137, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.354018, mae: 8.986509, mean_q: 17.336292\n",
            " 3037/10000: episode: 105, duration: 0.162s, episode steps:  22, steps per second: 136, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.601503, mae: 9.052247, mean_q: 17.349848\n",
            " 3052/10000: episode: 106, duration: 0.113s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.450672, mae: 9.055109, mean_q: 17.575483\n",
            " 3074/10000: episode: 107, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.894560, mae: 9.077043, mean_q: 17.556076\n",
            " 3114/10000: episode: 108, duration: 0.304s, episode steps:  40, steps per second: 132, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.112125, mae: 9.101542, mean_q: 17.793436\n",
            " 3180/10000: episode: 109, duration: 0.482s, episode steps:  66, steps per second: 137, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.761160, mae: 9.316064, mean_q: 17.959332\n",
            " 3216/10000: episode: 110, duration: 0.264s, episode steps:  36, steps per second: 136, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.636283, mae: 9.388240, mean_q: 18.287218\n",
            " 3266/10000: episode: 111, duration: 0.377s, episode steps:  50, steps per second: 133, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.650505, mae: 9.532986, mean_q: 18.429630\n",
            " 3296/10000: episode: 112, duration: 0.226s, episode steps:  30, steps per second: 133, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.934138, mae: 9.577358, mean_q: 18.574123\n",
            " 3326/10000: episode: 113, duration: 0.223s, episode steps:  30, steps per second: 134, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.870339, mae: 9.709375, mean_q: 18.757788\n",
            " 3375/10000: episode: 114, duration: 0.366s, episode steps:  49, steps per second: 134, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.233593, mae: 9.736152, mean_q: 18.838127\n",
            " 3411/10000: episode: 115, duration: 0.267s, episode steps:  36, steps per second: 135, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.502471, mae: 9.843372, mean_q: 19.152920\n",
            " 3428/10000: episode: 116, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.655913, mae: 9.983675, mean_q: 19.330502\n",
            " 3498/10000: episode: 117, duration: 0.513s, episode steps:  70, steps per second: 137, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.351663, mae: 10.000162, mean_q: 19.185953\n",
            " 3544/10000: episode: 118, duration: 0.342s, episode steps:  46, steps per second: 134, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.662695, mae: 10.059319, mean_q: 19.509453\n",
            " 3578/10000: episode: 119, duration: 0.258s, episode steps:  34, steps per second: 132, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.841144, mae: 10.125992, mean_q: 19.697996\n",
            " 3655/10000: episode: 120, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.345974, mae: 10.297934, mean_q: 20.009356\n",
            " 3678/10000: episode: 121, duration: 0.170s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.827264, mae: 10.330450, mean_q: 20.162268\n",
            " 3695/10000: episode: 122, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.785830, mae: 10.416485, mean_q: 20.171829\n",
            " 3817/10000: episode: 123, duration: 0.885s, episode steps: 122, steps per second: 138, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.967800, mae: 10.586298, mean_q: 20.538654\n",
            " 3873/10000: episode: 124, duration: 0.420s, episode steps:  56, steps per second: 133, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 3.761319, mae: 10.688448, mean_q: 20.809692\n",
            " 3923/10000: episode: 125, duration: 0.375s, episode steps:  50, steps per second: 133, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.955283, mae: 10.802868, mean_q: 21.071150\n",
            " 3947/10000: episode: 126, duration: 0.186s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 4.138943, mae: 10.883804, mean_q: 21.233492\n",
            " 4017/10000: episode: 127, duration: 0.512s, episode steps:  70, steps per second: 137, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.113844, mae: 11.023856, mean_q: 21.509735\n",
            " 4066/10000: episode: 128, duration: 0.364s, episode steps:  49, steps per second: 135, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 3.834324, mae: 11.145428, mean_q: 21.750864\n",
            " 4112/10000: episode: 129, duration: 0.351s, episode steps:  46, steps per second: 131, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.142619, mae: 11.219599, mean_q: 21.934258\n",
            " 4132/10000: episode: 130, duration: 0.151s, episode steps:  20, steps per second: 132, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.575498, mae: 11.210998, mean_q: 21.822161\n",
            " 4220/10000: episode: 131, duration: 0.662s, episode steps:  88, steps per second: 133, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.208201, mae: 11.392816, mean_q: 22.202595\n",
            " 4293/10000: episode: 132, duration: 0.529s, episode steps:  73, steps per second: 138, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.501759, mae: 11.560125, mean_q: 22.569132\n",
            " 4337/10000: episode: 133, duration: 0.326s, episode steps:  44, steps per second: 135, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.297137, mae: 11.623500, mean_q: 22.618294\n",
            " 4363/10000: episode: 134, duration: 0.189s, episode steps:  26, steps per second: 137, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.756930, mae: 11.818588, mean_q: 23.025002\n",
            " 4473/10000: episode: 135, duration: 0.789s, episode steps: 110, steps per second: 139, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.192602, mae: 11.883049, mean_q: 23.237350\n",
            " 4521/10000: episode: 136, duration: 0.348s, episode steps:  48, steps per second: 138, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.907843, mae: 12.023404, mean_q: 23.594086\n",
            " 4606/10000: episode: 137, duration: 0.617s, episode steps:  85, steps per second: 138, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.553960, mae: 12.191810, mean_q: 23.957760\n",
            " 4690/10000: episode: 138, duration: 0.616s, episode steps:  84, steps per second: 136, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 5.610796, mae: 12.365758, mean_q: 24.040333\n",
            " 4749/10000: episode: 139, duration: 0.442s, episode steps:  59, steps per second: 133, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.614788, mae: 12.430607, mean_q: 24.352009\n",
            " 4812/10000: episode: 140, duration: 0.460s, episode steps:  63, steps per second: 137, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.457489, mae: 12.516829, mean_q: 24.592363\n",
            " 4885/10000: episode: 141, duration: 0.538s, episode steps:  73, steps per second: 136, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.442352, mae: 12.651596, mean_q: 24.717333\n",
            " 4984/10000: episode: 142, duration: 0.719s, episode steps:  99, steps per second: 138, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.615608, mae: 12.830480, mean_q: 25.299583\n",
            " 5038/10000: episode: 143, duration: 0.404s, episode steps:  54, steps per second: 134, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.045194, mae: 12.887196, mean_q: 25.431658\n",
            " 5116/10000: episode: 144, duration: 0.571s, episode steps:  78, steps per second: 137, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.198619, mae: 13.076175, mean_q: 25.611429\n",
            " 5146/10000: episode: 145, duration: 0.225s, episode steps:  30, steps per second: 133, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.438947, mae: 13.115183, mean_q: 25.831675\n",
            " 5256/10000: episode: 146, duration: 0.807s, episode steps: 110, steps per second: 136, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.943820, mae: 13.351398, mean_q: 26.187706\n",
            " 5313/10000: episode: 147, duration: 0.423s, episode steps:  57, steps per second: 135, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 4.415112, mae: 13.430324, mean_q: 26.451324\n",
            " 5359/10000: episode: 148, duration: 0.334s, episode steps:  46, steps per second: 138, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 3.054358, mae: 13.644958, mean_q: 27.017637\n",
            " 5499/10000: episode: 149, duration: 1.024s, episode steps: 140, steps per second: 137, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.659446, mae: 13.804589, mean_q: 27.217306\n",
            " 5523/10000: episode: 150, duration: 0.178s, episode steps:  24, steps per second: 135, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.607128, mae: 14.022305, mean_q: 27.602699\n",
            " 5626/10000: episode: 151, duration: 0.755s, episode steps: 103, steps per second: 136, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 5.843815, mae: 14.129409, mean_q: 27.760609\n",
            " 5711/10000: episode: 152, duration: 0.622s, episode steps:  85, steps per second: 137, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.790096, mae: 14.244450, mean_q: 28.115499\n",
            " 5778/10000: episode: 153, duration: 0.491s, episode steps:  67, steps per second: 137, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.407643, mae: 14.434115, mean_q: 28.331961\n",
            " 5978/10000: episode: 154, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.461895, mae: 14.561091, mean_q: 28.549610\n",
            " 6089/10000: episode: 155, duration: 0.811s, episode steps: 111, steps per second: 137, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.809970, mae: 14.762284, mean_q: 29.020267\n",
            " 6186/10000: episode: 156, duration: 0.719s, episode steps:  97, steps per second: 135, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 5.091331, mae: 14.947818, mean_q: 29.532370\n",
            " 6384/10000: episode: 157, duration: 1.419s, episode steps: 198, steps per second: 140, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.821100, mae: 15.187214, mean_q: 29.907301\n",
            " 6481/10000: episode: 158, duration: 0.704s, episode steps:  97, steps per second: 138, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 5.121717, mae: 15.306464, mean_q: 30.205276\n",
            " 6582/10000: episode: 159, duration: 0.732s, episode steps: 101, steps per second: 138, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.351931, mae: 15.484544, mean_q: 30.731527\n",
            " 6680/10000: episode: 160, duration: 0.706s, episode steps:  98, steps per second: 139, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.567293, mae: 15.702043, mean_q: 31.177059\n",
            " 6843/10000: episode: 161, duration: 1.184s, episode steps: 163, steps per second: 138, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.864430, mae: 15.944178, mean_q: 31.627283\n",
            " 6897/10000: episode: 162, duration: 0.401s, episode steps:  54, steps per second: 135, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5.826820, mae: 16.082346, mean_q: 31.904922\n",
            " 7041/10000: episode: 163, duration: 1.031s, episode steps: 144, steps per second: 140, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.666582, mae: 16.277824, mean_q: 32.270950\n",
            " 7210/10000: episode: 164, duration: 1.220s, episode steps: 169, steps per second: 139, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 6.182755, mae: 16.571884, mean_q: 32.856968\n",
            " 7410/10000: episode: 165, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.404842, mae: 16.776337, mean_q: 33.368801\n",
            " 7464/10000: episode: 166, duration: 0.394s, episode steps:  54, steps per second: 137, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.716450, mae: 17.062248, mean_q: 33.892704\n",
            " 7616/10000: episode: 167, duration: 1.097s, episode steps: 152, steps per second: 139, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.159461, mae: 17.367853, mean_q: 34.471119\n",
            " 7809/10000: episode: 168, duration: 1.392s, episode steps: 193, steps per second: 139, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.773888, mae: 17.518929, mean_q: 34.921547\n",
            " 7993/10000: episode: 169, duration: 1.322s, episode steps: 184, steps per second: 139, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.237144, mae: 17.897343, mean_q: 35.779758\n",
            " 8193/10000: episode: 170, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.039841, mae: 18.181635, mean_q: 36.310596\n",
            " 8393/10000: episode: 171, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.281385, mae: 18.643618, mean_q: 37.191898\n",
            " 8533/10000: episode: 172, duration: 1.011s, episode steps: 140, steps per second: 138, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.137621, mae: 18.862963, mean_q: 37.628662\n",
            " 8690/10000: episode: 173, duration: 1.140s, episode steps: 157, steps per second: 138, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.523759, mae: 19.140051, mean_q: 38.224697\n",
            " 8874/10000: episode: 174, duration: 1.320s, episode steps: 184, steps per second: 139, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.883753, mae: 19.389370, mean_q: 38.739891\n",
            " 9074/10000: episode: 175, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.692316, mae: 19.773975, mean_q: 39.537121\n",
            " 9274/10000: episode: 176, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.730066, mae: 20.132513, mean_q: 40.241734\n",
            " 9474/10000: episode: 177, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 7.177041, mae: 20.410168, mean_q: 40.838524\n",
            " 9674/10000: episode: 178, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.325097, mae: 20.799080, mean_q: 41.635372\n",
            " 9874/10000: episode: 179, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.361576, mae: 21.119665, mean_q: 42.345352\n",
            "done, took 75.328 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 195.000, steps: 195\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 171.000, steps: 171\n",
            "Episode 14: reward: 199.000, steps: 199\n",
            "Episode 15: reward: 198.000, steps: 198\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa34a51fa10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy instead of BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "efM9jkXr5A3c",
        "outputId": "d88fd8e4-7951-40ec-e30d-821620eb48c5"
      },
      "source": [
        "from rl.policy import BoltzmannGumbelQPolicy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\r\n",
        "# based on the paper (https://arxiv.org/pdf/1705.10257.pdf).\r\n",
        "policy = BoltzmannGumbelQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   16/10000: episode: 1, duration: 1.120s, episode steps:  16, steps per second:  14, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.560935, mae: 0.626279, mean_q: 0.238333\n",
            "   31/10000: episode: 2, duration: 0.101s, episode steps:  15, steps per second: 148, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.133 [0.000, 1.000],  loss: 0.478536, mae: 0.612231, mean_q: 0.223342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   46/10000: episode: 3, duration: 0.108s, episode steps:  15, steps per second: 139, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.414500, mae: 0.673849, mean_q: 0.187327\n",
            "   55/10000: episode: 4, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.342510, mae: 0.659907, mean_q: 0.260872\n",
            "   65/10000: episode: 5, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.301094, mae: 0.629075, mean_q: 0.369332\n",
            "   77/10000: episode: 6, duration: 0.089s, episode steps:  12, steps per second: 134, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.277227, mae: 0.628563, mean_q: 0.518231\n",
            "   88/10000: episode: 7, duration: 0.077s, episode steps:  11, steps per second: 142, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.267064, mae: 0.619994, mean_q: 0.624173\n",
            "   98/10000: episode: 8, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.240016, mae: 0.617697, mean_q: 0.752242\n",
            "  108/10000: episode: 9, duration: 0.082s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.242203, mae: 0.625276, mean_q: 0.803485\n",
            "  117/10000: episode: 10, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.224574, mae: 0.628598, mean_q: 0.883425\n",
            "  127/10000: episode: 11, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.253383, mae: 0.651616, mean_q: 0.982243\n",
            "  138/10000: episode: 12, duration: 0.082s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.249183, mae: 0.659852, mean_q: 1.038742\n",
            "  148/10000: episode: 13, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.249672, mae: 0.659621, mean_q: 1.075659\n",
            "  157/10000: episode: 14, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.261512, mae: 0.681886, mean_q: 1.216525\n",
            "  166/10000: episode: 15, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.289499, mae: 0.692271, mean_q: 1.244112\n",
            "  175/10000: episode: 16, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.299607, mae: 0.704215, mean_q: 1.270622\n",
            "  184/10000: episode: 17, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.312389, mae: 0.758842, mean_q: 1.396800\n",
            "  194/10000: episode: 18, duration: 0.068s, episode steps:  10, steps per second: 146, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.342487, mae: 0.785793, mean_q: 1.470749\n",
            "  203/10000: episode: 19, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.312600, mae: 0.766190, mean_q: 1.532905\n",
            "  213/10000: episode: 20, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.375352, mae: 0.821733, mean_q: 1.617251\n",
            "  223/10000: episode: 21, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.372838, mae: 0.839078, mean_q: 1.753183\n",
            "  232/10000: episode: 22, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.398202, mae: 0.859133, mean_q: 1.734858\n",
            "  241/10000: episode: 23, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.361765, mae: 0.855705, mean_q: 1.817353\n",
            "  250/10000: episode: 24, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.441858, mae: 0.924638, mean_q: 1.946314\n",
            "  258/10000: episode: 25, duration: 0.058s, episode steps:   8, steps per second: 138, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.496067, mae: 0.984248, mean_q: 1.998760\n",
            "  269/10000: episode: 26, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.422186, mae: 0.994699, mean_q: 1.913635\n",
            "  279/10000: episode: 27, duration: 0.072s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.503038, mae: 1.111334, mean_q: 2.076385\n",
            "  287/10000: episode: 28, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.347271, mae: 1.018585, mean_q: 2.141333\n",
            "  296/10000: episode: 29, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.524404, mae: 1.099875, mean_q: 2.293418\n",
            "  305/10000: episode: 30, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.552557, mae: 1.166655, mean_q: 2.370083\n",
            "  314/10000: episode: 31, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.437745, mae: 1.068237, mean_q: 2.315953\n",
            "  324/10000: episode: 32, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.503152, mae: 1.152318, mean_q: 2.422924\n",
            "  333/10000: episode: 33, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.523454, mae: 1.234263, mean_q: 2.527736\n",
            "  344/10000: episode: 34, duration: 0.082s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.583383, mae: 1.223992, mean_q: 2.534249\n",
            "  353/10000: episode: 35, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.613467, mae: 1.263134, mean_q: 2.628057\n",
            "  363/10000: episode: 36, duration: 0.071s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.525312, mae: 1.289788, mean_q: 2.595744\n",
            "  372/10000: episode: 37, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.651220, mae: 1.347208, mean_q: 2.646362\n",
            "  381/10000: episode: 38, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.495582, mae: 1.370220, mean_q: 2.819037\n",
            "  390/10000: episode: 39, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.593721, mae: 1.343683, mean_q: 2.949728\n",
            "  400/10000: episode: 40, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.551832, mae: 1.361703, mean_q: 2.985819\n",
            "  410/10000: episode: 41, duration: 0.074s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.551217, mae: 1.389499, mean_q: 2.984888\n",
            "  418/10000: episode: 42, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.574947, mae: 1.417145, mean_q: 3.046948\n",
            "  427/10000: episode: 43, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.490955, mae: 1.454619, mean_q: 3.083729\n",
            "  436/10000: episode: 44, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.681320, mae: 1.541423, mean_q: 3.139213\n",
            "  445/10000: episode: 45, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.578246, mae: 1.565655, mean_q: 3.185901\n",
            "  454/10000: episode: 46, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.527361, mae: 1.604267, mean_q: 3.224844\n",
            "  462/10000: episode: 47, duration: 0.055s, episode steps:   8, steps per second: 144, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.562306, mae: 1.620922, mean_q: 3.361397\n",
            "  471/10000: episode: 48, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.560499, mae: 1.605625, mean_q: 3.432978\n",
            "  481/10000: episode: 49, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.507265, mae: 1.598866, mean_q: 3.452161\n",
            "  490/10000: episode: 50, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.604151, mae: 1.684435, mean_q: 3.576751\n",
            "  499/10000: episode: 51, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.559297, mae: 1.682301, mean_q: 3.548569\n",
            "  507/10000: episode: 52, duration: 0.058s, episode steps:   8, steps per second: 139, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.654851, mae: 1.738262, mean_q: 3.596942\n",
            "  517/10000: episode: 53, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.456888, mae: 1.613565, mean_q: 3.662442\n",
            "  527/10000: episode: 54, duration: 0.073s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.576944, mae: 1.665947, mean_q: 3.765509\n",
            "  536/10000: episode: 55, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.627146, mae: 1.711390, mean_q: 3.806524\n",
            "  545/10000: episode: 56, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.580264, mae: 1.744680, mean_q: 3.748542\n",
            "  554/10000: episode: 57, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.611962, mae: 1.671976, mean_q: 3.905516\n",
            "  564/10000: episode: 58, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.705366, mae: 1.715804, mean_q: 3.993314\n",
            "  573/10000: episode: 59, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.644868, mae: 1.709045, mean_q: 3.977547\n",
            "  582/10000: episode: 60, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.575870, mae: 1.736632, mean_q: 3.991523\n",
            "  591/10000: episode: 61, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.576158, mae: 1.693482, mean_q: 4.099881\n",
            "  600/10000: episode: 62, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.667002, mae: 1.814280, mean_q: 4.111670\n",
            "  610/10000: episode: 63, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.487324, mae: 1.698684, mean_q: 4.236048\n",
            "  618/10000: episode: 64, duration: 0.063s, episode steps:   8, steps per second: 126, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.567070, mae: 1.759664, mean_q: 4.332536\n",
            "  627/10000: episode: 65, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.809603, mae: 1.886072, mean_q: 4.361670\n",
            "  637/10000: episode: 66, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.547901, mae: 1.837354, mean_q: 4.301191\n",
            "  646/10000: episode: 67, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.530730, mae: 1.839072, mean_q: 4.433027\n",
            "  655/10000: episode: 68, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.591256, mae: 1.826055, mean_q: 4.536716\n",
            "  664/10000: episode: 69, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.586720, mae: 1.886709, mean_q: 4.568581\n",
            "  672/10000: episode: 70, duration: 0.059s, episode steps:   8, steps per second: 135, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.579826, mae: 1.880698, mean_q: 4.661571\n",
            "  681/10000: episode: 71, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.501863, mae: 1.873948, mean_q: 4.680256\n",
            "  689/10000: episode: 72, duration: 0.058s, episode steps:   8, steps per second: 138, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.699711, mae: 2.014484, mean_q: 4.695678\n",
            "  699/10000: episode: 73, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.668289, mae: 2.019134, mean_q: 4.700690\n",
            "  709/10000: episode: 74, duration: 0.079s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.726238, mae: 2.080858, mean_q: 4.791926\n",
            "  717/10000: episode: 75, duration: 0.061s, episode steps:   8, steps per second: 132, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.569461, mae: 2.023000, mean_q: 4.890236\n",
            "  726/10000: episode: 76, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.807771, mae: 2.168256, mean_q: 4.910122\n",
            "  736/10000: episode: 77, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.612966, mae: 2.145804, mean_q: 4.893764\n",
            "  746/10000: episode: 78, duration: 0.073s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.582900, mae: 2.136743, mean_q: 4.935475\n",
            "  755/10000: episode: 79, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.808818, mae: 2.229052, mean_q: 4.981812\n",
            "  764/10000: episode: 80, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.640097, mae: 2.123017, mean_q: 4.994345\n",
            "  773/10000: episode: 81, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.639698, mae: 2.166254, mean_q: 5.061641\n",
            "  783/10000: episode: 82, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.570581, mae: 2.128624, mean_q: 5.259339\n",
            "  793/10000: episode: 83, duration: 0.068s, episode steps:  10, steps per second: 146, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.755694, mae: 2.242270, mean_q: 5.222045\n",
            "  802/10000: episode: 84, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.587316, mae: 2.231542, mean_q: 5.242079\n",
            "  812/10000: episode: 85, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.748159, mae: 2.269320, mean_q: 5.293842\n",
            "  821/10000: episode: 86, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.654462, mae: 2.277736, mean_q: 5.321486\n",
            "  831/10000: episode: 87, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.701083, mae: 2.396985, mean_q: 5.297736\n",
            "  839/10000: episode: 88, duration: 0.058s, episode steps:   8, steps per second: 139, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.680858, mae: 2.380945, mean_q: 5.342327\n",
            "  849/10000: episode: 89, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.681781, mae: 2.319170, mean_q: 5.381557\n",
            "  858/10000: episode: 90, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.789212, mae: 2.347494, mean_q: 5.487046\n",
            "  867/10000: episode: 91, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.625697, mae: 2.311820, mean_q: 5.441282\n",
            "  876/10000: episode: 92, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.661083, mae: 2.345041, mean_q: 5.576761\n",
            "  886/10000: episode: 93, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.524748, mae: 2.257282, mean_q: 5.496905\n",
            "  895/10000: episode: 94, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.838986, mae: 2.422916, mean_q: 5.622739\n",
            "  904/10000: episode: 95, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.487396, mae: 2.264251, mean_q: 5.573710\n",
            "  913/10000: episode: 96, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.564659, mae: 2.325929, mean_q: 5.640541\n",
            "  922/10000: episode: 97, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.628636, mae: 2.295014, mean_q: 5.601542\n",
            "  931/10000: episode: 98, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.630851, mae: 2.399858, mean_q: 5.765306\n",
            "  940/10000: episode: 99, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.688806, mae: 2.327621, mean_q: 5.864631\n",
            "  949/10000: episode: 100, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.554114, mae: 2.358624, mean_q: 5.991019\n",
            "  959/10000: episode: 101, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.680713, mae: 2.447098, mean_q: 5.982636\n",
            "  969/10000: episode: 102, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.698235, mae: 2.418476, mean_q: 5.868452\n",
            "  979/10000: episode: 103, duration: 0.086s, episode steps:  10, steps per second: 116, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.588295, mae: 2.410647, mean_q: 5.833949\n",
            "  989/10000: episode: 104, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.583020, mae: 2.534808, mean_q: 5.974611\n",
            "  999/10000: episode: 105, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.551067, mae: 2.318238, mean_q: 5.821817\n",
            " 1009/10000: episode: 106, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.704065, mae: 2.422180, mean_q: 6.015616\n",
            " 1017/10000: episode: 107, duration: 0.060s, episode steps:   8, steps per second: 133, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.885051, mae: 2.501456, mean_q: 5.843729\n",
            " 1025/10000: episode: 108, duration: 0.058s, episode steps:   8, steps per second: 137, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.949071, mae: 2.547174, mean_q: 6.020508\n",
            " 1035/10000: episode: 109, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.457682, mae: 2.393214, mean_q: 5.796239\n",
            " 1046/10000: episode: 110, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.729346, mae: 2.505141, mean_q: 6.051577\n",
            " 1055/10000: episode: 111, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.538509, mae: 2.522476, mean_q: 6.167995\n",
            " 1065/10000: episode: 112, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.814942, mae: 2.594782, mean_q: 6.194679\n",
            " 1075/10000: episode: 113, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.447801, mae: 2.421414, mean_q: 5.984843\n",
            " 1084/10000: episode: 114, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.595306, mae: 2.458965, mean_q: 5.928649\n",
            " 1094/10000: episode: 115, duration: 0.072s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.613253, mae: 2.440696, mean_q: 6.087940\n",
            " 1104/10000: episode: 116, duration: 0.078s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.453240, mae: 2.553131, mean_q: 6.218197\n",
            " 1113/10000: episode: 117, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.522568, mae: 2.528656, mean_q: 6.139454\n",
            " 1122/10000: episode: 118, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.635438, mae: 2.510576, mean_q: 6.200047\n",
            " 1132/10000: episode: 119, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.070716, mae: 2.660822, mean_q: 6.277898\n",
            " 1142/10000: episode: 120, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.961373, mae: 2.739624, mean_q: 6.350940\n",
            " 1151/10000: episode: 121, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.649874, mae: 2.635459, mean_q: 6.345351\n",
            " 1161/10000: episode: 122, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.814334, mae: 2.591811, mean_q: 6.052446\n",
            " 1170/10000: episode: 123, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.535897, mae: 2.634667, mean_q: 6.361220\n",
            " 1178/10000: episode: 124, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.646312, mae: 2.565463, mean_q: 6.107731\n",
            " 1189/10000: episode: 125, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.499814, mae: 2.461694, mean_q: 5.989604\n",
            " 1198/10000: episode: 126, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.305377, mae: 2.488243, mean_q: 6.006287\n",
            " 1207/10000: episode: 127, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.750621, mae: 2.662317, mean_q: 6.285783\n",
            " 1217/10000: episode: 128, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.546658, mae: 2.673011, mean_q: 6.253236\n",
            " 1227/10000: episode: 129, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.738151, mae: 2.566658, mean_q: 6.006356\n",
            " 1237/10000: episode: 130, duration: 0.068s, episode steps:  10, steps per second: 146, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.484174, mae: 2.642009, mean_q: 6.278630\n",
            " 1246/10000: episode: 131, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.788307, mae: 2.609624, mean_q: 6.137458\n",
            " 1255/10000: episode: 132, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.844516, mae: 2.640137, mean_q: 6.167814\n",
            " 1266/10000: episode: 133, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.459745, mae: 2.585087, mean_q: 6.097031\n",
            " 1276/10000: episode: 134, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.949354, mae: 2.758244, mean_q: 6.239913\n",
            " 1286/10000: episode: 135, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.242112, mae: 2.547435, mean_q: 6.086617\n",
            " 1295/10000: episode: 136, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.211720, mae: 2.629386, mean_q: 6.263200\n",
            " 1304/10000: episode: 137, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.530549, mae: 2.806277, mean_q: 6.452890\n",
            " 1313/10000: episode: 138, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.780093, mae: 2.690603, mean_q: 6.076945\n",
            " 1323/10000: episode: 139, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.661844, mae: 2.744478, mean_q: 6.346380\n",
            " 1332/10000: episode: 140, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.141547, mae: 2.829208, mean_q: 6.405423\n",
            " 1341/10000: episode: 141, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.735534, mae: 2.771690, mean_q: 6.213338\n",
            " 1350/10000: episode: 142, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.022424, mae: 2.846720, mean_q: 6.221937\n",
            " 1360/10000: episode: 143, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.067929, mae: 2.870611, mean_q: 6.462616\n",
            " 1369/10000: episode: 144, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.487174, mae: 2.682266, mean_q: 6.034842\n",
            " 1380/10000: episode: 145, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.370254, mae: 2.664670, mean_q: 6.085576\n",
            " 1390/10000: episode: 146, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.883796, mae: 2.885058, mean_q: 6.380003\n",
            " 1400/10000: episode: 147, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.614390, mae: 2.768375, mean_q: 6.252650\n",
            " 1410/10000: episode: 148, duration: 0.067s, episode steps:  10, steps per second: 149, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.770830, mae: 2.956382, mean_q: 6.439745\n",
            " 1420/10000: episode: 149, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.361147, mae: 2.909404, mean_q: 6.227423\n",
            " 1428/10000: episode: 150, duration: 0.058s, episode steps:   8, steps per second: 139, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.780431, mae: 2.942338, mean_q: 6.478140\n",
            " 1437/10000: episode: 151, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.455723, mae: 2.776146, mean_q: 6.250861\n",
            " 1448/10000: episode: 152, duration: 0.076s, episode steps:  11, steps per second: 144, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.724489, mae: 2.900411, mean_q: 6.255014\n",
            " 1459/10000: episode: 153, duration: 0.082s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.018436, mae: 2.997524, mean_q: 6.470424\n",
            " 1469/10000: episode: 154, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.389187, mae: 2.916409, mean_q: 6.328064\n",
            " 1478/10000: episode: 155, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.671870, mae: 2.982361, mean_q: 6.520443\n",
            " 1488/10000: episode: 156, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.644466, mae: 2.907392, mean_q: 6.187605\n",
            " 1498/10000: episode: 157, duration: 0.077s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.727675, mae: 3.014114, mean_q: 6.369798\n",
            " 1509/10000: episode: 158, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.646797, mae: 2.962442, mean_q: 6.322676\n",
            " 1518/10000: episode: 159, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.714584, mae: 2.897914, mean_q: 6.126641\n",
            " 1527/10000: episode: 160, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.725487, mae: 2.862285, mean_q: 6.022818\n",
            " 1538/10000: episode: 161, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.687480, mae: 3.039694, mean_q: 6.357458\n",
            " 1548/10000: episode: 162, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.104851, mae: 2.945386, mean_q: 6.162124\n",
            " 1559/10000: episode: 163, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.505480, mae: 2.913972, mean_q: 6.205638\n",
            " 1570/10000: episode: 164, duration: 0.076s, episode steps:  11, steps per second: 144, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.453816, mae: 3.002239, mean_q: 6.360489\n",
            " 1580/10000: episode: 165, duration: 0.077s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.252968, mae: 2.947618, mean_q: 6.237702\n",
            " 1589/10000: episode: 166, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.748914, mae: 2.952918, mean_q: 6.097179\n",
            " 1600/10000: episode: 167, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.539208, mae: 3.126762, mean_q: 6.535736\n",
            " 1609/10000: episode: 168, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.884460, mae: 3.085732, mean_q: 6.283094\n",
            " 1620/10000: episode: 169, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.644854, mae: 2.881251, mean_q: 5.918744\n",
            " 1630/10000: episode: 170, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.545881, mae: 3.026948, mean_q: 6.144906\n",
            " 1641/10000: episode: 171, duration: 0.083s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.881570, mae: 3.103382, mean_q: 6.296545\n",
            " 1649/10000: episode: 172, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.259300, mae: 2.918815, mean_q: 5.972894\n",
            " 1658/10000: episode: 173, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.417179, mae: 2.950954, mean_q: 6.093576\n",
            " 1667/10000: episode: 174, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.103783, mae: 3.199011, mean_q: 6.410578\n",
            " 1676/10000: episode: 175, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.166726, mae: 2.990614, mean_q: 6.236393\n",
            " 1686/10000: episode: 176, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.821777, mae: 3.224243, mean_q: 6.447652\n",
            " 1695/10000: episode: 177, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.237689, mae: 3.081564, mean_q: 6.288754\n",
            " 1706/10000: episode: 178, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.719345, mae: 3.131260, mean_q: 6.306253\n",
            " 1715/10000: episode: 179, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.259762, mae: 3.326928, mean_q: 6.500952\n",
            " 1724/10000: episode: 180, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.027405, mae: 2.993772, mean_q: 5.932158\n",
            " 1733/10000: episode: 181, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.817926, mae: 3.108783, mean_q: 6.116480\n",
            " 1742/10000: episode: 182, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.707963, mae: 3.187518, mean_q: 6.318350\n",
            " 1753/10000: episode: 183, duration: 0.077s, episode steps:  11, steps per second: 143, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.435627, mae: 3.067953, mean_q: 6.140744\n",
            " 1763/10000: episode: 184, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.171594, mae: 3.421614, mean_q: 6.649693\n",
            " 1773/10000: episode: 185, duration: 0.074s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.454521, mae: 3.165267, mean_q: 6.313982\n",
            " 1781/10000: episode: 186, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.787976, mae: 3.138122, mean_q: 6.189249\n",
            " 1792/10000: episode: 187, duration: 0.076s, episode steps:  11, steps per second: 144, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.749044, mae: 3.237116, mean_q: 6.327699\n",
            " 1802/10000: episode: 188, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.164403, mae: 3.276306, mean_q: 6.316294\n",
            " 1811/10000: episode: 189, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.534661, mae: 3.110886, mean_q: 6.129801\n",
            " 1822/10000: episode: 190, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.299807, mae: 3.016172, mean_q: 5.980113\n",
            " 1831/10000: episode: 191, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.644160, mae: 3.233124, mean_q: 6.331434\n",
            " 1841/10000: episode: 192, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.318612, mae: 3.060138, mean_q: 6.120950\n",
            " 1850/10000: episode: 193, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.493293, mae: 3.069662, mean_q: 6.059602\n",
            " 1859/10000: episode: 194, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.432764, mae: 2.959119, mean_q: 5.792001\n",
            " 1870/10000: episode: 195, duration: 0.079s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.228813, mae: 3.141449, mean_q: 6.238544\n",
            " 1880/10000: episode: 196, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.474987, mae: 3.092948, mean_q: 6.084845\n",
            " 1890/10000: episode: 197, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.756341, mae: 3.269524, mean_q: 6.402119\n",
            " 1900/10000: episode: 198, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.624288, mae: 3.198668, mean_q: 6.191094\n",
            " 1910/10000: episode: 199, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.287925, mae: 3.037328, mean_q: 5.932206\n",
            " 1921/10000: episode: 200, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.161449, mae: 3.177533, mean_q: 6.278482\n",
            " 1951/10000: episode: 201, duration: 0.201s, episode steps:  30, steps per second: 150, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.791714, mae: 3.149390, mean_q: 6.065474\n",
            " 1960/10000: episode: 202, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.485593, mae: 3.121962, mean_q: 6.059954\n",
            " 1971/10000: episode: 203, duration: 0.075s, episode steps:  11, steps per second: 146, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.924640, mae: 3.214451, mean_q: 6.160722\n",
            " 1982/10000: episode: 204, duration: 0.085s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.340086, mae: 3.067231, mean_q: 5.942045\n",
            " 2010/10000: episode: 205, duration: 0.192s, episode steps:  28, steps per second: 146, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.829929, mae: 3.326102, mean_q: 6.340639\n",
            " 2020/10000: episode: 206, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.799284, mae: 3.159096, mean_q: 6.102124\n",
            " 2028/10000: episode: 207, duration: 0.059s, episode steps:   8, steps per second: 135, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.328673, mae: 3.129404, mean_q: 6.057504\n",
            " 2038/10000: episode: 208, duration: 0.082s, episode steps:  10, steps per second: 122, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.853779, mae: 3.349596, mean_q: 6.378774\n",
            " 2050/10000: episode: 209, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.699072, mae: 3.305888, mean_q: 6.345909\n",
            " 2061/10000: episode: 210, duration: 0.076s, episode steps:  11, steps per second: 144, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.451915, mae: 3.230445, mean_q: 6.266778\n",
            " 2072/10000: episode: 211, duration: 0.075s, episode steps:  11, steps per second: 146, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.511206, mae: 3.206679, mean_q: 6.153458\n",
            " 2122/10000: episode: 212, duration: 0.336s, episode steps:  50, steps per second: 149, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.340318, mae: 3.294883, mean_q: 6.315353\n",
            " 2169/10000: episode: 213, duration: 0.309s, episode steps:  47, steps per second: 152, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.711606, mae: 3.428249, mean_q: 6.479720\n",
            " 2183/10000: episode: 214, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.891706, mae: 3.521758, mean_q: 6.618012\n",
            " 2194/10000: episode: 215, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.883541, mae: 3.747917, mean_q: 7.042645\n",
            " 2206/10000: episode: 216, duration: 0.087s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.592251, mae: 3.630122, mean_q: 6.884807\n",
            " 2236/10000: episode: 217, duration: 0.199s, episode steps:  30, steps per second: 151, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.145465, mae: 3.648095, mean_q: 6.845393\n",
            " 2267/10000: episode: 218, duration: 0.219s, episode steps:  31, steps per second: 141, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.719727, mae: 3.668275, mean_q: 6.917938\n",
            " 2281/10000: episode: 219, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.346046, mae: 3.656691, mean_q: 6.802691\n",
            " 2295/10000: episode: 220, duration: 0.102s, episode steps:  14, steps per second: 138, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.769111, mae: 3.688309, mean_q: 6.991623\n",
            " 2311/10000: episode: 221, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.113681, mae: 3.817056, mean_q: 7.149269\n",
            " 2323/10000: episode: 222, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.668839, mae: 3.548415, mean_q: 6.705816\n",
            " 2335/10000: episode: 223, duration: 0.087s, episode steps:  12, steps per second: 138, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.839329, mae: 3.906047, mean_q: 7.391376\n",
            " 2346/10000: episode: 224, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.153340, mae: 3.942385, mean_q: 7.404083\n",
            " 2360/10000: episode: 225, duration: 0.093s, episode steps:  14, steps per second: 151, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.746903, mae: 4.084700, mean_q: 7.540951\n",
            " 2371/10000: episode: 226, duration: 0.078s, episode steps:  11, steps per second: 142, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.132350, mae: 3.995584, mean_q: 7.419357\n",
            " 2381/10000: episode: 227, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.245163, mae: 3.873219, mean_q: 7.245152\n",
            " 2393/10000: episode: 228, duration: 0.103s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.844375, mae: 3.925553, mean_q: 7.401540\n",
            " 2404/10000: episode: 229, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.770564, mae: 4.000999, mean_q: 7.678223\n",
            " 2416/10000: episode: 230, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.495510, mae: 4.106044, mean_q: 7.728355\n",
            " 2425/10000: episode: 231, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.683963, mae: 4.409060, mean_q: 8.264891\n",
            " 2436/10000: episode: 232, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.993410, mae: 4.147092, mean_q: 7.863269\n",
            " 2447/10000: episode: 233, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.208763, mae: 4.118876, mean_q: 7.741748\n",
            " 2458/10000: episode: 234, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.983805, mae: 4.215907, mean_q: 8.009711\n",
            " 2472/10000: episode: 235, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.349163, mae: 4.162099, mean_q: 7.784133\n",
            " 2481/10000: episode: 236, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.365669, mae: 4.210903, mean_q: 7.741215\n",
            " 2493/10000: episode: 237, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.739298, mae: 4.353133, mean_q: 8.056766\n",
            " 2504/10000: episode: 238, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.485568, mae: 4.263574, mean_q: 7.945917\n",
            " 2513/10000: episode: 239, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.177487, mae: 4.225063, mean_q: 7.923323\n",
            " 2523/10000: episode: 240, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.039983, mae: 4.306609, mean_q: 8.112265\n",
            " 2535/10000: episode: 241, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.094618, mae: 4.339858, mean_q: 8.237361\n",
            " 2548/10000: episode: 242, duration: 0.094s, episode steps:  13, steps per second: 138, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.920775, mae: 4.452182, mean_q: 8.298326\n",
            " 2562/10000: episode: 243, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.599610, mae: 4.293563, mean_q: 8.006849\n",
            " 2574/10000: episode: 244, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.003569, mae: 4.225212, mean_q: 7.971164\n",
            " 2586/10000: episode: 245, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.872524, mae: 4.602461, mean_q: 8.502194\n",
            " 2596/10000: episode: 246, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.301998, mae: 4.312999, mean_q: 8.170789\n",
            " 2609/10000: episode: 247, duration: 0.096s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.862889, mae: 4.541859, mean_q: 8.466740\n",
            " 2622/10000: episode: 248, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.724955, mae: 4.382896, mean_q: 8.170539\n",
            " 2636/10000: episode: 249, duration: 0.093s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.316309, mae: 4.492537, mean_q: 8.420833\n",
            " 2646/10000: episode: 250, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.687232, mae: 4.512534, mean_q: 8.458431\n",
            " 2659/10000: episode: 251, duration: 0.093s, episode steps:  13, steps per second: 139, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.378694, mae: 4.546088, mean_q: 8.613108\n",
            " 2668/10000: episode: 252, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.687492, mae: 4.757304, mean_q: 9.051080\n",
            " 2679/10000: episode: 253, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.452273, mae: 4.608841, mean_q: 8.690975\n",
            " 2692/10000: episode: 254, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.808537, mae: 4.708097, mean_q: 8.850010\n",
            " 2702/10000: episode: 255, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.752972, mae: 4.902199, mean_q: 9.203290\n",
            " 2714/10000: episode: 256, duration: 0.088s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.633406, mae: 4.828889, mean_q: 8.912322\n",
            " 2723/10000: episode: 257, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.444543, mae: 4.744357, mean_q: 8.680542\n",
            " 2732/10000: episode: 258, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.645939, mae: 4.717695, mean_q: 8.760509\n",
            " 2741/10000: episode: 259, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.791720, mae: 4.777758, mean_q: 8.699045\n",
            " 2750/10000: episode: 260, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.156204, mae: 4.816667, mean_q: 8.927926\n",
            " 2761/10000: episode: 261, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.979651, mae: 4.975253, mean_q: 9.104363\n",
            " 2771/10000: episode: 262, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.545492, mae: 4.993533, mean_q: 9.191633\n",
            " 2781/10000: episode: 263, duration: 0.078s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.177450, mae: 4.580377, mean_q: 8.623930\n",
            " 2791/10000: episode: 264, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.081128, mae: 4.760180, mean_q: 9.124838\n",
            " 2800/10000: episode: 265, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 3.099229, mae: 5.074195, mean_q: 9.459721\n",
            " 2810/10000: episode: 266, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.267869, mae: 5.011550, mean_q: 9.298279\n",
            " 2819/10000: episode: 267, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.416332, mae: 4.831091, mean_q: 9.176560\n",
            " 2829/10000: episode: 268, duration: 0.068s, episode steps:  10, steps per second: 146, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.862413, mae: 4.749316, mean_q: 9.124042\n",
            " 2839/10000: episode: 269, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.535018, mae: 4.945117, mean_q: 9.432663\n",
            " 2849/10000: episode: 270, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.544940, mae: 4.977989, mean_q: 9.367402\n",
            " 2859/10000: episode: 271, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.646892, mae: 4.874214, mean_q: 9.286674\n",
            " 2868/10000: episode: 272, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.574231, mae: 4.996002, mean_q: 9.555116\n",
            " 2878/10000: episode: 273, duration: 0.072s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.964784, mae: 4.914088, mean_q: 9.274205\n",
            " 2888/10000: episode: 274, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.439475, mae: 5.103499, mean_q: 9.609357\n",
            " 2896/10000: episode: 275, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.615332, mae: 4.888666, mean_q: 9.233975\n",
            " 2905/10000: episode: 276, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.514137, mae: 4.860162, mean_q: 9.086387\n",
            " 2915/10000: episode: 277, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.845300, mae: 5.078911, mean_q: 9.455766\n",
            " 2924/10000: episode: 278, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.443285, mae: 5.169677, mean_q: 9.844244\n",
            " 2934/10000: episode: 279, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.724532, mae: 4.875646, mean_q: 9.238804\n",
            " 2944/10000: episode: 280, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.597151, mae: 4.970880, mean_q: 9.274023\n",
            " 2954/10000: episode: 281, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.651606, mae: 4.900465, mean_q: 9.252213\n",
            " 2963/10000: episode: 282, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.655911, mae: 5.175083, mean_q: 9.896608\n",
            " 2971/10000: episode: 283, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.745359, mae: 5.101047, mean_q: 9.788516\n",
            " 2980/10000: episode: 284, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.363645, mae: 5.137877, mean_q: 9.730533\n",
            " 2990/10000: episode: 285, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.740068, mae: 5.042191, mean_q: 9.495439\n",
            " 2999/10000: episode: 286, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.136137, mae: 5.119103, mean_q: 9.646906\n",
            " 3008/10000: episode: 287, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.813392, mae: 5.213013, mean_q: 9.834191\n",
            " 3018/10000: episode: 288, duration: 0.082s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.126451, mae: 5.187490, mean_q: 9.843253\n",
            " 3026/10000: episode: 289, duration: 0.058s, episode steps:   8, steps per second: 138, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.798280, mae: 5.174644, mean_q: 10.220009\n",
            " 3037/10000: episode: 290, duration: 0.075s, episode steps:  11, steps per second: 146, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 3.334019, mae: 5.368463, mean_q: 10.042134\n",
            " 3046/10000: episode: 291, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.984103, mae: 5.262042, mean_q: 9.795570\n",
            " 3056/10000: episode: 292, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.555397, mae: 5.246531, mean_q: 9.810476\n",
            " 3064/10000: episode: 293, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 2.443237, mae: 5.238217, mean_q: 9.818546\n",
            " 3074/10000: episode: 294, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.756974, mae: 5.294404, mean_q: 9.945204\n",
            " 3083/10000: episode: 295, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 3.513265, mae: 5.468465, mean_q: 10.090673\n",
            " 3093/10000: episode: 296, duration: 0.078s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.659446, mae: 5.316900, mean_q: 9.986610\n",
            " 3102/10000: episode: 297, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.556118, mae: 5.302748, mean_q: 9.941618\n",
            " 3112/10000: episode: 298, duration: 0.074s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.924125, mae: 5.341889, mean_q: 9.938355\n",
            " 3121/10000: episode: 299, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.015894, mae: 5.309580, mean_q: 9.952631\n",
            " 3129/10000: episode: 300, duration: 0.060s, episode steps:   8, steps per second: 134, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.496798, mae: 5.258151, mean_q: 9.829205\n",
            " 3139/10000: episode: 301, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.141091, mae: 5.237790, mean_q: 9.897593\n",
            " 3148/10000: episode: 302, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.375067, mae: 5.416723, mean_q: 10.216573\n",
            " 3157/10000: episode: 303, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.298955, mae: 5.297832, mean_q: 10.041481\n",
            " 3166/10000: episode: 304, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.327027, mae: 5.446402, mean_q: 10.231067\n",
            " 3175/10000: episode: 305, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.212210, mae: 5.337232, mean_q: 10.123871\n",
            " 3186/10000: episode: 306, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.639020, mae: 5.312113, mean_q: 10.152832\n",
            " 3195/10000: episode: 307, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.234101, mae: 5.273251, mean_q: 10.035675\n",
            " 3204/10000: episode: 308, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.387388, mae: 5.303616, mean_q: 10.059084\n",
            " 3214/10000: episode: 309, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.864345, mae: 5.493914, mean_q: 10.349669\n",
            " 3223/10000: episode: 310, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 3.828549, mae: 5.549560, mean_q: 10.064687\n",
            " 3232/10000: episode: 311, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.526422, mae: 5.324961, mean_q: 9.961185\n",
            " 3242/10000: episode: 312, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.097494, mae: 5.565083, mean_q: 10.473722\n",
            " 3251/10000: episode: 313, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.783957, mae: 5.284473, mean_q: 10.026517\n",
            " 3262/10000: episode: 314, duration: 0.081s, episode steps:  11, steps per second: 136, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.804860, mae: 5.233989, mean_q: 9.971427\n",
            " 3272/10000: episode: 315, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.749452, mae: 5.280616, mean_q: 10.104967\n",
            " 3281/10000: episode: 316, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.436104, mae: 5.359558, mean_q: 10.114008\n",
            " 3291/10000: episode: 317, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.258080, mae: 5.404675, mean_q: 10.166501\n",
            " 3301/10000: episode: 318, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.348852, mae: 5.437833, mean_q: 10.192838\n",
            " 3313/10000: episode: 319, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.394619, mae: 5.364736, mean_q: 9.968623\n",
            " 3322/10000: episode: 320, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.222764, mae: 5.248459, mean_q: 9.840290\n",
            " 3331/10000: episode: 321, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.757520, mae: 5.326877, mean_q: 10.037623\n",
            " 3340/10000: episode: 322, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.355960, mae: 5.055058, mean_q: 9.618633\n",
            " 3351/10000: episode: 323, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.177177, mae: 5.359805, mean_q: 10.074539\n",
            " 3360/10000: episode: 324, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.972960, mae: 5.349561, mean_q: 10.126546\n",
            " 3370/10000: episode: 325, duration: 0.071s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.581437, mae: 5.330287, mean_q: 10.252449\n",
            " 3379/10000: episode: 326, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.837857, mae: 5.308846, mean_q: 10.176840\n",
            " 3389/10000: episode: 327, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.584647, mae: 5.376462, mean_q: 10.419776\n",
            " 3398/10000: episode: 328, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.891399, mae: 5.372345, mean_q: 10.049023\n",
            " 3407/10000: episode: 329, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.776851, mae: 5.053199, mean_q: 9.902828\n",
            " 3416/10000: episode: 330, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.235985, mae: 5.519433, mean_q: 10.392819\n",
            " 3425/10000: episode: 331, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.661180, mae: 5.319746, mean_q: 9.925482\n",
            " 3436/10000: episode: 332, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.190183, mae: 5.197062, mean_q: 9.713609\n",
            " 3446/10000: episode: 333, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.218781, mae: 5.146212, mean_q: 9.888510\n",
            " 3456/10000: episode: 334, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.102767, mae: 5.396668, mean_q: 10.185884\n",
            " 3467/10000: episode: 335, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.956965, mae: 5.305741, mean_q: 10.029754\n",
            " 3477/10000: episode: 336, duration: 0.071s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.485591, mae: 5.251725, mean_q: 10.131975\n",
            " 3485/10000: episode: 337, duration: 0.056s, episode steps:   8, steps per second: 142, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 2.077224, mae: 5.299032, mean_q: 10.070593\n",
            " 3495/10000: episode: 338, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.249620, mae: 5.162364, mean_q: 10.030603\n",
            " 3505/10000: episode: 339, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.225736, mae: 5.346856, mean_q: 10.145155\n",
            " 3514/10000: episode: 340, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.676711, mae: 5.212757, mean_q: 9.689336\n",
            " 3525/10000: episode: 341, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.963199, mae: 5.055389, mean_q: 9.802931\n",
            " 3534/10000: episode: 342, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.491986, mae: 5.020614, mean_q: 9.541274\n",
            " 3543/10000: episode: 343, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.967138, mae: 5.174735, mean_q: 9.815432\n",
            " 3552/10000: episode: 344, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.393180, mae: 5.200264, mean_q: 10.008697\n",
            " 3563/10000: episode: 345, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.911051, mae: 5.219389, mean_q: 9.946741\n",
            " 3573/10000: episode: 346, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.795819, mae: 4.963444, mean_q: 9.374325\n",
            " 3581/10000: episode: 347, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 1.576602, mae: 5.107933, mean_q: 9.719965\n",
            " 3590/10000: episode: 348, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.254176, mae: 4.848927, mean_q: 9.252616\n",
            " 3599/10000: episode: 349, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.906429, mae: 5.268382, mean_q: 10.070835\n",
            " 3610/10000: episode: 350, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.623430, mae: 5.171544, mean_q: 9.863853\n",
            " 3620/10000: episode: 351, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.765599, mae: 5.193801, mean_q: 9.945601\n",
            " 3628/10000: episode: 352, duration: 0.061s, episode steps:   8, steps per second: 130, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 1.178928, mae: 5.153910, mean_q: 10.025808\n",
            " 3638/10000: episode: 353, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.652036, mae: 5.211773, mean_q: 9.996881\n",
            " 3647/10000: episode: 354, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.271081, mae: 4.967799, mean_q: 9.527429\n",
            " 3656/10000: episode: 355, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.213193, mae: 5.156780, mean_q: 9.922209\n",
            " 3667/10000: episode: 356, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.327243, mae: 5.208796, mean_q: 9.778011\n",
            " 3679/10000: episode: 357, duration: 0.086s, episode steps:  12, steps per second: 139, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.157544, mae: 5.171820, mean_q: 9.963228\n",
            " 3688/10000: episode: 358, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.739949, mae: 5.105769, mean_q: 9.691738\n",
            " 3697/10000: episode: 359, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.202776, mae: 5.183052, mean_q: 10.002311\n",
            " 3707/10000: episode: 360, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.220110, mae: 5.113277, mean_q: 9.578058\n",
            " 3717/10000: episode: 361, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.328129, mae: 4.996046, mean_q: 9.541811\n",
            " 3728/10000: episode: 362, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.634309, mae: 5.145679, mean_q: 9.835110\n",
            " 3738/10000: episode: 363, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.331482, mae: 5.011381, mean_q: 9.530285\n",
            " 3749/10000: episode: 364, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.436513, mae: 5.069152, mean_q: 9.642973\n",
            " 3758/10000: episode: 365, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.103876, mae: 4.950131, mean_q: 9.501897\n",
            " 3770/10000: episode: 366, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.552366, mae: 5.106034, mean_q: 9.705102\n",
            " 3781/10000: episode: 367, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.414820, mae: 4.917370, mean_q: 9.333543\n",
            " 3794/10000: episode: 368, duration: 0.087s, episode steps:  13, steps per second: 150, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.455363, mae: 4.909652, mean_q: 9.287212\n",
            " 3805/10000: episode: 369, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.045228, mae: 4.976680, mean_q: 9.559748\n",
            " 3819/10000: episode: 370, duration: 0.101s, episode steps:  14, steps per second: 139, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.783045, mae: 5.106371, mean_q: 9.640814\n",
            " 3834/10000: episode: 371, duration: 0.105s, episode steps:  15, steps per second: 142, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.258090, mae: 4.958415, mean_q: 9.479589\n",
            " 3849/10000: episode: 372, duration: 0.113s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.551432, mae: 5.063561, mean_q: 9.574996\n",
            " 3862/10000: episode: 373, duration: 0.090s, episode steps:  13, steps per second: 144, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.587395, mae: 5.100031, mean_q: 9.636145\n",
            " 3876/10000: episode: 374, duration: 0.095s, episode steps:  14, steps per second: 147, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.466887, mae: 4.932773, mean_q: 9.378317\n",
            " 3892/10000: episode: 375, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.542156, mae: 4.772810, mean_q: 8.987494\n",
            " 3906/10000: episode: 376, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.972973, mae: 4.963473, mean_q: 9.527825\n",
            " 3917/10000: episode: 377, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.530780, mae: 4.750523, mean_q: 8.930126\n",
            " 3933/10000: episode: 378, duration: 0.113s, episode steps:  16, steps per second: 142, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.242550, mae: 4.996865, mean_q: 9.462243\n",
            " 3984/10000: episode: 379, duration: 0.347s, episode steps:  51, steps per second: 147, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.515691, mae: 4.917764, mean_q: 9.258947\n",
            " 3999/10000: episode: 380, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.688465, mae: 4.985346, mean_q: 9.343699\n",
            " 4018/10000: episode: 381, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.380909, mae: 4.825547, mean_q: 9.086311\n",
            " 4032/10000: episode: 382, duration: 0.100s, episode steps:  14, steps per second: 140, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.664875, mae: 4.987113, mean_q: 9.351618\n",
            " 4076/10000: episode: 383, duration: 0.293s, episode steps:  44, steps per second: 150, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.257854, mae: 4.804073, mean_q: 9.075116\n",
            " 4125/10000: episode: 384, duration: 0.340s, episode steps:  49, steps per second: 144, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.502690, mae: 4.901928, mean_q: 9.213602\n",
            " 4171/10000: episode: 385, duration: 0.308s, episode steps:  46, steps per second: 149, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.400913, mae: 4.810698, mean_q: 9.086209\n",
            " 4190/10000: episode: 386, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.100259, mae: 4.813437, mean_q: 9.131213\n",
            " 4227/10000: episode: 387, duration: 0.244s, episode steps:  37, steps per second: 152, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.471967, mae: 4.922918, mean_q: 9.290293\n",
            " 4273/10000: episode: 388, duration: 0.313s, episode steps:  46, steps per second: 147, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.331748, mae: 4.827658, mean_q: 9.087683\n",
            " 4327/10000: episode: 389, duration: 0.366s, episode steps:  54, steps per second: 148, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.349824, mae: 4.785369, mean_q: 9.013036\n",
            " 4382/10000: episode: 390, duration: 0.372s, episode steps:  55, steps per second: 148, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.215714, mae: 4.789106, mean_q: 9.023415\n",
            " 4441/10000: episode: 391, duration: 0.392s, episode steps:  59, steps per second: 151, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.078176, mae: 4.818861, mean_q: 9.139246\n",
            " 4492/10000: episode: 392, duration: 0.335s, episode steps:  51, steps per second: 152, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.238163, mae: 4.920521, mean_q: 9.300910\n",
            " 4545/10000: episode: 393, duration: 0.359s, episode steps:  53, steps per second: 148, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.454654, mae: 4.849353, mean_q: 9.098575\n",
            " 4626/10000: episode: 394, duration: 0.539s, episode steps:  81, steps per second: 150, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.224152, mae: 4.880684, mean_q: 9.175895\n",
            " 4693/10000: episode: 395, duration: 0.447s, episode steps:  67, steps per second: 150, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.268721, mae: 4.906286, mean_q: 9.183771\n",
            " 4770/10000: episode: 396, duration: 0.529s, episode steps:  77, steps per second: 146, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.231479, mae: 4.927348, mean_q: 9.254368\n",
            " 4838/10000: episode: 397, duration: 0.442s, episode steps:  68, steps per second: 154, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.284491, mae: 4.977594, mean_q: 9.344801\n",
            " 4906/10000: episode: 398, duration: 0.452s, episode steps:  68, steps per second: 150, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.262304, mae: 4.907870, mean_q: 9.192334\n",
            " 4971/10000: episode: 399, duration: 0.430s, episode steps:  65, steps per second: 151, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.181883, mae: 5.067223, mean_q: 9.548021\n",
            " 5029/10000: episode: 400, duration: 0.382s, episode steps:  58, steps per second: 152, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.208662, mae: 5.053485, mean_q: 9.513643\n",
            " 5114/10000: episode: 401, duration: 0.564s, episode steps:  85, steps per second: 151, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.216167, mae: 5.205369, mean_q: 9.828377\n",
            " 5198/10000: episode: 402, duration: 0.548s, episode steps:  84, steps per second: 153, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.347410, mae: 5.222880, mean_q: 9.822463\n",
            " 5282/10000: episode: 403, duration: 0.558s, episode steps:  84, steps per second: 150, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.386959, mae: 5.305377, mean_q: 9.937802\n",
            " 5347/10000: episode: 404, duration: 0.434s, episode steps:  65, steps per second: 150, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.228161, mae: 5.336657, mean_q: 10.057631\n",
            " 5417/10000: episode: 405, duration: 0.465s, episode steps:  70, steps per second: 151, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.377666, mae: 5.350629, mean_q: 10.075001\n",
            " 5486/10000: episode: 406, duration: 0.450s, episode steps:  69, steps per second: 153, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.111840, mae: 5.471335, mean_q: 10.415253\n",
            " 5566/10000: episode: 407, duration: 0.526s, episode steps:  80, steps per second: 152, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.523665, mae: 5.571912, mean_q: 10.480031\n",
            " 5657/10000: episode: 408, duration: 0.590s, episode steps:  91, steps per second: 154, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.115993, mae: 5.605697, mean_q: 10.632873\n",
            " 5755/10000: episode: 409, duration: 0.628s, episode steps:  98, steps per second: 156, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.428738, mae: 5.714423, mean_q: 10.825914\n",
            " 5808/10000: episode: 410, duration: 0.355s, episode steps:  53, steps per second: 149, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 1.812167, mae: 5.837173, mean_q: 10.989758\n",
            " 5867/10000: episode: 411, duration: 0.379s, episode steps:  59, steps per second: 156, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.657056, mae: 5.765301, mean_q: 10.833927\n",
            " 5946/10000: episode: 412, duration: 0.521s, episode steps:  79, steps per second: 152, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.336556, mae: 5.877793, mean_q: 11.175183\n",
            " 5993/10000: episode: 413, duration: 0.314s, episode steps:  47, steps per second: 150, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 1.643025, mae: 6.030692, mean_q: 11.454335\n",
            " 6063/10000: episode: 414, duration: 0.466s, episode steps:  70, steps per second: 150, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.583971, mae: 6.059293, mean_q: 11.495814\n",
            " 6141/10000: episode: 415, duration: 0.513s, episode steps:  78, steps per second: 152, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.471388, mae: 6.125211, mean_q: 11.660936\n",
            " 6235/10000: episode: 416, duration: 0.610s, episode steps:  94, steps per second: 154, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.634595, mae: 6.115048, mean_q: 11.573476\n",
            " 6308/10000: episode: 417, duration: 0.482s, episode steps:  73, steps per second: 151, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.472055, mae: 6.216579, mean_q: 11.855812\n",
            " 6391/10000: episode: 418, duration: 0.538s, episode steps:  83, steps per second: 154, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.686924, mae: 6.263887, mean_q: 11.961000\n",
            " 6487/10000: episode: 419, duration: 0.616s, episode steps:  96, steps per second: 156, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.004137, mae: 6.437046, mean_q: 12.213899\n",
            " 6558/10000: episode: 420, duration: 0.473s, episode steps:  71, steps per second: 150, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 1.260450, mae: 6.520609, mean_q: 12.512465\n",
            " 6618/10000: episode: 421, duration: 0.409s, episode steps:  60, steps per second: 147, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.802713, mae: 6.603480, mean_q: 12.578357\n",
            " 6673/10000: episode: 422, duration: 0.365s, episode steps:  55, steps per second: 151, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.590414, mae: 6.745795, mean_q: 12.924003\n",
            " 6749/10000: episode: 423, duration: 0.506s, episode steps:  76, steps per second: 150, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 1.839694, mae: 6.855506, mean_q: 13.148311\n",
            " 6836/10000: episode: 424, duration: 0.587s, episode steps:  87, steps per second: 148, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.203680, mae: 6.902617, mean_q: 13.132657\n",
            " 6919/10000: episode: 425, duration: 0.553s, episode steps:  83, steps per second: 150, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.706034, mae: 6.868351, mean_q: 13.136100\n",
            " 7011/10000: episode: 426, duration: 0.605s, episode steps:  92, steps per second: 152, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.699639, mae: 7.124898, mean_q: 13.701317\n",
            " 7085/10000: episode: 427, duration: 0.494s, episode steps:  74, steps per second: 150, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.983448, mae: 7.194545, mean_q: 13.806747\n",
            " 7154/10000: episode: 428, duration: 0.462s, episode steps:  69, steps per second: 149, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 2.157169, mae: 7.346344, mean_q: 14.051177\n",
            " 7271/10000: episode: 429, duration: 0.768s, episode steps: 117, steps per second: 152, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.075328, mae: 7.400351, mean_q: 14.197861\n",
            " 7471/10000: episode: 430, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.253135, mae: 7.549469, mean_q: 14.462220\n",
            " 7671/10000: episode: 431, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.017666, mae: 7.754344, mean_q: 14.974541\n",
            " 7767/10000: episode: 432, duration: 0.619s, episode steps:  96, steps per second: 155, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.134278, mae: 7.938006, mean_q: 15.352107\n",
            " 7847/10000: episode: 433, duration: 0.538s, episode steps:  80, steps per second: 149, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.020392, mae: 8.090154, mean_q: 15.694757\n",
            " 7911/10000: episode: 434, duration: 0.421s, episode steps:  64, steps per second: 152, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 2.435672, mae: 8.158288, mean_q: 15.717932\n",
            " 8001/10000: episode: 435, duration: 0.599s, episode steps:  90, steps per second: 150, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 2.493191, mae: 8.253600, mean_q: 15.889826\n",
            " 8088/10000: episode: 436, duration: 0.570s, episode steps:  87, steps per second: 153, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.579331, mae: 8.301368, mean_q: 15.969256\n",
            " 8236/10000: episode: 437, duration: 0.953s, episode steps: 148, steps per second: 155, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.685005, mae: 8.360147, mean_q: 16.076077\n",
            " 8436/10000: episode: 438, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.170138, mae: 8.579593, mean_q: 16.671806\n",
            " 8503/10000: episode: 439, duration: 0.440s, episode steps:  67, steps per second: 152, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 3.301648, mae: 8.818867, mean_q: 16.998989\n",
            " 8625/10000: episode: 440, duration: 0.798s, episode steps: 122, steps per second: 153, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.658199, mae: 8.905144, mean_q: 17.291813\n",
            " 8737/10000: episode: 441, duration: 0.743s, episode steps: 112, steps per second: 151, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.490438, mae: 9.159210, mean_q: 17.812105\n",
            " 8837/10000: episode: 442, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.931830, mae: 9.228946, mean_q: 17.865644\n",
            " 8925/10000: episode: 443, duration: 0.586s, episode steps:  88, steps per second: 150, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.172453, mae: 9.299163, mean_q: 18.149696\n",
            " 9045/10000: episode: 444, duration: 0.788s, episode steps: 120, steps per second: 152, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.616272, mae: 9.449740, mean_q: 18.425930\n",
            " 9137/10000: episode: 445, duration: 0.607s, episode steps:  92, steps per second: 152, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.705346, mae: 9.559913, mean_q: 18.690302\n",
            " 9232/10000: episode: 446, duration: 0.628s, episode steps:  95, steps per second: 151, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 2.801345, mae: 9.754226, mean_q: 19.045996\n",
            " 9333/10000: episode: 447, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.626520, mae: 9.867683, mean_q: 19.332937\n",
            " 9435/10000: episode: 448, duration: 0.672s, episode steps: 102, steps per second: 152, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.761938, mae: 10.070196, mean_q: 19.731077\n",
            " 9580/10000: episode: 449, duration: 0.945s, episode steps: 145, steps per second: 153, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.735372, mae: 10.231739, mean_q: 20.062555\n",
            " 9695/10000: episode: 450, duration: 0.751s, episode steps: 115, steps per second: 153, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.529091, mae: 10.391741, mean_q: 20.439014\n",
            " 9783/10000: episode: 451, duration: 0.579s, episode steps:  88, steps per second: 152, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 2.963170, mae: 10.591762, mean_q: 20.801624\n",
            " 9914/10000: episode: 452, duration: 0.855s, episode steps: 131, steps per second: 153, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.891829, mae: 10.730774, mean_q: 21.121313\n",
            "done, took 69.947 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxcdbn48c8zS9amTZd0oVtaKBSKUGgoZRPZBKqieHFBQFT8AV5UuHivF1RUvO4KiKhIFQRREBQUkMrSgkDZUyilKy2le9qkW/Zlluf3xzln5sxk0jZtJzPJPO/XK6/MfM+cyTcn7Xnm+a6iqhhjjDGeQK4rYIwxJr9YYDDGGJPCAoMxxpgUFhiMMcaksMBgjDEmRSjXFdhfI0aM0Orq6lxXwxhj+pWFCxduU9WqTMf6fWCorq6mtrY219Uwxph+RUTW9XTMmpKMMcaksMBgjDEmhQUGY4wxKSwwGGOMSWGBwRhjTIqsBgYRGS8iz4rIMhFZKiJXu+XDRORpEVnlfh/qlouI/FJEVovIYhE5Npv1M8YY0122M4Yo8DVVPQKYBVwlIkcA1wHzVXUKMN99DnAuMMX9uhy4Pcv1M8YYkyargUFV61T1DfdxM7AcGAt8FLjHfdk9wMfcxx8F/qiOV4BKERmTzToaY0yuvLJmO7fNX8XO1q5EWXNHhEcWbcphrfqwj0FEqoFjgFeBUapa5x7aAoxyH48FNvhO2+iWpb/X5SJSKyK1DQ0NWauzMcZk03UPLeamp9/h6WVbE2X/WrKFq/+yiPqmjpzVq08Cg4gMAh4CrlHVJv8xdXYK6tVuQao6R1VrVLWmqirjjG5jjMl7rV0xAKLx5C2wKxoHIBLP3SZqWQ8MIhLGCQp/VtWH3eKtXhOR+73eLd8EjPedPs4tM8aYAUt9n43j7q6a8YEaGEREgDuB5ap6s+/Qo8Cl7uNLgUd85Z91RyfNAhp9TU7GGDMg+WNALIcBwZPtRfROAi4B3haRRW7ZN4AfAw+KyGXAOuCT7rG5wGxgNdAGfD7L9TPGmJxxkwNUk8HACwyaw/iQ1cCgqgsA6eHwGRler8BV2ayTMcbkG38QSDQl5TAy2MxnY4zJme5BIBb3H8kNCwzGGJNjmTIGtYzBGGMKlz9j8EYjWcZgjDEFKNn5nCyLWcZgjDEmZR5DHoxKssBgjDE54t3745kyhr6vToIFBmOMybGMo5IsYzDGmMJl8xiMMcYAyQ7mfJv5bIHBGGNyLGVUUmK4qmUMxhhTsOIZJ7jlqDJYYDDGmJxJjkqypiRjjDH4Jrj5yhIZgzUlGWNM4cm0LpJlDMYYU8AyLolhq6saY0zhyjRnweYxGGNMAUsGgWTZgG9KEpG7RKReRJb4yh4QkUXu11pvy08RqRaRdt+x32azbsYYk2vxROezr48hERFyFxmyvefz3cCvgD96Bar6Ke+xiNwENPpe/66qTs9ynYwxJi9ohjkL3uqq8RxmDNne8/l5EanOdExEBPgkcHo262CMMfnKu/nH4zYqyXMKsFVVV/nKJonImyLynIic0tOJInK5iNSKSG1DQ0P2a2qMMVkQz7DEdqFv7XkhcL/veR0wQVWPAa4F7hORwZlOVNU5qlqjqjVVVVV9UFVjjDmwVDWRFWSc+ZyLSrlyEhhEJAR8HHjAK1PVTlXd7j5eCLwLHJqL+hljTLb5E4LUrT2d74U4XPVMYIWqbvQKRKRKRILu48nAFGBNjupnjDFZ5b/x+5uN4smhSjmT7eGq9wMvA4eJyEYRucw99GlSm5EA3g8sdoev/g24UlV3ZLN+xhiTK6krqiYf50NTUrZHJV3YQ/nnMpQ9BDyUzfoYY0y+SMkYMsxjKNRRScYYU7C0h4whOY+h8PoYjDGmoKX2MSTLYxmGsPY1CwzGGJMDe+p8LtR5DMYYU7AybecJljEYY0zB0p6akuLdj/c1CwzGGJMDMV/K4M8eMi2s19csMBhjTA5kCgZgi+gZY0zBSmlK8pVbH4MxxhSonjqfbR6DMcYUqD3OY7CmJGOMKSwpWUJKxuA9sozBGGMKSo/LbufB1p4WGIwxJgdsET1jjDEpUhfO8z/2RiVZU5IxxhQUyxiMMcakSFk4L2Mfg2UMxhhTUHqa+RzPZa+zK9tbe94lIvUissRX9l0R2SQii9yv2b5j14vIahFZKSJnZ7NuxhiTS4U8j+Fu4JwM5beo6nT3ay6AiByBsxf0NPec34hIMMv1M8aYnEjpcM4wj2HAdj6r6vPAjr18+UeBv6hqp6q+B6wGZmatcsYYk0PxnvoY3HJ/4Ohruepj+LKILHabmoa6ZWOBDb7XbHTLuhGRy0WkVkRqGxoasl1XY4w54FImuPnKE6ur9m11UuQiMNwOHAxMB+qAm3r7Bqo6R1VrVLWmqqrqQNfPGGOyLtPWnv6O54LaqEdVt6pqTFXjwO9INhdtAsb7XjrOLTPGmAEnU+dzLGVuQ+70eWAQkTG+p+cD3oilR4FPi0ixiEwCpgCv9XX9jDGmL2RadjuWJxlDKJtvLiL3Ax8ARojIRuA7wAdEZDpOQFwLXAGgqktF5EFgGRAFrlLVWDbrZ4wxuaIZVlftaQir37UPLOLc943hrCNGZa1uWQ0MqnphhuI7d/P6HwA/yF6NjDEmP6ROcHO+p2QMPZz3z8V1DCsvympg2OumJBG5WkQGi+NOEXlDRD6YtZoZY8wAlik78A9R7SljiMbjRGLZHcvamz6GL6hqE/BBYChwCfDjrNTKGGMGOC8wBAOSmMwWy9C8lHJOXIkrdMWy2//Qm8Ag7vfZwL2qutRXZowxphe8+34wIIlmpT01JXmBI58yhoUi8hROYHhSRCqAHM7NM8aY/iuRMYhk7HzO1JbkBY5sB4bedD5fhjMpbY2qtonIcODz2amWMcYMbF5yEApIxs7nTIusRvMtMKhqXESqgYtFRIEFqvr3bFXMGGMGMi87CAQkMXR1T/MYkhlDnvQxiMhvgCuBt3EmpV0hIr/OVsWMMWYg8278IV8fQ3wPM5/zsSnpdOBwdX8bEbkHZzKaMcaYXvKGpqaMSkrJGLqfE3VPyqfO59XABN/z8cCqA1sdY4wpDP7hql6QyLRMhl8iY4hmtympNxlDBbBcRF7DyXJmArUi8iiAqp6XhfoZY8yA5AUBJ2PwynZ/w4+6fQtdedSU9O2s1cIYYwqMv48hc+dz93Pyro9BVZ8TkYnAFFWdJyKlQEhVm7NXPWOMGZi8GBAISObVVTN0P/fVcNXejEr6f8DfgDvconHAP7JRKWOMGejiKRlDapnzuPs5XuCIxpTfv7CGBau2ZaVuvel8vgo4CWgCUNVVwMhsVMoYYwa6xDwG6SFj2M2opK5YnFvnreKZFfVZqVtvAkOnqnZ5T0QkRG43GTLGmH7Lu/GHgpk7nzM1Jfn7GKJxJRTMznJ1vQkMz4nIN4BSETkL+CvwWFZqZYwxA1xyuGrAtyRG8njmjCE58zkWV4KB3AeG64AGnJnPVwBzVfWbuztBRO4SkXoRWeIr+5mIrBCRxSLydxGpdMurRaRdRBa5X7/dh9/HGGP6hcRwVdn7rT3jiXkMcaLxOKE8CAxfUdXfqeonVPUCVf2diFy9h3PuBs5JK3saOFJVjwLeAa73HXtXVae7X1f2om7GGNOvJDufAxk7n3eXMXRG48SVvMgYLs1Q9rndnaCqzwM70sqeUtWo+/QVnNFNxhhTUDSxiF4PGUOGc7zj3gS3bGUMe5zHICIXAp8BJnmznF2DSbvp74MvAA/4nk8SkTdxRj59S1Vf6KFOlwOXA0yYMCHTS4wxJq8ll9329THsZcbgCQZ689l+7+3NBLeXgDpgBHCTr7wZWLyvP1hEvglEgT+7RXXABFXdLiIzgH+IyDR3O9EUqjoHmANQU1NjI6OMMf1Oytae3kY9KfsxZBqVlDqxLWcZg6quA9aJyJlAu7svw6HAVJyO6F4Tkc8BHwbO8FZrVdVOoNN9vFBE3gUOBWr35WcYY0w+86+VlGlrz51tXd1GHkXT9mEI5EEfw/NAiYiMBZ4CLsHpXO4VETkH+Dpwnqq2+cqrRCToPp4MTAHW9Pb9jTGmP1B/xkD3rT3/+PI6bnhkSco5sbSmpHwYlSTujfzjwG9U9RPAtN2eIHI/8DJwmIhsFJHLgF/hrNT6dNqw1PcDi0VkEc7SG1eq6v72YRhjTF7ymo1CKRlD6msefH0DAB2RGJCpjyFHTUk+IiInABfh7P8MENzdCap6YYbiO3t47UPAQ72ojzHG9Fv+RfQSq6um9StE48rDb2zk2gffYv7XTs3LjOFqnDkHf1fVpW5zz7NZqZUxxgxwGRfRy7By3lNLtwKwcktz/mUM7pyE533P1wBf9Z6LyG2q+pUDWz1jjBmYvGAQ7GERPY9I8vXpgSMf1krak5MO4HsZY8yAllhd1beDW3pTEvgCA9pn8xiy867GGGN2y7vJh4OBRCaQqSlJcCKDat/NY7DAYIwxORBPBAbJOPM5k77qYziQgSE7NTTGmAEomhiuGkjux5Bp27ZEU1J+zmMAQETKejh0637WxRhjCkZclYBAoIdltz3tXc4cBtVMfQw5DgwicqKILANWuM+PFpHfeMdV9e4DXz1jjBmYou5yF4GAb1RShoShsT2SeJweOHIeGIBbgLOB7QCq+hbObGVjjDG9FHcDgwi7ncfQ0uHsUhCJabe1kvIhMKCqG9KKYgewLsYYUzCicSUogrD7zueWTi8wxDOMSsrdstueDSJyIqAiEsaZCb08K7UyxpgBzls5NSAkFtHL1MfQ2pUMDHnXxwBcCVwFjAU2AdPd58YYY3opGRiSi+jtrimpKxrvs1FJvVkSYxvOAnrGGGP2U0yVYCCA+EYlZRqt6mUJkZj2Wefz3mzteRuZtx8FQFW/2tMxY4wxmcViSjAAIns3wc1rSgoGJBEgcrlWUi2wECgBjgVWuV/TgaKs1MoYYwa4mCqhQCAxM1hViceduQ2ZOJ3PSmk4udtBLrf2vAdARL4EnKyqUff5b4EXslIrY4wZ4GJxJRCAgPjWQlInI4hnmNDQ5WYMJeEALZ1OWT4sojcUGOx7PsgtM8YY00uxuJMxeB/644mMIXMWEIkqsXiccDCQ6FvIhyUxfgy8KSJ3i8g9wBvAD3d3gojcJSL1IrLEVzZMRJ4WkVXu96FuuYjIL0VktYgsFpFj9+UXMsaY/iDmNhtJIjAkRyplEonFicac42G3byHnw1VV9Q/A8cDfcbbgPMFrZtqNu4Fz0squA+ar6hRgvvsc4Fxgivt1OXD73tbNGGP6Gy9jEK8pCXWaknrIGOoaO5i/op7JVYMIB51bd84Dg2smcArOUhjH7enF7q5vO9KKPwp4AeUe4GO+8j+q4xWgUkTG9LJ+xhjTL0TjSsBdEgOSO7QFfDf7IaXhxONnV9bT2B7ha2cdSlG+BAYR+THObOdl7tdXRWS3TUk9GKWqde7jLcAo9/FYwL/kxka3LFNdLheRWhGpbWho2IcqGGNMbsVVCbkT3LznXuezxx8YvCGqB49MZgz50McwGzhLVe9S1btwmog+vD8/XFWV3cyR2M15c1S1RlVrqqqq9qcKxhiz3/69sp7Nu9p7dY6XMQR8GUMsTkrnsz8wAFSUhBhUHCIcypM+Blel7/GQffyZW70mIvd7vVu+CRjve904t8wYY/JWPK587g+v84nfvtzr80IBSWzd6Y1KCvruyumB4aAhpQCEA17GkPvhqj8idVTSQuAH+/AzHwUudR9fCjziK/+sOzppFtDoa3Iyxpi8tMvdL6GusXvGoKo8tXRLxsXxovG4s7qqf4e2tM7n9MAwekgJQP50Pqvq/cAs4GGSo5Ie2N05InI/8DJwmIhsFJHLcIa9niUiq4Az3ecAc4E1wGrgd8B/9vJ3McaYPtfQ7Mw2G1befSGIuW9v4fJ7F3LngjXdjsXjzo3du7lHY9qt83lwWmAY4wWGUHbnMez1InoichKwSFUfFZGLga+LyK2quq6nc1T1wh4OnZHhtYqt1mqM6We8wDC0LBkYVJWX12xn0642ALY2dXY7LxqPUxQKJZa46IjEdtv5DDC20m1KCjqL7wVynTHgzCtoE5GjgWuBd4E/ZqVWxhjTTzS0dAAw1Jcx/HNxHZ/53av86ZX1QLLpxy+mTsZQWuQEhraumDPBbTdNSUeNr0y8X7ayBehdYIi6n+o/CvxaVX8NVGSnWsYY0z94GcNwX2BYt70VgPU7nIyhyLcKantXjNX1LcTicScw+DKGuKY2JVWWpQaG6eMq3fcLZK1/AXq3g1uziFwPXAy8X0QCQHgP5xhjzIDmBYYS36qn6X3NRaHkZ/Cr7nuDZ1bUM2XkIIIBoazIuQ3vTcYwxA0U4aBkbUQS9C5j+BTQCVymqltwhpP+LCu1MsaYfmJbSxeQui1nt53WfE1Jz6xwRui3dcUIilBa5Bxr6Yywfkd7xpnPnz1hIktuPDvl/bKYMPRqB7ctwM2+5+uxPgZjTIFrcoer+jfZ0bQNdzLdw1s6owSDQmnYuQ3f/PQ7LK9roqI4eVv2AkNZkTOxzVMUDKQEmwNtj+8sIgvc780i0pT+PWs1M8aYfqDZ3ZM55ttDIX0ntkgs3u281s6omzE4TVBLNjm30+bOaOI1XmAoStupLRyU3PYxqOrJ7nfraDbGmDRNHU7GEPU1H6X3MXRFncDQ3hVLlEXdmc9lbmAYM6SEusaOlPMGl4YpKwoypCx1jsTQ8qJu/Q8HUm86n3H3SDgZZ5LeAlV9Myu1MsaYfiKRMcSTWUE8LTJ0uhnD4d9+IqU8EJBEp3WbL2h4ikMBHv/qKYmJbZ5rzjyUL54yef8r34PerK76bZxlsocDI4C7ReRb2aqYMcbko85o6g3cyxj8u3F2RlObjiLRzGuF+jMG731euu70xPGACJNGlKeMeAKnicmb7JYNvem9uAg4TlW/o6rfwVke45LsVMsYY/LPi6u3cdi3nuDJpVsAJzNo6eyeMbSnffrvinXPBsDJGLzJaqpQEg5wkO+Gn81+hN3pTWDYDPjzmWJs9VNjTAF5Z2szAFfcu5C/vLae1q4oXj9z1JcytEXSAkM0TjRDB7Q3e9nrgE6fIZ2juNCrPoZGYKmIPI3Tx3AW8JqI/BJAVb+ahfoZY0ze8E9UW7+jLdG/AKlzF7plDNF4IrPw8/ZeKA0Hae6IJnZm80gP23xmW28Cw9/dL8+/D2xVjDEmv3X5+g46IvFEvwCkDlFtj6QGgUhMaWp3yg4fM5jldc7QVC9j8PoZ/IEnl3ozwe0eESkFJqjqyizWyRhjcq6tK8oR336SX3xqOh87xtll2JuPUBoO0hGNJTKGUEB2mzF0RpNB5JJZE/nG398Gkn0IXudypsX2cqE3o5I+AiwCnnCfTxeRR7NVMWOMySVvqexb5r2TKIu4/QgVJSE6IjGa3Zt9ZVlRah9Dt87neCKIjKwoTpQH8zRj6E0tvgvMBHYBqOoiIHsDaY0xJoe8Zp6Ir/nIa0oaVBKiM5K82Q8tCxOLK7NvfYHfv7CGjm6dz8kgUpUhMPTU+ZwrveljiKhqY1pnSPdudmOMGQC8mcxdvkwgEosTDgoloSCd0VhinaShZUXsaOtiZV0zyx5vSskKnPOUJjeIVJaFCQWEaDy5KY+3XlJ/zBiWishngKCITBGR24CX9uWHishhIrLI99UkIteIyHdFZJOvfPa+vL8xxuwvrz9hW0snP5y7HHAyhnAwQEk44HY+J2/2/uGo3j7QABXFIbqi8UTGUFESTu7ZLKkZQ/qaSLnSm8DwFWAaztLb9+EMX71mX36oqq5U1emqOh2YAbSRHPF0i3dMVefuy/sbY8z+8o9AmvO8s2ezkzEEKAkH3T6GKOGgUF4cSpnt7D93REWxGxicIFJREiLsBoCg+70s3E/7GFS1TVW/qarHuV/fUtXEik9uBrEvzgDe3d3e0cYY09cyrYjaFVOKQm5giMZo6ogwuCRMMCC0ZpinADBiUBGRWJz2SIyiYIBwMJAIAOkZQ770MRzIWpy0j+d9Grjf9/zLIrJYRO4SkaGZThCRy0WkVkRqGxoa9vHHGmNMzyKx7usbRWJxinxNSc0dUSpKQoQCknERPHA6mzujcTojcYrdgJBoSgqkNyUNvMDQayJSBJwH/NUtuh04GJgO1AE3ZTpPVeeoao2q1lRVVfVJXY0xhSU9Y+iIxNw+hmTnc3NHhAo3Y4imr7XtGlwSpisWpzMaozjs3HK9JbOTnc9uxpAnTUm9WnY7C84F3lDVrQDedwAR+R3wz1xVzBhT2LrSAsPOtq5EH0NxWsaQabG7n11wFJt2tbOjtYuuaJzOaJzikBMADqosZcWW5m4zn4sHYMawL93pF+JrRhKRMb5j5wNL9rdSxhizLyJpS2fvbI04TUmhAMUhp/O5qT3Zx5Du3PeN4ZozD6UoGCAS8wKDc8s9qNJZj9QbCptvM597nTGIyGBAVbU57dCtvXyfcpyF+K7wFf9URKbjLNK3Nu2YMcb0mfQ+hl3tXXTFNDEqqbkjSnNHC9PHVyY++XuKQgHK3SygtChIeyRGe1cs0ensLa29pbEdSGYM4VB+DFfd68AgIscBdwEVzlPZBXxBVRcCqOrdvfnBqtqKs+mPv8z2dzDG5IX0PoZdbREi0WTns2dHaxfDB6VOaBtcEkqsjDq4JIyq0xRV7GYG3iY7m3c5Azu9PoaiYOqGPLnSm7zlTuA/VbVaVScCVwF/yE61jDEmt9L7GH70r+W0R2KEQ5Kyo9pnjp9AeguQ15cAzrwFgIbmzkRT0mlTR3L0+Eq+csYhgG+4ap5kDL0JDDFVfcF7oqoLgMwDd40xpp9Lzxg27Ghn0YZdzjwENxJ8+rjxnHH4KIKB1FtpsW90UUWJMwJpW0syMAwuCfPIVScxdfRgIJkx9JvOZxE5VkSOBZ4TkTtE5AMicqqI/Abbk8EYM0B5nc/FoQDP/vcHEuVFwUBiyQtvQbxMfQweL2No64qlZBJ+ZUXOa7zO50tmTeRDR43J+Nq+sDd9DOlzCb7tfhecTmJjjBlwvM7nhTecxaDiEAcNKWFzYwfhUICGZqdvwFssL31Ukj8wDHbnLACJeQzpSosCKef938eOPEC/xb7ZY2BQ1dMARKQE+A+g2neeBQZjzIDk9TF46xodcdAQNjd2UBQMcMGMcdz/2gbOOHwUkCEwBLtnDJDaxORXmpYx5Fpvhqv+A2cvhjcAb40kCwzGmAHJ62MIu/0HowY72UE4KMyYOIy1P/5Q4rXpTUn+zCA1MGRuShpeXsTkqnIOG11xYCq/n3oTGMap6jlZq4kxxuSRSCxOKCAE3Ju+158QkO4jh3aXMQwu8TUl9ZAxlISDPPO1D+xvlQ+Y3uQtL4nI+7JWE2OMySMRdzKbxwsM3t7NfrvrYygOBRLNUT31MeSb3mQMJwOfE5H3cPZkEJwZ0EdlpWbGGJND3oJ5nip3Etuutj0HBn+TkYhQURJmR2tXj01J+aY3geHcrNXCGGPyyOKNu2jriqZ88h9R0XNg2N1wVXCWvNjR2nNTUr7Z68BgG+kYYwrB6vpmzvvViwCMGVKSKB9eXgRAY3umjCH1hp8eGKoqitm4s73fBIb+UUtjjOkjq+tbE4/9fQwjK5wgcfa00d3OSR9lGk7LILz5DsXhgdeUZIwxA9667f7AkLzBlxYFeevbH2RQSffbZnrGEEgLDKMGu8tsR7tvF5qPLDAYY4zPqvqWxOP0CWdDysLpLweSezd7hMwZQ31zB/2BNSUZY4yrtTPKS6u3JZ6n9xX0pD2Sut9z+r49NdXDAJg8onz/KthHLGMwxhjXQ29sZHNjB4NLQjR1RPd6iYr0TCB9+OqsycOZd+2pHFzVPwKDZQzGGOOqa+wgHBTOPdJZ2bRkLyekjXI7po8aNwQgsUmP3yEjB2Usz0c5yxhEZC3QDMSAqKrWiMgw4AGchfrWAp9U1Z25qqMxZmCKx7VbBzHAztYuhpYVEYk7ncTHTxre7TWZfPzYsUyuKufF1dtYvLGxW1NSf5PrjOE0VZ2uqjXu8+uA+ao6BZjvPjcmr6gqqrZ+ZH+1bHMTk78xl2dX1nc7tqO1i2HlRYn1jc50V0/dExHhmAlD8f5ZZFpPqT/JdWBI91HgHvfxPcDHclgXYzI64ttP8uX738x1Ncw+WrjeaYSYt2xrt2M727qoLAvz32cfxgOXz+KIgwb36r3jicCw39XMqVwGBgWeEpGFInK5WzZKVevcx1uAjOFaRC4XkVoRqW1oaOiLuhqT0B6J8fjiuj2/0OSntGzv3Ftf4Mp7FwKwsy3CsPIiBhWHOH7y3jUj+cXc9+4vfQk9yeWopJNVdZOIjASeFpEV/oOqqiKSMV9X1TnAHICamhrL6U1OdERiKZvCm/5HVVle18TyuiYg2cewP+8H1pS0z1R1k/u9Hvg7MBPYKiJjANzv3RsBjckTSzc35boKZj/51z1qaO5kZ5vTx7CvKt2gMqw880S4/iInGYOIlAMBVW12H38Q+B7wKHAp8GP3+yO5qJ8xeyPTuvymf1m/oy3xeMHqBuKavLnvi0tPmEh5UZBP1Iw/ENXLmVw1JY0C/u62w4WA+1T1CRF5HXhQRC4D1gGfzFH9jNmjaMxaMfs7f2B4r8FZI2l/Pu2HggE+PXPCftcr13ISGFR1DXB0hvLtwBl9XyNj9o5/mGo01j8WRDOZicCGHe2J51ubOgH2q49hoMi34arG5LVoPBkYInHLGPq7ls5kc+CWJmdZi/3pYxgoLDAY0wuxuGUMA0nE1xy41Q0MljFYYDCmVyK+YGB9DP1fVzSe2JazodltSrKMwQKDMb0RS2lKsoyhP/KH82g8TlmRMxdle2sXRcEA5UU2N8UCgzG9EE1pSrKMoT9KCe5RpSQcxJuPNrQ83O9nLR8IFhiM6QV/MIhYH0O/5AUGQYjE44SDgcS+C9a/4LDAYEwvRH3NRxHLGPol7++mKJGYUhQKUGSBIYUFBmN6wUYl9X/RlAEEccJBIRx0mo8qSmxTS7DAYEyv+LMEm8fQP/n/bpFYnFAgkNjbudQ6ngELDMb0imUM/V/MbQ6MxpSumBIOJfsYSkIWGMACgzG94u9jiCAViEoAABd/SURBVFrG0C95AwiicXWakgKS6GOwjMFhgcGYXrBRSf2f1xwYjcWJxFJHJRWH7ZYIFhiM6ZVM8xieXVHPii22N0N/4WV9kbgzKikUFEJu57M1JTksMBiTpiMS45FFm1JWUvWk9DHE4yzZ1Mjn736dr9oe0P2GlzHEYkokFqcoGEjs1WxNSQ4bm2VMmp8/uZLfL3iPyrIiTj20KuWYv4+hK6o8tngzAJWlNv69v0h0PsfjRGNKOBhIlJWE7LMyWMZgTDebG501+pvaI2za1c6STY2JY/4+hmg8TltnDCAx3NHkP+9vGHEzhlBQEhmD7eHtyMm/ZhEZLyLPisgyEVkqIle75d8VkU0issj9mp2L+pnC5rUgicBJP36GD9+2IHEsltbH0BFxAkO7+93kP28eQyyudHlNSW6ZBQZHrpqSosDXVPUNEakAForI0+6xW1T15zmqlzFk6FpIiKZNjuqIOp2W7V0WGPoLb/5JJOY0JYWCkvi7WmBw5Gprzzqgzn3cLCLLgbG5qIsx6ZTkImvpvJtK2L2ZqJspdFjG0G9EfPMYvOGqsURgsCZByIM+BhGpBo4BXnWLviwii0XkLhEZ2sM5l4tIrYjUNjQ09FFNTaGJ+1IHb4RS4pNlKOhkDNaU1O8kO5+TgcH7W1vG4MhpYBCRQcBDwDWq2gTcDhwMTMfJKG7KdJ6qzlHVGlWtqaqqyvQSY/aZFw+6ot1XUvU+WRaHg9bH0E95wd2Z4KaEg5L4u5ZaYAByGBhEJIwTFP6sqg8DqOpWVY2pahz4HTAzV/UzhcvLEzp9gaEj6tz4vdnOpUUBovE4HRHnufUx9B/e3zAaU6JxyxgyydWoJAHuBJar6s2+8jG+l50PLOnruhnjZQyd0eTNvsO98cdSmpI0kSl0RuOJkS0mvyWGq8bj7sxn62NIl6tRSScBlwBvi8git+wbwIUiMh3nQ9ta4IrcVM8UNucmkZIxRJLt0uB8snQyhmTw6IzGbeZsP+D9DTvdv2mRjUrqJlejkhZAhiEfMLev62JMukTGEOnelOSNSioNB2lsjyQCBjj9DBYY8p83e317aycAoWAg8Te3tZIcljcZk8abANXha0ry+hCiic7nAJF4nM5IjEHFzucr64DuH7ymJC+oh4MBjp80DLDVVT22VpIxabzmodbOaLeymL8pye1jGD2khJbOqHVA9xPpy6UXBYXbPnMM63e0WVOSq6DD49ptrbmugslDnW4Q2NUWSZR1RFP7GErDQToiMaJxTWwgb4Ehv6kqqtptg6VQMEBZUYipowfnqGb5p2ADw2Nvbeb0m/7NglXbcl0Vk2e8JobG9mRgSDQlxZKBwcsohpY7geEjv1rAtQ8swuSXjkgMVWXWj+Zz42PLiERTMwbbia+7gg0Mp08dyUGVpVx856v84cX3cl0dk0e8voVdvsDgDV2NxeOIOKuptrrBYmhZOPG6h9/c1Ic1NXvyvceWMfWGJ/jiPbVsberk7pfWsqWpI+U1Wxs7eji7cBVsYCgvDvHnLx4PwOKNjXt4tSkkXn9CY1tXt7JoXAkFJLEVJMDw8uKU8+ubOjj4G3P598r6bu/95NItVF/3OI2+ZqpdbV2c9vN/c+8r6wBo6Yxyyk+fsQ8s++ntjY3c5V7D+SuSf4u4wrVnHcpFx08ASPlbGkdBX5GJw8s5rnoode76+8ZAcv7CrkxNSXElGBAmVZUnjh09fgg3njeNU6aMAODB2g3E4sptz6zu9t4/nLscgJVbmxNl33tsGe9ta+Uvr61neV0TR37nSTbsaOfGx5bxkdsWcM9La/nkHS9n3FHOZHbb/FV85FfOcun/c/ZhAHzgsOTyOTUTh/L9jx3JLZ86mitOnZyTOuazgg4MAKOHlFLX2MGFc17h1892/49sCk9HD53PF855hTnPryEcCHDM+MrEsfKiEJeeWM21Zx0KwM+fegeAhuZOzrz5OQ6/4Qm+99gyALa3OFnIJ+94mQdrN/Dqmu08/OYmqoeXsXRzE+fe+kJKXd7e1Mic59fw2ns7mHrDE6zyBZR89aO5y7nuocVZ/RnbWjo55xfPM2/Z1pRyVeX837zITU+/kyi77ORJ3HjeNG6/aEaibPywMkSE848ZZyORMij4wDBmSAnrtrfx8prt3Pz0O7y9sZFoLM7lf6zdY6BYsGob5/1qAdtaOhNlndEYF9z+Ek8t3ZLtqmedqnL9w29z42NL9/jae15ay1X3vZFStnFnG2ff8jxvrt+Z8ZzGtgizb32BZ31NLi+u3kbN9+fxv39bjKry4uptnPOL53nunQbe/9NnOf6H8zj9pn/z5NItnPOL5znr5uc4/ofz+OI9r6e89389sIjfPvcuAE8sqeP837xIY3uEGx9byg3/WML3/7mMHzzu3Kxfe28Hs299gc272mnuiKTMePbUrt3By2u2O9cFmDq6InHsuGpnDPxR4yq54v3JT5/rd7Sxur6F9kiMu158j5rvz6PFNwT2639bzO8XvMeg4hAPXnlC4j0DAleddnDidZt2ORltZzTOfa+tB+DaBxfxkydWZLyuvbVueyvn3voC67e37fG1Ty/bygW3v5SyXEhbV5TzfrWAuW/X0RmNccfza/jL6xvoiMT49JyXeeytzSnvoap8/g+v8etnV/PFe17nDvfvBHDtA4u4/d/vsr2lk3NvfSExOOQ7jyzhhn8kV8j5yb9WsGJLc6L57WdPruD4H87jsG89wZvrdyVeN3pwCSXhIJeeWJ0y+XDMkJJeXqXCIv09Pa2pqdHa2tp9Pv+uBe/xvX86N4iqimIamjsZNbiYrU2diMDfrjyRYydU8t9/XcySTY20dEb5+LFj+doHD+PKexfyxNItfPzYsdz8yeksWLWNi+98NfHen6wZx3fPm8YV9y5k86527vt/sxg12PkHGYnF+cp9b3LCwcP54LRRXPT7Vxk3tIw7Lp7Bt/6xhOrhZXzljCk8vriOe15ey+dOrObel9fxm4uOTYyCufnpd/hb7Qaum30497+6nqaOCL+9eAaPv13HH19ay+DSMF3ROCXhIOGgcN70sZx6aBXXP7yYb394Gu8bNwRw/qP+70OLee6d5BLmo4eUcuX7J/OlPzs3++HlRVSWhYnFleMnDWfs0FIeeH0DFSUhDhtdwSOLnP/8nzuxmieWbKGiJMSUUYOY+7YTIL8xeyrnHjmGrz34Fl8/5zDqmzv5zz8nA8mowcWMrCihqqKYZ9z24LGVpYmb4t547Ztn0NIR5ZI7X0uct+aHsznzludY09BKKCDdRqAcNW4IIsJbG3ZRVhQkHAzQ3BFJbPV4wuThvPre9sTzS2ZN5KhxQ/hEzXgeWbSJcUPLmDExdXX4f71dR31zJ9951AmoU0YOYlV9CwCfP6maP7y4NuX1H51+ELd++hg27mzj5J88y+jBJVx28iR+4DY7iSRnY589bRQ3nncks340P3HdhpQ6nd/RuNLaGaUoFKAkFKStK8b1s6dy/2vraWjupLE9wohBxYl/I8vrmjhsdAU/fWJloi7HVQ9la1MnlWVhWjqifOq48by3rZXRQ0q45sxDmXz948QVLpw5nnXb2/hEzTj+64G3EvUsDTs/F+Cbsw9P/A6TR5QzrLyIn15wFB2ROLN/mZoZTR1dwbrtbYlJgjeeNy1x/RZ+60xmfH8eANMOGsy0gwbzz8V1tHXFGFlRzLihpbzhCwbeNZ05aRjHVQ/j0FHJIL5qazNLNjdy/jHjKHQislBVazIeK/TA8MiiTVz9l0XMnDSMb33ocK55YBFrGlo59dAqVte30NgeYUhpuNsNavywUjbsSJaNG1rKxp3db2LDyovY0ZrsxJwwrIyAuxjI2gyf0PyvnzCsjPU7Ul8zvLyIUYNLEIGlm5sAUm54lWVhGtsjPe5CVlEcorkzSnEokAhSsbiyaVc7Zx4+ihGDitjVFuGJpVsIByWx3HRvHDOhkg072tjW0kVJOED18HJWbm1mcIlTt/Qb9MWzJrCjtSsRRC6cOZ431+9ixZZmjh5fyVsbnP/0IwYV8c0PHU5AhKv/khwWOn18JYvc11SUhGjuSH4qH1tZyubGdiYOK0tc76JQIGVJbYCDhpTw/kOdNugTDxnBV+9/E4AlN57NRb9/lbc27OKUKSO497Lj9/o6PPbWZgaVhKgaVMyHb1vA4WMG86+rT6H6usdTXvfQl05MBJc/vbKOmuqhTBxWzu3/Xs0vn1nNzOphfHrmeH4xbxV1je1UlhXR0NzJ7PeNZmdrJJHJAHyqZjzzlm9lu/tvyH+tTzx4OG9t2MX4YWWs2NL7JqlM/x4zOX3qSJ5ZUZ/xOleUhCgKBtje2sUFM8ZRPbws0fTmVxIOpCw3kskJk4cnfvdLT5jI6CGliSxqzQ9nEwhkWnXHeHYXGAp+5vNpU0dy5akH86UPHMyQ0jB//9JJ3Pnie1x6wkTqGjv4w4triasytrKUYEA4/5ix/OX1Dby+dgcbdrRz6KhBvLO1hY072/lkzTiqR5QTCgjbW7sIBwJs2tXOCZOH83W3zfXIsYN5t76VlVubOfHg4QwtL+LxxXWEg8L/ffRIXn1vB+OGltIRibGtpYuZk4bxt4UbKQoGuOq0Q3hvWwv/WJRMzb2f/4WTJnHa1CoefmMTw8qL+OwJE3n4jU0EA8LNbnvrh44aQ3EwwLSxQ1i2uSllI5rJI8q56rRDCASESCxOzffn0dge4aLjJzB8UDEfPmoMTy/byllHjOKJJVto6Yxy0fETePiNTbz07jZeX7uTYyZUcvS4Sv7rzENZs62FP72yngtmjOPIsYP5xbxV7GjtYtpBg1mxpZlwMMDYyhKmjh7MmUeMIhZX5r7tLJV1yaxqrjotxEMLN3HFqZN5+d3tPLZ4M5edPIlpBw1BVdnS2EEwIJSEg1wwYxxTb3gCgLMOH8XUMRW0d8XZ0dpJU0eUcFD4r7MO5fxfv8SWpg5+deExiZFoFSUh3tnawpc+MJlDRiY/WY6sKGbTznYGFYeYWT2Utzbs4v1Terf3x0eOPgiAeFy55swp/MexzqfUuz9/HG1dzrpKDU2dKRnHxbMmJh5f+8HDGD2klMNGD2LGxGEcMnIQd7+0FlWoHl7O1WdOoSMS47fPvUtJOMghVYM484hRrN3WyqNvbaa+uYM/vbKe844+iMlV5Vx56sH8deFGbvjHEg4aUsIFM8axcmszTy512um9DyXDyou46rRDaO6I8MDrG6hr7OD0qSMZUhpm1uRhTBxezsadbUwcXs7aba2MH1bGpBHlhIMB2iMxPnLUGH4xbxXrd7RxyMhBNHVEKA0HqSgJs3RTIwocOXYIl508CUj2yfz6M8eyur4l8Xc7e9oorvyTk1V+4aRJvL52B29vaqRm4lCOmVDJZ0+o5rZnVnHiwSP42DHOBpBHjRvClsYOCwr7y5sN2F+/ZsyYobnQ3hXVHzy+TNc0tOgPH1+m72xp2u3r/72yXn/3/LuqqtrQ3KE/mrtct7d0qqrqnOfe1effqe/x3L/WbtB/vLkx8fzRRZv0pqdW6v89tlTXbmvRGx9dqq2dkR7Pf+XdbYmfvbf+VrtBv3zfG7q6vnmPr93S2K7ffXTJbuuwNx5ZtEn/Wrthn879/QtrdnsNVVVXbW3Snz+5QuPxeK/ee0dLp9746FJt7ti/36+vrdvWqt97bKm2d0UTZbFYXG+d946+tWFnouyO51Zr7dod2tjepTc+ulR3tXYljtWu3aG3PL0yq/VcuG6H/ubZ1RmPPfD6en100SZVVV3T0KI//tdyjcZ69/czmQG12sN9teCbkowxphDtrimp4EclGWOMSWWBwRhjTAoLDMYYY1LkZWAQkXNEZKWIrBaR63JdH2OMKSR5FxhEJAj8GjgXOAJnH+gjclsrY4wpHHkXGICZwGpVXaOqXcBfgI/muE7GGFMw8jEwjAU2+J5vdMsSRORyEakVkdqGhgaMMcYcOPkYGPZIVeeoao2q1lRV9W42qjHGmN3LxyUxNgHjfc/HuWUZLVy4cJuIrNvHnzUCsL09k+x6JNm1SGXXI2mgXIuJPR3Iu5nPIhIC3gHOwAkIrwOfUdU9r/3c+59V29PMv0Jk1yPJrkUqux5JhXAt8i5jUNWoiHwZeBIIAndlIygYY4zJLO8CA4CqzgXm5roexhhTiPpl5/MBNCfXFcgzdj2S7FqksuuRNOCvRd71MRhjjMmtQs8YjDHGpLHAYIwxJkXBBoZCXKhPRO4SkXoRWeIrGyYiT4vIKvf7ULdcROSX7vVZLCLH5q7mB56IjBeRZ0VkmYgsFZGr3fKCux4iUiIir4nIW+61uNEtnyQir7q/8wMiUuSWF7vPV7vHq3NZ/2wQkaCIvCki/3SfF9S1KMjAUMAL9d0NnJNWdh0wX1WnAPPd5+Bcmynu1+XA7X1Ux74SBb6mqkcAs4Cr3H8DhXg9OoHTVfVoYDpwjojMAn4C3KKqhwA7gcvc118G7HTLb3FfN9BcDSz3PS+sa9HTnp8D+Qs4AXjS9/x64Ppc16uPfvdqYInv+UpgjPt4DLDSfXwHcGGm1w3EL+AR4KxCvx5AGfAGcDzO7N6QW574P4Mzx+gE93HIfZ3kuu4H8BqMw/lQcDrwT0AK7VoUZMbAXizUV0BGqWqd+3gLMMp9XDDXyE3/jwFepUCvh9t0sgioB54G3gV2qWrUfYn/901cC/d4IzC8b2ucVb8Avg7E3efDKbBrUaiBwWSgzseeghq/LCKDgIeAa1S1yX+skK6HqsZUdTrOp+WZwNQcVyknROTDQL2qLsx1XXKpUANDrxbqG+C2isgYAPd7vVs+4K+RiIRxgsKfVfVht7hgrweAqu4CnsVpLql01y6D1N83cS3c40OA7X1c1Ww5CThPRNbi7AVzOnArBXYtCjUwvA5McUcaFAGfBh7NcZ1y5VHgUvfxpTht7V75Z93ROLOARl8TS78nIgLcCSxX1Zt9hwrueohIlYhUuo9LcfpaluMEiAvcl6VfC+8aXQA842ZX/Z6qXq+q41S1Gue+8IyqXkShXYtcd3Lk6guYjbOK67vAN3Ndnz76ne8H6oAITjvpZTjtofOBVcA8YJj7WsEZufUu8DZQk+v6H+BrcTJOM9FiYJH7NbsQrwdwFPCmey2WAN92yycDrwGrgb8CxW55ift8tXt8cq5/hyxdlw8A/yzEa2FLYhhjjElRqE1JxhhjemCBwRhjTAoLDMYYY1JYYDDGGJPCAoMxxpgUFhiM2Qci8j0ROfMAvE/LgaiPMQeSDVc1JodEpEVVB+W6Hsb4WcZgjEtELnb3JVgkIne4C8u1iMgt7j4F80Wkyn3t3SJygfv4x+6+DotF5OduWbWIPOOWzReRCW75JBF5WUTeFpHvp/38/xGR191zvD0RykXkcXevhCUi8qm+vSqmEFlgMAYQkcOBTwEnqbOYXAy4CCgHalV1GvAc8J2084YD5wPTVPUowLvZ3wbc45b9GfilW34rcLuqvg9nFrr3Ph/E2ethJs6eCDNE5P04+2dsVtWjVfVI4IkD/ssbk8YCgzGOM4AZwOvu8tNn4CyDEAcecF/zJ5ylNPwagQ7gThH5ONDmlp8A3Oc+vtd33kk4S5N45Z4Pul9v4uyHMBUnULwNnCUiPxGRU1S1cT9/T2P2KLTnlxhTEATnE/71KYUiN6S9LqVTTlWjIjITJ5BcAHwZZ0XO3cnUsSfAj1T1jm4HnG1EZwPfF5H5qvq9Pby/MfvFMgZjHPOBC0RkJCT2fp6I83/EW1XzM8AC/0nufg5DVHUu8F/A0e6hl3BW5wSnSeoF9/GLaeWeJ4EvuO+HiIwVkZEichDQpqp/An4GDJi9pk3+sozBGEBVl4nIt4CnRCSAswLtVUArMNM9Vo/TD+FXATwiIiU4n/qvdcu/AvxBRP4HaAA+75ZfDdwnIv9LculmVPUpt5/jZWdFcFqAi4FDgJ+JSNyt05cO7G9uTHc2XNWY3bDhpKYQWVOSMcaYFJYxGGOMSWEZgzHGmBQWGIwxxqSwwGCMMSaFBQZjjDEpLDAYY4xJ8f8B0vrG3RgaTzkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 100.000, steps: 100\n",
            "Episode 2: reward: 108.000, steps: 108\n",
            "Episode 3: reward: 100.000, steps: 100\n",
            "Episode 4: reward: 122.000, steps: 122\n",
            "Episode 5: reward: 146.000, steps: 146\n",
            "Episode 6: reward: 102.000, steps: 102\n",
            "Episode 7: reward: 110.000, steps: 110\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 105.000, steps: 105\n",
            "Episode 10: reward: 116.000, steps: 116\n",
            "Episode 11: reward: 106.000, steps: 106\n",
            "Episode 12: reward: 106.000, steps: 106\n",
            "Episode 13: reward: 126.000, steps: 126\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 132.000, steps: 132\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 114.000, steps: 114\n",
            "Episode 18: reward: 104.000, steps: 104\n",
            "Episode 19: reward: 100.000, steps: 100\n",
            "Episode 20: reward: 136.000, steps: 136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa34b505150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCvKlzqw5I1K"
      },
      "source": [
        "## Implement DQN with BoltzmannQPolicy and LinearAnnaeledPolicy, changing the tau parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bezkVlP45YGv",
        "outputId": "9c42d91e-987c-4073-9b87-354939003bfd"
      },
      "source": [
        "from rl.policy import BoltzmannQPolicy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\r\n",
        "policy =  LinearAnnealedPolicy(inner_policy=BoltzmannQPolicy(),   # policy used to select actions\r\n",
        "                               attr='tau',                        # attribute in the inner policy to vary             \r\n",
        "                               value_max=1,                       # maximum value of attribute that is varying\r\n",
        "                               value_min=.1,                      # minimum value of attribute that is varying\r\n",
        "                               value_test=.05,                    # test if the value selected is < 0.05\r\n",
        "                               nb_steps=10000)                    # the number of steps between value_max and value_min\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   10/10000: episode: 1, duration: 0.234s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_tau: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   32/10000: episode: 2, duration: 1.130s, episode steps:  22, steps per second:  19, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.614277, mae: 0.757352, mean_q: 0.001071, mean_tau: 0.998110\n",
            "   42/10000: episode: 3, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.507593, mae: 0.622799, mean_q: 0.083170, mean_tau: 0.996715\n",
            "   91/10000: episode: 4, duration: 0.322s, episode steps:  49, steps per second: 152, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.388 [0.000, 1.000],  loss: 0.395888, mae: 0.565525, mean_q: 0.291105, mean_tau: 0.994060\n",
            "  101/10000: episode: 5, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.321278, mae: 0.530837, mean_q: 0.482307, mean_tau: 0.991405\n",
            "  124/10000: episode: 6, duration: 0.164s, episode steps:  23, steps per second: 141, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.281578, mae: 0.531939, mean_q: 0.589890, mean_tau: 0.989920\n",
            "  151/10000: episode: 7, duration: 0.179s, episode steps:  27, steps per second: 150, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 0.253355, mae: 0.590290, mean_q: 0.746286, mean_tau: 0.987670\n",
            "  160/10000: episode: 8, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.218396, mae: 0.600108, mean_q: 0.825976, mean_tau: 0.986050\n",
            "  175/10000: episode: 9, duration: 0.102s, episode steps:  15, steps per second: 147, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.221494, mae: 0.654010, mean_q: 0.926780, mean_tau: 0.984970\n",
            "  187/10000: episode: 10, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.204364, mae: 0.681082, mean_q: 1.022490, mean_tau: 0.983755\n",
            "  205/10000: episode: 11, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.194801, mae: 0.725763, mean_q: 1.166232, mean_tau: 0.982405\n",
            "  217/10000: episode: 12, duration: 0.087s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.176566, mae: 0.781040, mean_q: 1.259025, mean_tau: 0.981055\n",
            "  231/10000: episode: 13, duration: 0.101s, episode steps:  14, steps per second: 139, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.194515, mae: 0.840503, mean_q: 1.376695, mean_tau: 0.979885\n",
            "  244/10000: episode: 14, duration: 0.098s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.206226, mae: 0.894986, mean_q: 1.470882, mean_tau: 0.978670\n",
            "  258/10000: episode: 15, duration: 0.103s, episode steps:  14, steps per second: 136, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.199251, mae: 0.923201, mean_q: 1.541343, mean_tau: 0.977455\n",
            "  268/10000: episode: 16, duration: 0.074s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.195193, mae: 0.988726, mean_q: 1.682473, mean_tau: 0.976375\n",
            "  279/10000: episode: 17, duration: 0.084s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.204897, mae: 1.003428, mean_q: 1.703465, mean_tau: 0.975430\n",
            "  292/10000: episode: 18, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.212982, mae: 1.045069, mean_q: 1.814459, mean_tau: 0.974350\n",
            "  305/10000: episode: 19, duration: 0.091s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.206966, mae: 1.094049, mean_q: 1.894803, mean_tau: 0.973180\n",
            "  319/10000: episode: 20, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.213943, mae: 1.149389, mean_q: 2.034902, mean_tau: 0.971965\n",
            "  336/10000: episode: 21, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.238972, mae: 1.209334, mean_q: 2.131980, mean_tau: 0.970570\n",
            "  356/10000: episode: 22, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.215280, mae: 1.271415, mean_q: 2.253074, mean_tau: 0.968905\n",
            "  371/10000: episode: 23, duration: 0.105s, episode steps:  15, steps per second: 142, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.277272, mae: 1.343945, mean_q: 2.377615, mean_tau: 0.967330\n",
            "  385/10000: episode: 24, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.226579, mae: 1.379224, mean_q: 2.464814, mean_tau: 0.966025\n",
            "  418/10000: episode: 25, duration: 0.239s, episode steps:  33, steps per second: 138, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.697 [0.000, 1.000],  loss: 0.288727, mae: 1.462775, mean_q: 2.605337, mean_tau: 0.963910\n",
            "  429/10000: episode: 26, duration: 0.081s, episode steps:  11, steps per second: 136, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.259255, mae: 1.553099, mean_q: 2.859625, mean_tau: 0.961930\n",
            "  443/10000: episode: 27, duration: 0.101s, episode steps:  14, steps per second: 139, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.386512, mae: 1.649344, mean_q: 2.994625, mean_tau: 0.960805\n",
            "  454/10000: episode: 28, duration: 0.083s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.386262, mae: 1.675785, mean_q: 3.100544, mean_tau: 0.959680\n",
            "  466/10000: episode: 29, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.375384, mae: 1.705794, mean_q: 3.135430, mean_tau: 0.958645\n",
            "  487/10000: episode: 30, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.420527, mae: 1.800653, mean_q: 3.289424, mean_tau: 0.957160\n",
            "  503/10000: episode: 31, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.547851, mae: 1.875015, mean_q: 3.398604, mean_tau: 0.955495\n",
            "  519/10000: episode: 32, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.354220, mae: 1.882230, mean_q: 3.479503, mean_tau: 0.954055\n",
            "  531/10000: episode: 33, duration: 0.086s, episode steps:  12, steps per second: 140, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.385293, mae: 1.952389, mean_q: 3.623738, mean_tau: 0.952795\n",
            "  544/10000: episode: 34, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.375991, mae: 1.959073, mean_q: 3.689527, mean_tau: 0.951670\n",
            "  556/10000: episode: 35, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.385834, mae: 2.058880, mean_q: 3.865855, mean_tau: 0.950545\n",
            "  583/10000: episode: 36, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.559669, mae: 2.142130, mean_q: 3.972107, mean_tau: 0.948790\n",
            "  594/10000: episode: 37, duration: 0.083s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.615568, mae: 2.248438, mean_q: 4.076295, mean_tau: 0.947080\n",
            "  616/10000: episode: 38, duration: 0.149s, episode steps:  22, steps per second: 147, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.592889, mae: 2.277371, mean_q: 4.138933, mean_tau: 0.945595\n",
            "  632/10000: episode: 39, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.566709, mae: 2.345138, mean_q: 4.304793, mean_tau: 0.943885\n",
            "  643/10000: episode: 40, duration: 0.081s, episode steps:  11, steps per second: 136, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.515497, mae: 2.399572, mean_q: 4.434993, mean_tau: 0.942670\n",
            "  655/10000: episode: 41, duration: 0.086s, episode steps:  12, steps per second: 140, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.674490, mae: 2.481761, mean_q: 4.579145, mean_tau: 0.941635\n",
            "  664/10000: episode: 42, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.574617, mae: 2.487699, mean_q: 4.571021, mean_tau: 0.940690\n",
            "  674/10000: episode: 43, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.643826, mae: 2.511015, mean_q: 4.597764, mean_tau: 0.939835\n",
            "  697/10000: episode: 44, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.304 [0.000, 1.000],  loss: 0.550927, mae: 2.559356, mean_q: 4.745024, mean_tau: 0.938350\n",
            "  715/10000: episode: 45, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.628219, mae: 2.635712, mean_q: 4.873609, mean_tau: 0.936505\n",
            "  732/10000: episode: 46, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.627800, mae: 2.717927, mean_q: 5.043335, mean_tau: 0.934930\n",
            "  769/10000: episode: 47, duration: 0.253s, episode steps:  37, steps per second: 146, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.568535, mae: 2.791049, mean_q: 5.209253, mean_tau: 0.932500\n",
            "  783/10000: episode: 48, duration: 0.099s, episode steps:  14, steps per second: 142, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.748176, mae: 2.912349, mean_q: 5.413339, mean_tau: 0.930205\n",
            "  799/10000: episode: 49, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.629030, mae: 2.942509, mean_q: 5.449039, mean_tau: 0.928855\n",
            "  843/10000: episode: 50, duration: 0.312s, episode steps:  44, steps per second: 141, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.823254, mae: 3.070755, mean_q: 5.608950, mean_tau: 0.926155\n",
            "  856/10000: episode: 51, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.818923, mae: 3.141776, mean_q: 5.820736, mean_tau: 0.923590\n",
            "  868/10000: episode: 52, duration: 0.084s, episode steps:  12, steps per second: 143, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.595851, mae: 3.144453, mean_q: 5.904681, mean_tau: 0.922465\n",
            "  884/10000: episode: 53, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.679392, mae: 3.226959, mean_q: 5.990863, mean_tau: 0.921205\n",
            "  902/10000: episode: 54, duration: 0.125s, episode steps:  18, steps per second: 143, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.851843, mae: 3.298209, mean_q: 6.061081, mean_tau: 0.919675\n",
            "  924/10000: episode: 55, duration: 0.151s, episode steps:  22, steps per second: 145, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.690831, mae: 3.341988, mean_q: 6.201220, mean_tau: 0.917875\n",
            "  939/10000: episode: 56, duration: 0.103s, episode steps:  15, steps per second: 146, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.973212, mae: 3.433147, mean_q: 6.345036, mean_tau: 0.916210\n",
            "  949/10000: episode: 57, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.692370, mae: 3.429900, mean_q: 6.366026, mean_tau: 0.915085\n",
            "  968/10000: episode: 58, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.870673, mae: 3.509799, mean_q: 6.469633, mean_tau: 0.913780\n",
            "  981/10000: episode: 59, duration: 0.095s, episode steps:  13, steps per second: 136, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.567897, mae: 3.513156, mean_q: 6.508933, mean_tau: 0.912340\n",
            " 1022/10000: episode: 60, duration: 0.275s, episode steps:  41, steps per second: 149, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.415 [0.000, 1.000],  loss: 0.787143, mae: 3.642710, mean_q: 6.707716, mean_tau: 0.909910\n",
            " 1035/10000: episode: 61, duration: 0.092s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.020692, mae: 3.743819, mean_q: 6.886132, mean_tau: 0.907480\n",
            " 1090/10000: episode: 62, duration: 0.368s, episode steps:  55, steps per second: 149, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.822450, mae: 3.796812, mean_q: 7.023888, mean_tau: 0.904420\n",
            " 1132/10000: episode: 63, duration: 0.298s, episode steps:  42, steps per second: 141, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.816020, mae: 3.933432, mean_q: 7.317858, mean_tau: 0.900055\n",
            " 1149/10000: episode: 64, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.701013, mae: 4.008874, mean_q: 7.498033, mean_tau: 0.897400\n",
            " 1173/10000: episode: 65, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.874718, mae: 4.067081, mean_q: 7.615266, mean_tau: 0.895555\n",
            " 1200/10000: episode: 66, duration: 0.209s, episode steps:  27, steps per second: 129, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 0.768530, mae: 4.164381, mean_q: 7.829920, mean_tau: 0.893260\n",
            " 1212/10000: episode: 67, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.943264, mae: 4.245674, mean_q: 7.945084, mean_tau: 0.891505\n",
            " 1244/10000: episode: 68, duration: 0.219s, episode steps:  32, steps per second: 146, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.977553, mae: 4.269224, mean_q: 7.913263, mean_tau: 0.889525\n",
            " 1259/10000: episode: 69, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.797232, mae: 4.306137, mean_q: 8.023712, mean_tau: 0.887410\n",
            " 1276/10000: episode: 70, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.934700, mae: 4.398698, mean_q: 8.211013, mean_tau: 0.885970\n",
            " 1298/10000: episode: 71, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.783247, mae: 4.401251, mean_q: 8.292629, mean_tau: 0.884215\n",
            " 1340/10000: episode: 72, duration: 0.301s, episode steps:  42, steps per second: 140, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.869183, mae: 4.534661, mean_q: 8.534166, mean_tau: 0.881335\n",
            " 1356/10000: episode: 73, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.962175, mae: 4.587447, mean_q: 8.582626, mean_tau: 0.878725\n",
            " 1370/10000: episode: 74, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.311893, mae: 4.673766, mean_q: 8.694300, mean_tau: 0.877375\n",
            " 1401/10000: episode: 75, duration: 0.221s, episode steps:  31, steps per second: 140, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.409604, mae: 4.759288, mean_q: 8.821357, mean_tau: 0.875350\n",
            " 1413/10000: episode: 76, duration: 0.092s, episode steps:  12, steps per second: 131, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.900250, mae: 4.727253, mean_q: 8.888884, mean_tau: 0.873415\n",
            " 1438/10000: episode: 77, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.688437, mae: 4.826080, mean_q: 9.197793, mean_tau: 0.871750\n",
            " 1473/10000: episode: 78, duration: 0.237s, episode steps:  35, steps per second: 148, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.180757, mae: 4.933601, mean_q: 9.322745, mean_tau: 0.869050\n",
            " 1495/10000: episode: 79, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.982847, mae: 5.019499, mean_q: 9.507772, mean_tau: 0.866485\n",
            " 1521/10000: episode: 80, duration: 0.182s, episode steps:  26, steps per second: 143, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.219319, mae: 5.082543, mean_q: 9.597751, mean_tau: 0.864325\n",
            " 1552/10000: episode: 81, duration: 0.216s, episode steps:  31, steps per second: 143, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.130647, mae: 5.120931, mean_q: 9.610248, mean_tau: 0.861760\n",
            " 1575/10000: episode: 82, duration: 0.159s, episode steps:  23, steps per second: 145, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.064138, mae: 5.211047, mean_q: 9.933616, mean_tau: 0.859330\n",
            " 1714/10000: episode: 83, duration: 0.926s, episode steps: 139, steps per second: 150, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.336852, mae: 5.444971, mean_q: 10.323115, mean_tau: 0.852040\n",
            " 1758/10000: episode: 84, duration: 0.295s, episode steps:  44, steps per second: 149, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.271920, mae: 5.674551, mean_q: 10.794611, mean_tau: 0.843805\n",
            " 1793/10000: episode: 85, duration: 0.238s, episode steps:  35, steps per second: 147, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.653338, mae: 5.843460, mean_q: 11.088033, mean_tau: 0.840250\n",
            " 1810/10000: episode: 86, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.327526, mae: 5.868142, mean_q: 11.111073, mean_tau: 0.837910\n",
            " 1827/10000: episode: 87, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.887274, mae: 5.921604, mean_q: 11.200342, mean_tau: 0.836380\n",
            " 1876/10000: episode: 88, duration: 0.337s, episode steps:  49, steps per second: 145, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.351685, mae: 6.011953, mean_q: 11.515942, mean_tau: 0.833410\n",
            " 1911/10000: episode: 89, duration: 0.241s, episode steps:  35, steps per second: 145, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.529202, mae: 6.156410, mean_q: 11.821872, mean_tau: 0.829630\n",
            " 1938/10000: episode: 90, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.636542, mae: 6.222836, mean_q: 11.903164, mean_tau: 0.826840\n",
            " 1952/10000: episode: 91, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.690967, mae: 6.351605, mean_q: 12.137396, mean_tau: 0.824995\n",
            " 1992/10000: episode: 92, duration: 0.284s, episode steps:  40, steps per second: 141, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.651435, mae: 6.383468, mean_q: 12.150551, mean_tau: 0.822565\n",
            " 2021/10000: episode: 93, duration: 0.196s, episode steps:  29, steps per second: 148, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.898587, mae: 6.511583, mean_q: 12.373371, mean_tau: 0.819460\n",
            " 2070/10000: episode: 94, duration: 0.338s, episode steps:  49, steps per second: 145, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.753722, mae: 6.551413, mean_q: 12.442262, mean_tau: 0.815950\n",
            " 2102/10000: episode: 95, duration: 0.217s, episode steps:  32, steps per second: 148, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.804909, mae: 6.719727, mean_q: 12.837568, mean_tau: 0.812305\n",
            " 2144/10000: episode: 96, duration: 0.292s, episode steps:  42, steps per second: 144, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.620542, mae: 6.784707, mean_q: 13.069961, mean_tau: 0.808975\n",
            " 2191/10000: episode: 97, duration: 0.317s, episode steps:  47, steps per second: 148, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.917839, mae: 6.931083, mean_q: 13.275800, mean_tau: 0.804970\n",
            " 2208/10000: episode: 98, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.068603, mae: 7.029646, mean_q: 13.489083, mean_tau: 0.802090\n",
            " 2239/10000: episode: 99, duration: 0.218s, episode steps:  31, steps per second: 142, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.029635, mae: 7.093905, mean_q: 13.599728, mean_tau: 0.799930\n",
            " 2296/10000: episode: 100, duration: 0.395s, episode steps:  57, steps per second: 144, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.226772, mae: 7.208917, mean_q: 13.769636, mean_tau: 0.795970\n",
            " 2335/10000: episode: 101, duration: 0.266s, episode steps:  39, steps per second: 147, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.675082, mae: 7.337594, mean_q: 13.915530, mean_tau: 0.791650\n",
            " 2355/10000: episode: 102, duration: 0.137s, episode steps:  20, steps per second: 145, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.984981, mae: 7.372243, mean_q: 14.078161, mean_tau: 0.788995\n",
            " 2370/10000: episode: 103, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.438069, mae: 7.356673, mean_q: 14.211300, mean_tau: 0.787420\n",
            " 2402/10000: episode: 104, duration: 0.220s, episode steps:  32, steps per second: 146, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.311714, mae: 7.538685, mean_q: 14.450915, mean_tau: 0.785305\n",
            " 2443/10000: episode: 105, duration: 0.293s, episode steps:  41, steps per second: 140, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.406847, mae: 7.603348, mean_q: 14.504338, mean_tau: 0.782020\n",
            " 2478/10000: episode: 106, duration: 0.240s, episode steps:  35, steps per second: 146, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.795974, mae: 7.755627, mean_q: 14.793172, mean_tau: 0.778600\n",
            " 2519/10000: episode: 107, duration: 0.287s, episode steps:  41, steps per second: 143, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.649867, mae: 7.751309, mean_q: 14.746874, mean_tau: 0.775180\n",
            " 2567/10000: episode: 108, duration: 0.331s, episode steps:  48, steps per second: 145, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.688532, mae: 7.856561, mean_q: 15.163331, mean_tau: 0.771175\n",
            " 2604/10000: episode: 109, duration: 0.251s, episode steps:  37, steps per second: 148, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.047325, mae: 8.043924, mean_q: 15.309186, mean_tau: 0.767350\n",
            " 2658/10000: episode: 110, duration: 0.363s, episode steps:  54, steps per second: 149, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.560583, mae: 8.057054, mean_q: 15.384487, mean_tau: 0.763255\n",
            " 2714/10000: episode: 111, duration: 0.382s, episode steps:  56, steps per second: 147, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.511051, mae: 8.198663, mean_q: 15.734796, mean_tau: 0.758305\n",
            " 2730/10000: episode: 112, duration: 0.112s, episode steps:  16, steps per second: 142, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 3.888177, mae: 8.291574, mean_q: 15.738040, mean_tau: 0.755065\n",
            " 2779/10000: episode: 113, duration: 0.334s, episode steps:  49, steps per second: 147, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.875309, mae: 8.305387, mean_q: 15.879195, mean_tau: 0.752140\n",
            " 2816/10000: episode: 114, duration: 0.267s, episode steps:  37, steps per second: 138, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.423587, mae: 8.371199, mean_q: 16.091193, mean_tau: 0.748270\n",
            " 2842/10000: episode: 115, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.564416, mae: 8.578442, mean_q: 16.379836, mean_tau: 0.745435\n",
            " 2886/10000: episode: 116, duration: 0.311s, episode steps:  44, steps per second: 141, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.140260, mae: 8.602205, mean_q: 16.406153, mean_tau: 0.742285\n",
            " 2952/10000: episode: 117, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.454924, mae: 8.713918, mean_q: 16.787842, mean_tau: 0.737335\n",
            " 2985/10000: episode: 118, duration: 0.225s, episode steps:  33, steps per second: 147, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.856909, mae: 8.825157, mean_q: 16.996600, mean_tau: 0.732880\n",
            " 3066/10000: episode: 119, duration: 0.551s, episode steps:  81, steps per second: 147, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.222206, mae: 8.929909, mean_q: 17.138907, mean_tau: 0.727750\n",
            " 3115/10000: episode: 120, duration: 0.333s, episode steps:  49, steps per second: 147, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.301284, mae: 9.101962, mean_q: 17.514651, mean_tau: 0.721900\n",
            " 3148/10000: episode: 121, duration: 0.235s, episode steps:  33, steps per second: 140, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.096133, mae: 9.165248, mean_q: 17.488771, mean_tau: 0.718210\n",
            " 3172/10000: episode: 122, duration: 0.166s, episode steps:  24, steps per second: 144, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.485636, mae: 9.235774, mean_q: 17.848776, mean_tau: 0.715645\n",
            " 3247/10000: episode: 123, duration: 0.504s, episode steps:  75, steps per second: 149, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.793211, mae: 9.377820, mean_q: 18.183902, mean_tau: 0.711190\n",
            " 3269/10000: episode: 124, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.104932, mae: 9.438736, mean_q: 18.215475, mean_tau: 0.706825\n",
            " 3326/10000: episode: 125, duration: 0.390s, episode steps:  57, steps per second: 146, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.774189, mae: 9.536730, mean_q: 18.320899, mean_tau: 0.703270\n",
            " 3350/10000: episode: 126, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.841106, mae: 9.597818, mean_q: 18.700800, mean_tau: 0.699625\n",
            " 3385/10000: episode: 127, duration: 0.240s, episode steps:  35, steps per second: 146, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.924419, mae: 9.753741, mean_q: 18.612315, mean_tau: 0.696970\n",
            " 3428/10000: episode: 128, duration: 0.293s, episode steps:  43, steps per second: 147, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.832869, mae: 9.846022, mean_q: 18.939192, mean_tau: 0.693460\n",
            " 3478/10000: episode: 129, duration: 0.344s, episode steps:  50, steps per second: 145, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.536581, mae: 9.872420, mean_q: 19.012878, mean_tau: 0.689275\n",
            " 3513/10000: episode: 130, duration: 0.247s, episode steps:  35, steps per second: 142, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.174678, mae: 9.973276, mean_q: 19.323144, mean_tau: 0.685450\n",
            " 3636/10000: episode: 131, duration: 0.819s, episode steps: 123, steps per second: 150, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.817454, mae: 10.104528, mean_q: 19.464194, mean_tau: 0.678340\n",
            " 3657/10000: episode: 132, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.270595, mae: 10.200412, mean_q: 19.806527, mean_tau: 0.671860\n",
            " 3698/10000: episode: 133, duration: 0.282s, episode steps:  41, steps per second: 145, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.964392, mae: 10.376460, mean_q: 20.052972, mean_tau: 0.669070\n",
            " 3731/10000: episode: 134, duration: 0.229s, episode steps:  33, steps per second: 144, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.980273, mae: 10.354936, mean_q: 19.973881, mean_tau: 0.665740\n",
            " 3766/10000: episode: 135, duration: 0.245s, episode steps:  35, steps per second: 143, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.973158, mae: 10.479486, mean_q: 20.223032, mean_tau: 0.662680\n",
            " 3814/10000: episode: 136, duration: 0.328s, episode steps:  48, steps per second: 146, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.514826, mae: 10.571952, mean_q: 20.490657, mean_tau: 0.658945\n",
            " 3932/10000: episode: 137, duration: 0.793s, episode steps: 118, steps per second: 149, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.759504, mae: 10.717250, mean_q: 20.807159, mean_tau: 0.651475\n",
            " 3998/10000: episode: 138, duration: 0.442s, episode steps:  66, steps per second: 149, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.499109, mae: 10.911477, mean_q: 21.215177, mean_tau: 0.643195\n",
            " 4089/10000: episode: 139, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.976397, mae: 11.138973, mean_q: 21.605938, mean_tau: 0.636130\n",
            " 4148/10000: episode: 140, duration: 0.397s, episode steps:  59, steps per second: 149, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.046616, mae: 11.309433, mean_q: 22.083086, mean_tau: 0.629380\n",
            " 4235/10000: episode: 141, duration: 0.588s, episode steps:  87, steps per second: 148, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.353470, mae: 11.466107, mean_q: 22.211780, mean_tau: 0.622810\n",
            " 4335/10000: episode: 142, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.796024, mae: 11.628724, mean_q: 22.510535, mean_tau: 0.614395\n",
            " 4376/10000: episode: 143, duration: 0.283s, episode steps:  41, steps per second: 145, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4.158725, mae: 11.762649, mean_q: 22.822521, mean_tau: 0.608050\n",
            " 4430/10000: episode: 144, duration: 0.368s, episode steps:  54, steps per second: 147, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.434459, mae: 11.925420, mean_q: 23.174090, mean_tau: 0.603775\n",
            " 4475/10000: episode: 145, duration: 0.307s, episode steps:  45, steps per second: 147, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.783961, mae: 11.903662, mean_q: 23.233971, mean_tau: 0.599320\n",
            " 4585/10000: episode: 146, duration: 0.770s, episode steps: 110, steps per second: 143, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 3.576126, mae: 12.098902, mean_q: 23.663663, mean_tau: 0.592345\n",
            " 4630/10000: episode: 147, duration: 0.311s, episode steps:  45, steps per second: 145, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 4.921809, mae: 12.275428, mean_q: 23.879517, mean_tau: 0.585370\n",
            " 4769/10000: episode: 148, duration: 0.925s, episode steps: 139, steps per second: 150, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.605778, mae: 12.443603, mean_q: 24.233663, mean_tau: 0.577090\n",
            " 4860/10000: episode: 149, duration: 0.620s, episode steps:  91, steps per second: 147, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 5.823805, mae: 12.721774, mean_q: 24.712364, mean_tau: 0.566740\n",
            " 4933/10000: episode: 150, duration: 0.515s, episode steps:  73, steps per second: 142, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 5.052653, mae: 12.796509, mean_q: 24.978692, mean_tau: 0.559360\n",
            " 5002/10000: episode: 151, duration: 0.468s, episode steps:  69, steps per second: 147, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 5.238372, mae: 12.921286, mean_q: 25.191592, mean_tau: 0.552970\n",
            " 5070/10000: episode: 152, duration: 0.465s, episode steps:  68, steps per second: 146, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 3.909258, mae: 12.989424, mean_q: 25.475438, mean_tau: 0.546805\n",
            " 5159/10000: episode: 153, duration: 0.611s, episode steps:  89, steps per second: 146, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 4.396447, mae: 13.165014, mean_q: 25.785454, mean_tau: 0.539740\n",
            " 5227/10000: episode: 154, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.592155, mae: 13.378288, mean_q: 26.103148, mean_tau: 0.532675\n",
            " 5347/10000: episode: 155, duration: 0.812s, episode steps: 120, steps per second: 148, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.388621, mae: 13.465616, mean_q: 26.289467, mean_tau: 0.524215\n",
            " 5488/10000: episode: 156, duration: 0.949s, episode steps: 141, steps per second: 149, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.036818, mae: 13.779555, mean_q: 26.945123, mean_tau: 0.512470\n",
            " 5667/10000: episode: 157, duration: 1.199s, episode steps: 179, steps per second: 149, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.073593, mae: 14.034081, mean_q: 27.495966, mean_tau: 0.498070\n",
            " 5799/10000: episode: 158, duration: 0.883s, episode steps: 132, steps per second: 149, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.487431, mae: 14.315120, mean_q: 28.125034, mean_tau: 0.484075\n",
            " 5920/10000: episode: 159, duration: 0.827s, episode steps: 121, steps per second: 146, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.934440, mae: 14.478512, mean_q: 28.338729, mean_tau: 0.472690\n",
            " 6084/10000: episode: 160, duration: 1.099s, episode steps: 164, steps per second: 149, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 5.526422, mae: 14.789658, mean_q: 29.045267, mean_tau: 0.459865\n",
            " 6216/10000: episode: 161, duration: 0.899s, episode steps: 132, steps per second: 147, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 5.091597, mae: 14.962601, mean_q: 29.471077, mean_tau: 0.446545\n",
            " 6416/10000: episode: 162, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.854711, mae: 15.323647, mean_q: 30.300011, mean_tau: 0.431605\n",
            " 6616/10000: episode: 163, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.377705, mae: 15.745943, mean_q: 31.078722, mean_tau: 0.413605\n",
            " 6774/10000: episode: 164, duration: 1.048s, episode steps: 158, steps per second: 151, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.669175, mae: 16.120718, mean_q: 31.897534, mean_tau: 0.397495\n",
            " 6937/10000: episode: 165, duration: 1.083s, episode steps: 163, steps per second: 150, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.939502, mae: 16.304868, mean_q: 32.234214, mean_tau: 0.383050\n",
            " 7108/10000: episode: 166, duration: 1.138s, episode steps: 171, steps per second: 150, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.052528, mae: 16.533124, mean_q: 32.660054, mean_tau: 0.368020\n",
            " 7308/10000: episode: 167, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.600469, mae: 16.793054, mean_q: 33.169647, mean_tau: 0.351325\n",
            " 7488/10000: episode: 168, duration: 1.208s, episode steps: 180, steps per second: 149, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.856022, mae: 17.074125, mean_q: 33.831303, mean_tau: 0.334225\n",
            " 7688/10000: episode: 169, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.251588, mae: 17.372015, mean_q: 34.515437, mean_tau: 0.317125\n",
            " 7876/10000: episode: 170, duration: 1.260s, episode steps: 188, steps per second: 149, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 6.921306, mae: 17.693153, mean_q: 35.100920, mean_tau: 0.299665\n",
            " 8047/10000: episode: 171, duration: 1.135s, episode steps: 171, steps per second: 151, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.137757, mae: 17.924576, mean_q: 35.573779, mean_tau: 0.283510\n",
            " 8247/10000: episode: 172, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.365745, mae: 18.188169, mean_q: 36.205056, mean_tau: 0.266815\n",
            " 8439/10000: episode: 173, duration: 1.277s, episode steps: 192, steps per second: 150, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.456314, mae: 18.542845, mean_q: 36.933182, mean_tau: 0.249175\n",
            " 8634/10000: episode: 174, duration: 1.289s, episode steps: 195, steps per second: 151, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.886814, mae: 18.834761, mean_q: 37.510245, mean_tau: 0.231760\n",
            " 8823/10000: episode: 175, duration: 1.256s, episode steps: 189, steps per second: 150, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.262610, mae: 19.074430, mean_q: 38.088095, mean_tau: 0.214480\n",
            " 9001/10000: episode: 176, duration: 1.222s, episode steps: 178, steps per second: 146, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.930937, mae: 19.462586, mean_q: 38.791297, mean_tau: 0.197965\n",
            " 9193/10000: episode: 177, duration: 1.260s, episode steps: 192, steps per second: 152, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.562286, mae: 19.711813, mean_q: 39.390606, mean_tau: 0.181315\n",
            " 9393/10000: episode: 178, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.869222, mae: 20.003822, mean_q: 39.988373, mean_tau: 0.163675\n",
            " 9593/10000: episode: 179, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.375845, mae: 20.415233, mean_q: 40.827086, mean_tau: 0.145675\n",
            " 9793/10000: episode: 180, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.113983, mae: 20.725527, mean_q: 41.500891, mean_tau: 0.127675\n",
            " 9993/10000: episode: 181, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.884449, mae: 21.136661, mean_q: 42.331157, mean_tau: 0.109675\n",
            "done, took 69.369 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa34b599990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjzJMsq9-vB"
      },
      "source": [
        "## Double DQN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RtdWVWj_-DDm",
        "outputId": "55045cdf-bd8d-4397-8492-76b3ae00a022"
      },
      "source": [
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# define the policy (how we select the actions)\r\n",
        "policy = BoltzmannQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               enable_double_dqn=True,          # a boolean which enables a target network as a second network proposed by van Hasselt et al. to decrease overfitting\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   11/10000: episode: 1, duration: 1.291s, episode steps:  11, steps per second:   9, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
            "   33/10000: episode: 2, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.573160, mae: 0.575699, mean_q: -0.010063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   58/10000: episode: 3, duration: 0.201s, episode steps:  25, steps per second: 124, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.443116, mae: 0.525977, mean_q: 0.156823\n",
            "   92/10000: episode: 4, duration: 0.258s, episode steps:  34, steps per second: 132, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.389077, mae: 0.534718, mean_q: 0.322848\n",
            "  121/10000: episode: 5, duration: 0.221s, episode steps:  29, steps per second: 131, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.342883, mae: 0.561623, mean_q: 0.445189\n",
            "  180/10000: episode: 6, duration: 0.447s, episode steps:  59, steps per second: 132, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 0.299238, mae: 0.628068, mean_q: 0.654785\n",
            "  193/10000: episode: 7, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.231903, mae: 0.690935, mean_q: 0.896549\n",
            "  223/10000: episode: 8, duration: 0.229s, episode steps:  30, steps per second: 131, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.198146, mae: 0.739223, mean_q: 1.047636\n",
            "  233/10000: episode: 9, duration: 0.084s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.180665, mae: 0.789479, mean_q: 1.188066\n",
            "  273/10000: episode: 10, duration: 0.300s, episode steps:  40, steps per second: 133, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.156617, mae: 0.849934, mean_q: 1.348263\n",
            "  288/10000: episode: 11, duration: 0.130s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.120022, mae: 0.926118, mean_q: 1.535013\n",
            "  314/10000: episode: 12, duration: 0.201s, episode steps:  26, steps per second: 129, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.120419, mae: 0.992105, mean_q: 1.672543\n",
            "  323/10000: episode: 13, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.110106, mae: 1.060000, mean_q: 1.814453\n",
            "  352/10000: episode: 14, duration: 0.222s, episode steps:  29, steps per second: 131, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.131664, mae: 1.153982, mean_q: 1.972074\n",
            "  365/10000: episode: 15, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.103017, mae: 1.189231, mean_q: 2.091454\n",
            "  392/10000: episode: 16, duration: 0.206s, episode steps:  27, steps per second: 131, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.135780, mae: 1.281425, mean_q: 2.238052\n",
            "  414/10000: episode: 17, duration: 0.169s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.116576, mae: 1.353459, mean_q: 2.418872\n",
            "  425/10000: episode: 18, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.140771, mae: 1.439008, mean_q: 2.579509\n",
            "  449/10000: episode: 19, duration: 0.184s, episode steps:  24, steps per second: 131, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.160481, mae: 1.490736, mean_q: 2.669090\n",
            "  466/10000: episode: 20, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.157196, mae: 1.557403, mean_q: 2.803609\n",
            "  493/10000: episode: 21, duration: 0.215s, episode steps:  27, steps per second: 126, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.164395, mae: 1.637811, mean_q: 2.965550\n",
            "  503/10000: episode: 22, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.184887, mae: 1.718619, mean_q: 3.107027\n",
            "  520/10000: episode: 23, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.213913, mae: 1.768973, mean_q: 3.194953\n",
            "  544/10000: episode: 24, duration: 0.194s, episode steps:  24, steps per second: 124, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.165161, mae: 1.801122, mean_q: 3.321411\n",
            "  555/10000: episode: 25, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.174910, mae: 1.855496, mean_q: 3.418288\n",
            "  574/10000: episode: 26, duration: 0.148s, episode steps:  19, steps per second: 128, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.224258, mae: 1.947087, mean_q: 3.559648\n",
            "  595/10000: episode: 27, duration: 0.165s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.246331, mae: 2.015271, mean_q: 3.683404\n",
            "  616/10000: episode: 28, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.283893, mae: 2.101909, mean_q: 3.828551\n",
            "  659/10000: episode: 29, duration: 0.324s, episode steps:  43, steps per second: 133, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.266605, mae: 2.197453, mean_q: 4.040136\n",
            "  695/10000: episode: 30, duration: 0.283s, episode steps:  36, steps per second: 127, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.346308, mae: 2.331796, mean_q: 4.283240\n",
            "  718/10000: episode: 31, duration: 0.179s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.286103, mae: 2.415985, mean_q: 4.480673\n",
            "  727/10000: episode: 32, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.373279, mae: 2.500002, mean_q: 4.629921\n",
            "  755/10000: episode: 33, duration: 0.219s, episode steps:  28, steps per second: 128, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.388675, mae: 2.566556, mean_q: 4.742142\n",
            "  775/10000: episode: 34, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.324123, mae: 2.623342, mean_q: 4.867662\n",
            "  801/10000: episode: 35, duration: 0.209s, episode steps:  26, steps per second: 124, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.419150, mae: 2.735107, mean_q: 5.068336\n",
            "  823/10000: episode: 36, duration: 0.169s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.427008, mae: 2.814398, mean_q: 5.237018\n",
            "  848/10000: episode: 37, duration: 0.195s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.429322, mae: 2.877906, mean_q: 5.353196\n",
            "  870/10000: episode: 38, duration: 0.170s, episode steps:  22, steps per second: 129, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.537730, mae: 2.992075, mean_q: 5.534769\n",
            "  888/10000: episode: 39, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.464300, mae: 3.043266, mean_q: 5.700107\n",
            "  904/10000: episode: 40, duration: 0.127s, episode steps:  16, steps per second: 126, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.563041, mae: 3.123823, mean_q: 5.784548\n",
            "  931/10000: episode: 41, duration: 0.214s, episode steps:  27, steps per second: 126, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 0.468125, mae: 3.180732, mean_q: 5.958744\n",
            "  947/10000: episode: 42, duration: 0.129s, episode steps:  16, steps per second: 124, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.695560, mae: 3.283453, mean_q: 6.063348\n",
            "  986/10000: episode: 43, duration: 0.306s, episode steps:  39, steps per second: 128, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.594088, mae: 3.349127, mean_q: 6.252899\n",
            " 1006/10000: episode: 44, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.603724, mae: 3.454944, mean_q: 6.418381\n",
            " 1024/10000: episode: 45, duration: 0.149s, episode steps:  18, steps per second: 121, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.649225, mae: 3.512414, mean_q: 6.602062\n",
            " 1044/10000: episode: 46, duration: 0.167s, episode steps:  20, steps per second: 120, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.596998, mae: 3.585307, mean_q: 6.783441\n",
            " 1088/10000: episode: 47, duration: 0.351s, episode steps:  44, steps per second: 126, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.830764, mae: 3.731198, mean_q: 6.936258\n",
            " 1122/10000: episode: 48, duration: 0.277s, episode steps:  34, steps per second: 123, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.820461, mae: 3.848529, mean_q: 7.205369\n",
            " 1133/10000: episode: 49, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.655779, mae: 3.899359, mean_q: 7.361475\n",
            " 1161/10000: episode: 50, duration: 0.224s, episode steps:  28, steps per second: 125, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.718768, mae: 3.974602, mean_q: 7.553036\n",
            " 1207/10000: episode: 51, duration: 0.355s, episode steps:  46, steps per second: 130, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.587 [0.000, 1.000],  loss: 0.806046, mae: 4.090288, mean_q: 7.726357\n",
            " 1243/10000: episode: 52, duration: 0.279s, episode steps:  36, steps per second: 129, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.920429, mae: 4.225841, mean_q: 7.965479\n",
            " 1262/10000: episode: 53, duration: 0.147s, episode steps:  19, steps per second: 129, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.849291, mae: 4.275196, mean_q: 8.096535\n",
            " 1274/10000: episode: 54, duration: 0.109s, episode steps:  12, steps per second: 110, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.695170, mae: 4.324626, mean_q: 8.213361\n",
            " 1292/10000: episode: 55, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.154357, mae: 4.439062, mean_q: 8.306915\n",
            " 1320/10000: episode: 56, duration: 0.222s, episode steps:  28, steps per second: 126, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.959182, mae: 4.483494, mean_q: 8.473494\n",
            " 1336/10000: episode: 57, duration: 0.127s, episode steps:  16, steps per second: 126, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.056435, mae: 4.528826, mean_q: 8.559227\n",
            " 1360/10000: episode: 58, duration: 0.191s, episode steps:  24, steps per second: 126, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.959134, mae: 4.587460, mean_q: 8.727645\n",
            " 1395/10000: episode: 59, duration: 0.272s, episode steps:  35, steps per second: 129, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.971858, mae: 4.678660, mean_q: 8.843534\n",
            " 1411/10000: episode: 60, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.950522, mae: 4.757151, mean_q: 9.073417\n",
            " 1455/10000: episode: 61, duration: 0.342s, episode steps:  44, steps per second: 129, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.110848, mae: 4.879453, mean_q: 9.281402\n",
            " 1488/10000: episode: 62, duration: 0.257s, episode steps:  33, steps per second: 128, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 0.918723, mae: 4.962417, mean_q: 9.509043\n",
            " 1509/10000: episode: 63, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.930278, mae: 5.057985, mean_q: 9.705224\n",
            " 1530/10000: episode: 64, duration: 0.172s, episode steps:  21, steps per second: 122, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.336000, mae: 5.164571, mean_q: 9.902537\n",
            " 1549/10000: episode: 65, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 1.117435, mae: 5.195654, mean_q: 9.939659\n",
            " 1656/10000: episode: 66, duration: 0.802s, episode steps: 107, steps per second: 133, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.131149, mae: 5.385746, mean_q: 10.345807\n",
            " 1673/10000: episode: 67, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.913272, mae: 5.641798, mean_q: 10.713418\n",
            " 1734/10000: episode: 68, duration: 0.464s, episode steps:  61, steps per second: 132, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.464179, mae: 5.709174, mean_q: 10.896171\n",
            " 1749/10000: episode: 69, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.608295, mae: 5.823355, mean_q: 11.092512\n",
            " 1890/10000: episode: 70, duration: 1.050s, episode steps: 141, steps per second: 134, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 1.422132, mae: 6.059709, mean_q: 11.655431\n",
            " 1919/10000: episode: 71, duration: 0.223s, episode steps:  29, steps per second: 130, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.154605, mae: 6.255358, mean_q: 12.129889\n",
            " 1929/10000: episode: 72, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.602624, mae: 6.343342, mean_q: 12.218458\n",
            " 1956/10000: episode: 73, duration: 0.217s, episode steps:  27, steps per second: 125, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.743662, mae: 6.391534, mean_q: 12.267550\n",
            " 1971/10000: episode: 74, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.345094, mae: 6.390550, mean_q: 12.360136\n",
            " 1983/10000: episode: 75, duration: 0.101s, episode steps:  12, steps per second: 119, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.042654, mae: 6.475027, mean_q: 12.401493\n",
            " 2019/10000: episode: 76, duration: 0.282s, episode steps:  36, steps per second: 128, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.053215, mae: 6.550728, mean_q: 12.471510\n",
            " 2036/10000: episode: 77, duration: 0.134s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.873267, mae: 6.643552, mean_q: 12.723993\n",
            " 2120/10000: episode: 78, duration: 0.628s, episode steps:  84, steps per second: 134, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.888959, mae: 6.715016, mean_q: 12.835840\n",
            " 2169/10000: episode: 79, duration: 0.366s, episode steps:  49, steps per second: 134, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 1.278965, mae: 6.811332, mean_q: 13.255594\n",
            " 2201/10000: episode: 80, duration: 0.259s, episode steps:  32, steps per second: 123, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.417928, mae: 6.959378, mean_q: 13.517611\n",
            " 2235/10000: episode: 81, duration: 0.274s, episode steps:  34, steps per second: 124, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.075538, mae: 7.101122, mean_q: 13.705677\n",
            " 2265/10000: episode: 82, duration: 0.236s, episode steps:  30, steps per second: 127, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.774469, mae: 7.227413, mean_q: 13.710676\n",
            " 2306/10000: episode: 83, duration: 0.316s, episode steps:  41, steps per second: 130, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.634 [0.000, 1.000],  loss: 1.564007, mae: 7.231909, mean_q: 14.091553\n",
            " 2323/10000: episode: 84, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.956804, mae: 7.315521, mean_q: 14.151147\n",
            " 2359/10000: episode: 85, duration: 0.286s, episode steps:  36, steps per second: 126, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.250171, mae: 7.431288, mean_q: 14.361097\n",
            " 2404/10000: episode: 86, duration: 0.343s, episode steps:  45, steps per second: 131, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.518565, mae: 7.495869, mean_q: 14.374564\n",
            " 2428/10000: episode: 87, duration: 0.205s, episode steps:  24, steps per second: 117, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.353456, mae: 7.594120, mean_q: 14.665675\n",
            " 2507/10000: episode: 88, duration: 0.602s, episode steps:  79, steps per second: 131, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.443679, mae: 7.716073, mean_q: 14.890786\n",
            " 2539/10000: episode: 89, duration: 0.251s, episode steps:  32, steps per second: 128, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.614811, mae: 7.891459, mean_q: 15.262226\n",
            " 2654/10000: episode: 90, duration: 0.860s, episode steps: 115, steps per second: 134, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.502036, mae: 8.033285, mean_q: 15.542641\n",
            " 2714/10000: episode: 91, duration: 0.451s, episode steps:  60, steps per second: 133, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.978700, mae: 8.220951, mean_q: 16.045195\n",
            " 2746/10000: episode: 92, duration: 0.244s, episode steps:  32, steps per second: 131, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.212292, mae: 8.356874, mean_q: 16.302999\n",
            " 2766/10000: episode: 93, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.313843, mae: 8.468122, mean_q: 16.498339\n",
            " 2806/10000: episode: 94, duration: 0.307s, episode steps:  40, steps per second: 130, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.701314, mae: 8.494412, mean_q: 16.479893\n",
            " 2906/10000: episode: 95, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.901218, mae: 8.709507, mean_q: 16.876066\n",
            " 2955/10000: episode: 96, duration: 0.376s, episode steps:  49, steps per second: 130, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.033035, mae: 8.964463, mean_q: 17.370192\n",
            " 2999/10000: episode: 97, duration: 0.335s, episode steps:  44, steps per second: 131, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.971302, mae: 9.045739, mean_q: 17.745550\n",
            " 3095/10000: episode: 98, duration: 0.739s, episode steps:  96, steps per second: 130, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.317165, mae: 9.170422, mean_q: 17.767223\n",
            " 3229/10000: episode: 99, duration: 1.005s, episode steps: 134, steps per second: 133, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.703947, mae: 9.434198, mean_q: 18.425615\n",
            " 3337/10000: episode: 100, duration: 0.815s, episode steps: 108, steps per second: 132, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.318650, mae: 9.731750, mean_q: 18.971224\n",
            " 3393/10000: episode: 101, duration: 0.425s, episode steps:  56, steps per second: 132, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.981214, mae: 9.939699, mean_q: 19.463470\n",
            " 3464/10000: episode: 102, duration: 0.538s, episode steps:  71, steps per second: 132, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.766448, mae: 10.131968, mean_q: 19.893469\n",
            " 3524/10000: episode: 103, duration: 0.453s, episode steps:  60, steps per second: 132, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.410782, mae: 10.281994, mean_q: 20.305046\n",
            " 3601/10000: episode: 104, duration: 0.595s, episode steps:  77, steps per second: 129, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.451524, mae: 10.472768, mean_q: 20.570259\n",
            " 3706/10000: episode: 105, duration: 0.788s, episode steps: 105, steps per second: 133, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.652579, mae: 10.756058, mean_q: 21.031624\n",
            " 3750/10000: episode: 106, duration: 0.342s, episode steps:  44, steps per second: 129, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.003868, mae: 10.911432, mean_q: 21.463034\n",
            " 3830/10000: episode: 107, duration: 0.624s, episode steps:  80, steps per second: 128, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 3.710826, mae: 11.045614, mean_q: 21.719828\n",
            " 3845/10000: episode: 108, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.400283, mae: 11.020455, mean_q: 21.693825\n",
            " 3886/10000: episode: 109, duration: 0.318s, episode steps:  41, steps per second: 129, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.150157, mae: 11.261150, mean_q: 22.054070\n",
            " 3939/10000: episode: 110, duration: 0.406s, episode steps:  53, steps per second: 130, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 4.386282, mae: 11.412080, mean_q: 22.324022\n",
            " 4007/10000: episode: 111, duration: 0.533s, episode steps:  68, steps per second: 128, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.843570, mae: 11.458110, mean_q: 22.432785\n",
            " 4043/10000: episode: 112, duration: 0.273s, episode steps:  36, steps per second: 132, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.612777, mae: 11.552945, mean_q: 22.685286\n",
            " 4086/10000: episode: 113, duration: 0.331s, episode steps:  43, steps per second: 130, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.011197, mae: 11.707670, mean_q: 22.846245\n",
            " 4165/10000: episode: 114, duration: 0.592s, episode steps:  79, steps per second: 133, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.529859, mae: 11.890139, mean_q: 23.279486\n",
            " 4214/10000: episode: 115, duration: 0.377s, episode steps:  49, steps per second: 130, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.773612, mae: 12.002779, mean_q: 23.533827\n",
            " 4284/10000: episode: 116, duration: 0.531s, episode steps:  70, steps per second: 132, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.427239, mae: 12.197985, mean_q: 23.952883\n",
            " 4430/10000: episode: 117, duration: 1.102s, episode steps: 146, steps per second: 132, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.392869, mae: 12.379803, mean_q: 24.306139\n",
            " 4447/10000: episode: 118, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.342015, mae: 12.601374, mean_q: 24.693108\n",
            " 4525/10000: episode: 119, duration: 0.594s, episode steps:  78, steps per second: 131, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.732150, mae: 12.667023, mean_q: 24.859791\n",
            " 4626/10000: episode: 120, duration: 0.748s, episode steps: 101, steps per second: 135, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.418071, mae: 12.833746, mean_q: 25.286449\n",
            " 4724/10000: episode: 121, duration: 0.732s, episode steps:  98, steps per second: 134, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.943031, mae: 13.068148, mean_q: 25.730780\n",
            " 4792/10000: episode: 122, duration: 0.517s, episode steps:  68, steps per second: 131, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.710627, mae: 13.337374, mean_q: 26.290817\n",
            " 4951/10000: episode: 123, duration: 1.169s, episode steps: 159, steps per second: 136, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.794692, mae: 13.513798, mean_q: 26.652590\n",
            " 5011/10000: episode: 124, duration: 0.458s, episode steps:  60, steps per second: 131, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.836789, mae: 13.621672, mean_q: 26.763502\n",
            " 5097/10000: episode: 125, duration: 0.650s, episode steps:  86, steps per second: 132, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.225516, mae: 13.825140, mean_q: 27.395922\n",
            " 5152/10000: episode: 126, duration: 0.434s, episode steps:  55, steps per second: 127, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 6.417521, mae: 14.046591, mean_q: 27.588911\n",
            " 5218/10000: episode: 127, duration: 0.504s, episode steps:  66, steps per second: 131, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 5.490014, mae: 14.049847, mean_q: 27.685078\n",
            " 5276/10000: episode: 128, duration: 0.445s, episode steps:  58, steps per second: 130, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 4.989007, mae: 14.321133, mean_q: 28.286251\n",
            " 5354/10000: episode: 129, duration: 0.594s, episode steps:  78, steps per second: 131, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.379145, mae: 14.362974, mean_q: 28.313145\n",
            " 5554/10000: episode: 130, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.026777, mae: 14.610991, mean_q: 28.953859\n",
            " 5694/10000: episode: 131, duration: 1.040s, episode steps: 140, steps per second: 135, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 5.220786, mae: 14.974616, mean_q: 29.710484\n",
            " 5777/10000: episode: 132, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 6.112051, mae: 15.236574, mean_q: 30.153227\n",
            " 5906/10000: episode: 133, duration: 0.962s, episode steps: 129, steps per second: 134, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.353834, mae: 15.390899, mean_q: 30.516554\n",
            " 6017/10000: episode: 134, duration: 0.842s, episode steps: 111, steps per second: 132, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.453662, mae: 15.741758, mean_q: 31.127768\n",
            " 6113/10000: episode: 135, duration: 0.725s, episode steps:  96, steps per second: 132, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.256998, mae: 15.820043, mean_q: 31.322916\n",
            " 6251/10000: episode: 136, duration: 1.037s, episode steps: 138, steps per second: 133, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 5.516149, mae: 16.047728, mean_q: 31.765852\n",
            " 6451/10000: episode: 137, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.739828, mae: 16.549171, mean_q: 32.743484\n",
            " 6528/10000: episode: 138, duration: 0.619s, episode steps:  77, steps per second: 124, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.295970, mae: 16.688538, mean_q: 32.951469\n",
            " 6673/10000: episode: 139, duration: 1.104s, episode steps: 145, steps per second: 131, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6.580569, mae: 16.923815, mean_q: 33.586185\n",
            " 6873/10000: episode: 140, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.598102, mae: 17.218115, mean_q: 34.172756\n",
            " 7016/10000: episode: 141, duration: 1.059s, episode steps: 143, steps per second: 135, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.282228, mae: 17.555401, mean_q: 34.915581\n",
            " 7192/10000: episode: 142, duration: 1.321s, episode steps: 176, steps per second: 133, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6.581635, mae: 17.829451, mean_q: 35.451725\n",
            " 7378/10000: episode: 143, duration: 1.361s, episode steps: 186, steps per second: 137, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 7.545147, mae: 18.220427, mean_q: 36.268028\n",
            " 7578/10000: episode: 144, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 7.167343, mae: 18.507610, mean_q: 36.850796\n",
            " 7731/10000: episode: 145, duration: 1.141s, episode steps: 153, steps per second: 134, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.233824, mae: 18.764978, mean_q: 37.424110\n",
            " 7931/10000: episode: 146, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.237178, mae: 19.135059, mean_q: 38.334827\n",
            " 8131/10000: episode: 147, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.784323, mae: 19.576035, mean_q: 39.119209\n",
            " 8302/10000: episode: 148, duration: 1.274s, episode steps: 171, steps per second: 134, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 7.396566, mae: 19.972424, mean_q: 39.929195\n",
            " 8502/10000: episode: 149, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.690916, mae: 20.334204, mean_q: 40.693661\n",
            " 8689/10000: episode: 150, duration: 1.401s, episode steps: 187, steps per second: 133, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 8.528739, mae: 20.707375, mean_q: 41.320488\n",
            " 8889/10000: episode: 151, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.657955, mae: 20.967257, mean_q: 41.960148\n",
            " 9074/10000: episode: 152, duration: 1.399s, episode steps: 185, steps per second: 132, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 9.068890, mae: 21.367220, mean_q: 42.643234\n",
            " 9274/10000: episode: 153, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.278666, mae: 21.659807, mean_q: 43.308662\n",
            " 9474/10000: episode: 154, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 8.331222, mae: 22.035498, mean_q: 44.115047\n",
            " 9673/10000: episode: 155, duration: 1.466s, episode steps: 199, steps per second: 136, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 8.833369, mae: 22.512274, mean_q: 45.085430\n",
            " 9868/10000: episode: 156, duration: 1.461s, episode steps: 195, steps per second: 133, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 9.193601, mae: 22.791058, mean_q: 45.615025\n",
            "done, took 77.286 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 196.000, steps: 196\n",
            "Episode 5: reward: 194.000, steps: 194\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 187.000, steps: 187\n",
            "Episode 9: reward: 198.000, steps: 198\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 190.000, steps: 190\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 194.000, steps: 194\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 197.000, steps: 197\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa34ac51d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyw0K86m-LGO"
      },
      "source": [
        "## Dueling DQN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HNY4Tur2-SAA",
        "outputId": "60b151f1-0f6d-437a-bf75-6c7cba76409d"
      },
      "source": [
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# define the policy (how we select the actions)\r\n",
        "policy = BoltzmannQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               enable_dueling_network=True,     # a boolean which enables duelling architecture proposed by Mnih et al.\r\n",
        "               dueling_type='avg',              # if enable_duelling_dqn is set to true, a type of duellin garchitecture must be chosen. 'avg' is recommended\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_8 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   11/10000: episode: 1, duration: 1.536s, episode steps:  11, steps per second:   7, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
            "   27/10000: episode: 2, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.796586, mae: 0.750080, mean_q: -0.043674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   45/10000: episode: 3, duration: 0.141s, episode steps:  18, steps per second: 128, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.557516, mae: 0.623282, mean_q: 0.040503\n",
            "   63/10000: episode: 4, duration: 0.136s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.420971, mae: 0.573569, mean_q: 0.199130\n",
            "  117/10000: episode: 5, duration: 0.398s, episode steps:  54, steps per second: 136, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.319741, mae: 0.548318, mean_q: 0.367938\n",
            "  144/10000: episode: 6, duration: 0.194s, episode steps:  27, steps per second: 139, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.268376, mae: 0.597034, mean_q: 0.586644\n",
            "  169/10000: episode: 7, duration: 0.182s, episode steps:  25, steps per second: 138, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.241627, mae: 0.619168, mean_q: 0.680717\n",
            "  199/10000: episode: 8, duration: 0.220s, episode steps:  30, steps per second: 137, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.206752, mae: 0.691233, mean_q: 0.866780\n",
            "  210/10000: episode: 9, duration: 0.083s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.179922, mae: 0.754138, mean_q: 1.055955\n",
            "  221/10000: episode: 10, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.166750, mae: 0.795004, mean_q: 1.169014\n",
            "  244/10000: episode: 11, duration: 0.172s, episode steps:  23, steps per second: 134, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.175017, mae: 0.847709, mean_q: 1.301078\n",
            "  282/10000: episode: 12, duration: 0.295s, episode steps:  38, steps per second: 129, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.342 [0.000, 1.000],  loss: 0.142746, mae: 0.917560, mean_q: 1.509869\n",
            "  302/10000: episode: 13, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.182089, mae: 1.029377, mean_q: 1.802512\n",
            "  326/10000: episode: 14, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.154143, mae: 1.133592, mean_q: 2.006436\n",
            "  358/10000: episode: 15, duration: 0.230s, episode steps:  32, steps per second: 139, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.210928, mae: 1.249172, mean_q: 2.281923\n",
            "  371/10000: episode: 16, duration: 0.094s, episode steps:  13, steps per second: 138, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.178291, mae: 1.339345, mean_q: 2.475497\n",
            "  393/10000: episode: 17, duration: 0.169s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.185119, mae: 1.374642, mean_q: 2.564766\n",
            "  403/10000: episode: 18, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.195110, mae: 1.429848, mean_q: 2.680469\n",
            "  415/10000: episode: 19, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.315701, mae: 1.521076, mean_q: 2.808987\n",
            "  440/10000: episode: 20, duration: 0.182s, episode steps:  25, steps per second: 137, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.274362, mae: 1.604855, mean_q: 2.977870\n",
            "  453/10000: episode: 21, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.217689, mae: 1.659767, mean_q: 3.123910\n",
            "  465/10000: episode: 22, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.211006, mae: 1.689563, mean_q: 3.229401\n",
            "  488/10000: episode: 23, duration: 0.170s, episode steps:  23, steps per second: 136, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.319519, mae: 1.831185, mean_q: 3.463552\n",
            "  500/10000: episode: 24, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.241063, mae: 1.905622, mean_q: 3.629910\n",
            "  510/10000: episode: 25, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.365862, mae: 1.994836, mean_q: 3.817986\n",
            "  543/10000: episode: 26, duration: 0.258s, episode steps:  33, steps per second: 128, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.357698, mae: 2.058458, mean_q: 3.881722\n",
            "  555/10000: episode: 27, duration: 0.088s, episode steps:  12, steps per second: 136, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.367876, mae: 2.167225, mean_q: 4.119515\n",
            "  566/10000: episode: 28, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.414717, mae: 2.216810, mean_q: 4.183535\n",
            "  582/10000: episode: 29, duration: 0.124s, episode steps:  16, steps per second: 129, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.454894, mae: 2.293203, mean_q: 4.294575\n",
            "  595/10000: episode: 30, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.487602, mae: 2.375979, mean_q: 4.405387\n",
            "  608/10000: episode: 31, duration: 0.096s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.244756, mae: 2.375386, mean_q: 4.537386\n",
            "  623/10000: episode: 32, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.368312, mae: 2.430384, mean_q: 4.679893\n",
            "  634/10000: episode: 33, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.358055, mae: 2.510484, mean_q: 4.811356\n",
            "  656/10000: episode: 34, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.427547, mae: 2.597696, mean_q: 4.906121\n",
            "  667/10000: episode: 35, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.594277, mae: 2.691389, mean_q: 5.038201\n",
            "  680/10000: episode: 36, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.644339, mae: 2.781474, mean_q: 5.151494\n",
            "  696/10000: episode: 37, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.443065, mae: 2.772248, mean_q: 5.182524\n",
            "  731/10000: episode: 38, duration: 0.277s, episode steps:  35, steps per second: 126, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.585097, mae: 2.907670, mean_q: 5.442605\n",
            "  764/10000: episode: 39, duration: 0.243s, episode steps:  33, steps per second: 136, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 0.601112, mae: 3.020456, mean_q: 5.646469\n",
            "  794/10000: episode: 40, duration: 0.231s, episode steps:  30, steps per second: 130, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.530114, mae: 3.130554, mean_q: 5.939885\n",
            "  805/10000: episode: 41, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.521487, mae: 3.188408, mean_q: 6.061745\n",
            "  847/10000: episode: 42, duration: 0.305s, episode steps:  42, steps per second: 138, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.581003, mae: 3.307994, mean_q: 6.281084\n",
            "  864/10000: episode: 43, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.665491, mae: 3.423529, mean_q: 6.470582\n",
            "  885/10000: episode: 44, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.744502, mae: 3.500222, mean_q: 6.531553\n",
            "  910/10000: episode: 45, duration: 0.177s, episode steps:  25, steps per second: 141, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.575355, mae: 3.554352, mean_q: 6.771645\n",
            "  935/10000: episode: 46, duration: 0.188s, episode steps:  25, steps per second: 133, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.694296, mae: 3.681448, mean_q: 6.998810\n",
            "  950/10000: episode: 47, duration: 0.109s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.650298, mae: 3.735881, mean_q: 7.068687\n",
            "  965/10000: episode: 48, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.690234, mae: 3.785461, mean_q: 7.175219\n",
            "  989/10000: episode: 49, duration: 0.177s, episode steps:  24, steps per second: 135, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.841778, mae: 3.877508, mean_q: 7.263622\n",
            " 1047/10000: episode: 50, duration: 0.429s, episode steps:  58, steps per second: 135, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.745105, mae: 3.994787, mean_q: 7.579925\n",
            " 1061/10000: episode: 51, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.731042, mae: 4.122967, mean_q: 7.840882\n",
            " 1151/10000: episode: 52, duration: 0.637s, episode steps:  90, steps per second: 141, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.837542, mae: 4.300225, mean_q: 8.153540\n",
            " 1164/10000: episode: 53, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.012895, mae: 4.512023, mean_q: 8.533803\n",
            " 1183/10000: episode: 54, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.844126, mae: 4.535103, mean_q: 8.616153\n",
            " 1214/10000: episode: 55, duration: 0.234s, episode steps:  31, steps per second: 132, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.982290, mae: 4.609865, mean_q: 8.754889\n",
            " 1234/10000: episode: 56, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.947059, mae: 4.675110, mean_q: 8.897681\n",
            " 1247/10000: episode: 57, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.005669, mae: 4.776313, mean_q: 9.147634\n",
            " 1263/10000: episode: 58, duration: 0.117s, episode steps:  16, steps per second: 136, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.642865, mae: 4.763986, mean_q: 9.187763\n",
            " 1280/10000: episode: 59, duration: 0.131s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.881104, mae: 4.825890, mean_q: 9.279531\n",
            " 1295/10000: episode: 60, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.085204, mae: 4.936356, mean_q: 9.440084\n",
            " 1368/10000: episode: 61, duration: 0.531s, episode steps:  73, steps per second: 137, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.060658, mae: 5.041723, mean_q: 9.605634\n",
            " 1405/10000: episode: 62, duration: 0.279s, episode steps:  37, steps per second: 133, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.160843, mae: 5.239705, mean_q: 10.029289\n",
            " 1417/10000: episode: 63, duration: 0.091s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.304709, mae: 5.328910, mean_q: 10.165261\n",
            " 1512/10000: episode: 64, duration: 0.681s, episode steps:  95, steps per second: 140, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.020452, mae: 5.454734, mean_q: 10.476672\n",
            " 1524/10000: episode: 65, duration: 0.091s, episode steps:  12, steps per second: 131, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.904886, mae: 5.659965, mean_q: 10.969304\n",
            " 1541/10000: episode: 66, duration: 0.129s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.113479, mae: 5.703033, mean_q: 10.982868\n",
            " 1590/10000: episode: 67, duration: 0.355s, episode steps:  49, steps per second: 138, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.408983, mae: 5.790410, mean_q: 11.066109\n",
            " 1632/10000: episode: 68, duration: 0.308s, episode steps:  42, steps per second: 136, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.449619, mae: 5.946659, mean_q: 11.398535\n",
            " 1661/10000: episode: 69, duration: 0.214s, episode steps:  29, steps per second: 135, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.811768, mae: 6.031814, mean_q: 11.428034\n",
            " 1680/10000: episode: 70, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.808143, mae: 6.151757, mean_q: 11.642180\n",
            " 1705/10000: episode: 71, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.776654, mae: 6.185481, mean_q: 11.821087\n",
            " 1747/10000: episode: 72, duration: 0.315s, episode steps:  42, steps per second: 133, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.174477, mae: 6.210752, mean_q: 11.991327\n",
            " 1760/10000: episode: 73, duration: 0.097s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.841681, mae: 6.377366, mean_q: 12.255209\n",
            " 1786/10000: episode: 74, duration: 0.195s, episode steps:  26, steps per second: 133, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.519464, mae: 6.400109, mean_q: 12.304047\n",
            " 1824/10000: episode: 75, duration: 0.277s, episode steps:  38, steps per second: 137, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.639473, mae: 6.491245, mean_q: 12.446607\n",
            " 1849/10000: episode: 76, duration: 0.183s, episode steps:  25, steps per second: 137, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.264819, mae: 6.631979, mean_q: 12.603744\n",
            " 1866/10000: episode: 77, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.774767, mae: 6.613820, mean_q: 12.571342\n",
            " 1911/10000: episode: 78, duration: 0.330s, episode steps:  45, steps per second: 136, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.589941, mae: 6.688360, mean_q: 12.915051\n",
            " 1967/10000: episode: 79, duration: 0.397s, episode steps:  56, steps per second: 141, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.737572, mae: 6.847708, mean_q: 13.202545\n",
            " 2001/10000: episode: 80, duration: 0.241s, episode steps:  34, steps per second: 141, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.865949, mae: 7.000560, mean_q: 13.491506\n",
            " 2039/10000: episode: 81, duration: 0.290s, episode steps:  38, steps per second: 131, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.974981, mae: 7.102252, mean_q: 13.700075\n",
            " 2078/10000: episode: 82, duration: 0.284s, episode steps:  39, steps per second: 137, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.832579, mae: 7.152517, mean_q: 13.718365\n",
            " 2119/10000: episode: 83, duration: 0.298s, episode steps:  41, steps per second: 138, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.193004, mae: 7.280963, mean_q: 13.967853\n",
            " 2186/10000: episode: 84, duration: 0.495s, episode steps:  67, steps per second: 135, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.229856, mae: 7.434049, mean_q: 14.299289\n",
            " 2208/10000: episode: 85, duration: 0.161s, episode steps:  22, steps per second: 137, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.846592, mae: 7.577181, mean_q: 14.517777\n",
            " 2281/10000: episode: 86, duration: 0.527s, episode steps:  73, steps per second: 139, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.202819, mae: 7.639381, mean_q: 14.715159\n",
            " 2324/10000: episode: 87, duration: 0.314s, episode steps:  43, steps per second: 137, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.384792, mae: 7.844452, mean_q: 15.199622\n",
            " 2359/10000: episode: 88, duration: 0.260s, episode steps:  35, steps per second: 135, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.066499, mae: 7.916089, mean_q: 15.202649\n",
            " 2425/10000: episode: 89, duration: 0.471s, episode steps:  66, steps per second: 140, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.805069, mae: 8.041507, mean_q: 15.537992\n",
            " 2446/10000: episode: 90, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.513153, mae: 8.156641, mean_q: 15.762687\n",
            " 2484/10000: episode: 91, duration: 0.279s, episode steps:  38, steps per second: 136, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.266690, mae: 8.224340, mean_q: 15.978284\n",
            " 2524/10000: episode: 92, duration: 0.289s, episode steps:  40, steps per second: 139, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.588114, mae: 8.287321, mean_q: 16.013741\n",
            " 2568/10000: episode: 93, duration: 0.321s, episode steps:  44, steps per second: 137, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.513762, mae: 8.462846, mean_q: 16.439880\n",
            " 2610/10000: episode: 94, duration: 0.299s, episode steps:  42, steps per second: 141, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.158770, mae: 8.493670, mean_q: 16.530748\n",
            " 2625/10000: episode: 95, duration: 0.116s, episode steps:  15, steps per second: 130, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.145971, mae: 8.642625, mean_q: 17.013226\n",
            " 2672/10000: episode: 96, duration: 0.343s, episode steps:  47, steps per second: 137, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.353079, mae: 8.751994, mean_q: 17.055891\n",
            " 2692/10000: episode: 97, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.248760, mae: 8.887239, mean_q: 17.183872\n",
            " 2722/10000: episode: 98, duration: 0.227s, episode steps:  30, steps per second: 132, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.140450, mae: 8.922882, mean_q: 17.216311\n",
            " 2757/10000: episode: 99, duration: 0.258s, episode steps:  35, steps per second: 136, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.444102, mae: 8.926030, mean_q: 17.429697\n",
            " 2783/10000: episode: 100, duration: 0.189s, episode steps:  26, steps per second: 138, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.011637, mae: 9.087605, mean_q: 17.695194\n",
            " 2828/10000: episode: 101, duration: 0.326s, episode steps:  45, steps per second: 138, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.064404, mae: 9.143668, mean_q: 17.802444\n",
            " 2873/10000: episode: 102, duration: 0.333s, episode steps:  45, steps per second: 135, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.073835, mae: 9.261983, mean_q: 17.987484\n",
            " 2900/10000: episode: 103, duration: 0.199s, episode steps:  27, steps per second: 136, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.703970, mae: 9.384410, mean_q: 18.180126\n",
            " 2983/10000: episode: 104, duration: 0.608s, episode steps:  83, steps per second: 137, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.620433, mae: 9.523016, mean_q: 18.414631\n",
            " 3074/10000: episode: 105, duration: 0.650s, episode steps:  91, steps per second: 140, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.156833, mae: 9.660951, mean_q: 18.858835\n",
            " 3099/10000: episode: 106, duration: 0.181s, episode steps:  25, steps per second: 138, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.945873, mae: 9.839239, mean_q: 19.418995\n",
            " 3121/10000: episode: 107, duration: 0.172s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.837085, mae: 9.974031, mean_q: 19.471870\n",
            " 3160/10000: episode: 108, duration: 0.286s, episode steps:  39, steps per second: 136, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.377602, mae: 10.033928, mean_q: 19.541580\n",
            " 3208/10000: episode: 109, duration: 0.347s, episode steps:  48, steps per second: 139, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.648560, mae: 10.150746, mean_q: 19.764923\n",
            " 3251/10000: episode: 110, duration: 0.313s, episode steps:  43, steps per second: 137, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 3.391238, mae: 10.238977, mean_q: 20.018005\n",
            " 3314/10000: episode: 111, duration: 0.459s, episode steps:  63, steps per second: 137, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.601644, mae: 10.398119, mean_q: 20.234442\n",
            " 3380/10000: episode: 112, duration: 0.491s, episode steps:  66, steps per second: 134, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.531064, mae: 10.511152, mean_q: 20.627165\n",
            " 3405/10000: episode: 113, duration: 0.187s, episode steps:  25, steps per second: 134, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.241911, mae: 10.658495, mean_q: 20.759356\n",
            " 3449/10000: episode: 114, duration: 0.316s, episode steps:  44, steps per second: 139, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.521288, mae: 10.681885, mean_q: 20.880468\n",
            " 3539/10000: episode: 115, duration: 0.647s, episode steps:  90, steps per second: 139, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.223905, mae: 10.900260, mean_q: 21.333038\n",
            " 3587/10000: episode: 116, duration: 0.358s, episode steps:  48, steps per second: 134, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.967450, mae: 11.072820, mean_q: 21.558266\n",
            " 3629/10000: episode: 117, duration: 0.305s, episode steps:  42, steps per second: 138, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 3.484921, mae: 11.146118, mean_q: 21.910433\n",
            " 3645/10000: episode: 118, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 5.659014, mae: 11.196089, mean_q: 21.743378\n",
            " 3687/10000: episode: 119, duration: 0.312s, episode steps:  42, steps per second: 134, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 4.543068, mae: 11.257036, mean_q: 21.999657\n",
            " 3802/10000: episode: 120, duration: 0.824s, episode steps: 115, steps per second: 140, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.071452, mae: 11.481142, mean_q: 22.533508\n",
            " 3840/10000: episode: 121, duration: 0.285s, episode steps:  38, steps per second: 134, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.096419, mae: 11.617395, mean_q: 22.793005\n",
            " 3872/10000: episode: 122, duration: 0.243s, episode steps:  32, steps per second: 132, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4.355072, mae: 11.792071, mean_q: 23.184900\n",
            " 3907/10000: episode: 123, duration: 0.260s, episode steps:  35, steps per second: 135, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.797543, mae: 11.834876, mean_q: 23.074268\n",
            " 3939/10000: episode: 124, duration: 0.235s, episode steps:  32, steps per second: 136, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 5.047798, mae: 11.953491, mean_q: 23.368492\n",
            " 3994/10000: episode: 125, duration: 0.410s, episode steps:  55, steps per second: 134, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.524548, mae: 11.982462, mean_q: 23.503916\n",
            " 4071/10000: episode: 126, duration: 0.556s, episode steps:  77, steps per second: 138, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.216842, mae: 12.164393, mean_q: 23.904959\n",
            " 4104/10000: episode: 127, duration: 0.249s, episode steps:  33, steps per second: 132, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.031329, mae: 12.402418, mean_q: 24.043516\n",
            " 4145/10000: episode: 128, duration: 0.306s, episode steps:  41, steps per second: 134, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.597689, mae: 12.367774, mean_q: 24.014181\n",
            " 4238/10000: episode: 129, duration: 0.677s, episode steps:  93, steps per second: 137, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.797316, mae: 12.488173, mean_q: 24.436106\n",
            " 4266/10000: episode: 130, duration: 0.209s, episode steps:  28, steps per second: 134, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.491771, mae: 12.524193, mean_q: 24.597391\n",
            " 4334/10000: episode: 131, duration: 0.504s, episode steps:  68, steps per second: 135, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.935747, mae: 12.720201, mean_q: 24.863663\n",
            " 4378/10000: episode: 132, duration: 0.331s, episode steps:  44, steps per second: 133, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 6.326417, mae: 12.832206, mean_q: 24.967897\n",
            " 4431/10000: episode: 133, duration: 0.382s, episode steps:  53, steps per second: 139, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.040699, mae: 12.897434, mean_q: 25.201700\n",
            " 4544/10000: episode: 134, duration: 0.807s, episode steps: 113, steps per second: 140, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 6.064646, mae: 13.060957, mean_q: 25.438036\n",
            " 4575/10000: episode: 135, duration: 0.232s, episode steps:  31, steps per second: 134, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.134796, mae: 13.074127, mean_q: 25.599863\n",
            " 4642/10000: episode: 136, duration: 0.490s, episode steps:  67, steps per second: 137, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 5.492454, mae: 13.277730, mean_q: 25.992722\n",
            " 4722/10000: episode: 137, duration: 0.565s, episode steps:  80, steps per second: 142, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.020620, mae: 13.362114, mean_q: 26.254740\n",
            " 4800/10000: episode: 138, duration: 0.561s, episode steps:  78, steps per second: 139, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.123696, mae: 13.558142, mean_q: 26.546173\n",
            " 4949/10000: episode: 139, duration: 1.056s, episode steps: 149, steps per second: 141, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.622157, mae: 13.753267, mean_q: 26.940390\n",
            " 5016/10000: episode: 140, duration: 0.486s, episode steps:  67, steps per second: 138, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 5.003867, mae: 13.968441, mean_q: 27.522453\n",
            " 5071/10000: episode: 141, duration: 0.404s, episode steps:  55, steps per second: 136, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.528300, mae: 14.070843, mean_q: 27.702171\n",
            " 5204/10000: episode: 142, duration: 0.948s, episode steps: 133, steps per second: 140, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.343614, mae: 14.247058, mean_q: 28.095329\n",
            " 5298/10000: episode: 143, duration: 0.676s, episode steps:  94, steps per second: 139, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.255706, mae: 14.525494, mean_q: 28.619026\n",
            " 5498/10000: episode: 144, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.222634, mae: 14.752673, mean_q: 28.921297\n",
            " 5677/10000: episode: 145, duration: 1.261s, episode steps: 179, steps per second: 142, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.539400, mae: 15.058291, mean_q: 29.654385\n",
            " 5741/10000: episode: 146, duration: 0.464s, episode steps:  64, steps per second: 138, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.969749, mae: 15.282338, mean_q: 30.065083\n",
            " 5783/10000: episode: 147, duration: 0.308s, episode steps:  42, steps per second: 136, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.894418, mae: 15.420265, mean_q: 30.270929\n",
            " 5849/10000: episode: 148, duration: 0.481s, episode steps:  66, steps per second: 137, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.467368, mae: 15.548527, mean_q: 30.721548\n",
            " 5945/10000: episode: 149, duration: 0.690s, episode steps:  96, steps per second: 139, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 5.878010, mae: 15.593781, mean_q: 30.733927\n",
            " 6114/10000: episode: 150, duration: 1.200s, episode steps: 169, steps per second: 141, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.375285, mae: 15.866130, mean_q: 31.348289\n",
            " 6306/10000: episode: 151, duration: 1.336s, episode steps: 192, steps per second: 144, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.117490, mae: 16.186316, mean_q: 31.935568\n",
            " 6390/10000: episode: 152, duration: 0.605s, episode steps:  84, steps per second: 139, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 5.569696, mae: 16.520876, mean_q: 32.662415\n",
            " 6590/10000: episode: 153, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.879324, mae: 16.739389, mean_q: 33.182396\n",
            " 6762/10000: episode: 154, duration: 1.221s, episode steps: 172, steps per second: 141, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.427324, mae: 17.145922, mean_q: 34.013065\n",
            " 6962/10000: episode: 155, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.410524, mae: 17.509230, mean_q: 34.843357\n",
            " 7073/10000: episode: 156, duration: 0.789s, episode steps: 111, steps per second: 141, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 7.074010, mae: 17.890747, mean_q: 35.568859\n",
            " 7273/10000: episode: 157, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.431686, mae: 18.246954, mean_q: 36.382793\n",
            " 7473/10000: episode: 158, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.170083, mae: 18.727634, mean_q: 37.433693\n",
            " 7673/10000: episode: 159, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.225325, mae: 19.208996, mean_q: 38.453812\n",
            " 7873/10000: episode: 160, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.731093, mae: 19.724218, mean_q: 39.487118\n",
            " 8073/10000: episode: 161, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.339572, mae: 20.221798, mean_q: 40.439163\n",
            " 8273/10000: episode: 162, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.226754, mae: 20.726049, mean_q: 41.570385\n",
            " 8473/10000: episode: 163, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.009739, mae: 21.275467, mean_q: 42.669247\n",
            " 8673/10000: episode: 164, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.630434, mae: 21.689262, mean_q: 43.524410\n",
            " 8873/10000: episode: 165, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.530192, mae: 22.244112, mean_q: 44.734131\n",
            " 9073/10000: episode: 166, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.984170, mae: 22.783083, mean_q: 45.738438\n",
            " 9273/10000: episode: 167, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.819670, mae: 23.200300, mean_q: 46.611259\n",
            " 9473/10000: episode: 168, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 11.589827, mae: 23.706915, mean_q: 47.446144\n",
            " 9673/10000: episode: 169, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 9.511491, mae: 24.051191, mean_q: 48.273788\n",
            " 9873/10000: episode: 170, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.331038, mae: 24.471706, mean_q: 49.272282\n",
            "done, took 73.873 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eZgcV3nv/32rqtfZRzPaJUuWZXkF4QgDtgEbEmIcEpaskHDZfjEQ1iSXBEJu4ElIQhbCTbhAYn4Qm1yzBgyEOIDDYjDGxrItC8u2bGvfNfv09FLruX9UnVOnqqt6mZnWaHrO53nmme7q6u7TPdJ5632/70KMMSgUCoVC0QhtqRegUCgUivMfZSwUCoVC0RRlLBQKhULRFGUsFAqFQtEUZSwUCoVC0RRjqRewUEZGRtiWLVuWehkKhUKxrHjwwQfHGWOjrZ6/7I3Fli1bsHv37qVehkKhUCwriOhIO+erMJRCoVAomqKMhUKhUCiaooyFQqFQKJqijIVCoVAomqKMhUKhUCia0lFjQUSbiOj7RPQYEe0joncFx4eJ6C4ieir4PRQcJyL6JyJ6moj2EtFVnVyfQqFQKFqj056FA+APGWOXAXgugLcR0WUA3gvgu4yx7QC+G9wHgJcC2B783Azgkx1en0KhUChaoKN1FoyxUwBOBbdLRPQ4gA0AXg7g+uC02wD8AMAfB8c/y/y+6fcR0SARrQteR6FQKBaVPcemoRPhyo0D83r+sckKnh6bww07VgMAvr//LLav7sXGoSJmazb+7SdHYNruYi45wrM2D+GGS1Z37PVlzllRHhFtAfAsAPcDWCMZgNMA1gS3NwA4Jj3teHAsYiyI6Gb4ngc2b97csTUrFIru5q/ufBwA8KU3P29ez//sTw7j9vuP4rE/vxEA8Nb/+yBe97wteN9Nl+IH+8fwd9/eDwAgWpTl1vGGa7Z2l7Egol4AXwHwbsbYLEnfHGOMEVFbE5gYY7cAuAUAdu3apaY3KRSKeWE6Hko1e97PtxwPFcuF6bhgDKjZHkzH81878Ch+9Ec3YNNwcVHWu5R0PBuKiDLwDcXtjLGvBofPENG64PF1AM4Gx08A2CQ9fWNwTKFQKBYd1/MwWbbm/XzH869VSzUHs4HRsV0v8lhG746k005nQxGATwN4nDH2D9JD3wDwuuD26wB8XTr+P4KsqOcCmFF6hUKh6BSOyzBdseEEG3y7uIFBmK3amK064jX93/5rGnqHYlDnmE6Hoa4F8FoAPyOiPcGxPwHwYQBfIqI3ATgC4DeCx+4EcBOApwFUALyhw+tTKBQrGL7ZT1VsjPbl2n4+9x5maw485t+2Pd9I2IHRMDRlLJrCGLsHQNo39eKE8xmAt3VyTQqFQsHhm/1k2ZqXsXBFGMoWt4Vn4XHPojvCUMu+RblCoVDMF76hT5RNAH3zeD4PQzlwWdRIdJtn0R0mT6FQKOaB64aeRatMVyywwDB4Igxli6wq2416GErgVigUimUO9wwm5lozFqdnanj2X/43fnJgIni+70VEBW5PPEYE6F3iWagwlEKhWLFwnWGiRc/i+FQFtstwtmRGnj9bsxHcFAbIdhkyWvdcjytjoVAoViy8JmKybLZ0/lQluZaiVHOE4RCPuV7XpM0CylgoFIoVjOu1p1lMBefx58l1Fi73LEQ2FOsacRtQxkKhUKxg2tUspipW5HmuVGchPAvJw+gWcRtQxkKhUKxg2vYsgjCUqKmIeBbRym3HZSoMpVAoFMsdxlikKK8VplM9C0ngdsNKbkMJ3AqFQrG84Zu7Rn54yfVY0zTXSaFZNBC4vdCzyHSRZ9E9Zk+hUCjagGctjfTm4LHQa2jEdBCGCj0Lqc4iKMqT2310S6sPQBkLhUKxQuGewOp+vydUK6EoLnC7sSrtsuWiZocps4BfZ9FN2VDKWCgUihUJ9w5W9+UBtFaYF8+G4p1mZXg2lNNl2VDd80kUCoWiDbhnsaZFz4IxJsJQ8WwoTl/ekNp9dFc2lDIWCoViRcL7OrXqWZRMJ2zlETzX9Rh6sro4Z1VPNsyGcr2uavfRPZ9EoVAo2oBv6nyOxWSTwrzpcjirW9Yshnqy4vhwTzaSDaU8C4VCoVjm8FBSPqOjkNExZ9oNz5+UsqXkOovhmLEI6yyYyoZqFSL6DBGdJaJHpWNfJKI9wc9hPm6ViLYQUVV67J87uTaFQrGy4Ru+oREMjdBsDPeUZCxEbyjGMFSMGQuP+QV/rodMF2VDdboo71YA/wfAZ/kBxthv8ttE9BEAM9L5BxhjOzu8JoVCoRA1ErpG0HUS99OYTvEshooZcXy4Jyce77YwVKdncP+QiLYkPUZEBOA3ALyok2tQKBSKJGTPQieqy2yKMxloFoWMHlZwux76CxkQARoR+gtGcJz57T5UGGpReD6AM4yxp6RjW4noYSK6m4ien/ZEIrqZiHYT0e6xsbHOr1ShUHQdXFswdA26RiK0lMZ0xYJGYagJ8D2LjK6hL2egL28gGxgH2/P8dh9dFIZaSmPxagCfl+6fArCZMfYsAH8A4HNE1J/0RMbYLYyxXYyxXaOjo+dgqQqFotuIaxbNPIupioWBQgZZQ4vUWRgaob+QQX8+Iyq2HZcFw4+6x7NYkkaCRGQAeBWAn+PHGGMmADO4/SARHQBwMYDdS7FGhULR3cQ1C6+ZsSjbGCpmoUmGhTcf7M9noGkQxsFxPdhedzUSXKqusz8P4AnG2HF+gIhGAUwyxlwiuhDAdgAHl2h9CoWiyxFhKI1gaFpLnsVQTxZl0xF1Fi7zPYttq3tBgDAOthd4Fl1UlNdRY0FEnwdwPYARIjoO4AOMsU8D+C1EQ1AA8AIAf05ENgAPwFsYY5OdXJ9CoVi58FCSrlFLmsVUxcaGwTxqtgvH8+B5DIwBmkb437/pJ3F+7eETAHzPQmVDtQFj7NUpx1+fcOwrAL7SyfUoFAoFR2gWOtcsmqfOXr6+H2Ml00+NlbOpAq2CGwc7yIZSjQQVCoVimcONg6Fp0KiVbCgbg4WM8EJCzyTcRrlxcIJsKNWiXKFQKJY5XLPQNYKhN8+GslwPuYzm6xsuk4xNaBAi2VCq3YdCoVAsf1wpDNVMs+CeREaqyZA1Dw73LGq2699XnoVCoVAsb+p7Q6UbCz6CNaNrgRfiJRoLrllUA2OhPAuFQqFY5siag96kKI8bi5zR2LPgqbJVK/AsuigbShkLhUKxIuEGgNdZNPYs/McyuiZqMmTPhJOJexYqDKVQKBTLm7hm0Ypn4RuLBp5FTLNQYSiFQqFY5jh1RXnpdRaWw42F3xok4lno9dlQNTs8v1tQxkKhUKxIhGfBNQu3uWeRNXzPwnG9hnUWYRiqe7bY7vkkCoVC0QayZ2FoBI+1plnwkJUwFpSQDWW5kfvdgDIWCoViReJKRXXz0SwcqWstJ6PF6iyUZqFQKBTLG9uN9oZqlA1lyppFkA3lJmRD1dVZqGwohUKhWN5ENQutNc0i4lkEYSg9PQylPAuFQqFY5vDNXiNA19BaBbfBxXAv0bPgYaiwglt5FgqFQrGscT0PhkYgCkNLabReZ8FTZ1U2lEKhUHQFTjASFUBgABrVWfBsKIKha6nZUPHUWVVnoVAoFMsced5Es66zjTSLpKK8MHW2e7bYjn4SIvoMEZ0lokelYx8kohNEtCf4uUl67H1E9DQR7SeiX+zk2hQKxcrGleZNtNN1lqfZOi5PnQ23Ue6pVO36WRfLnU6bvVsB3Jhw/KOMsZ3Bz50AQESXwZ/NfXnwnE8Qkd7h9SkUihWKE2gWAEQLjzTiFdzyMdkgEBEyOqk6i3ZhjP0QwGSLp78cwBcYYyZj7BCApwFc3bHFKRSKFY1bp1mkGwtLruAOwk689kKPeQ+GpqkK7kXk7US0NwhTDQXHNgA4Jp1zPDhWBxHdTES7iWj32NhYp9eqUCiWIU+cnsXTZ0upj0c0C/I9C5bS8sN2opoFAJgpoSZDp1DgVtlQC+KTALYB2AngFICPtPsCjLFbGGO7GGO7RkdHF3t9CoWiC/jA1/fhL775eOrjrseEl8B1hzTnIqyzIHGu6fgGQYsZi4yuqTqLxYAxdoYx5jLGPACfQhhqOgFgk3TqxuCYQqFQtE3FckU4KAnbY6IOgm/qTkr6bLzOAgjDUHWehUaipbkyFguAiNZJd18JgGdKfQPAbxFRjoi2AtgO4Kfnen0KhaI7sF0PlpteO+HKAnfwO63UgmsWvOkgkK5ZyKJ2N4WhjE6+OBF9HsD1AEaI6DiADwC4noh2AmAADgN4MwAwxvYR0ZcAPAbAAfA2xlj6ZYFCoVA0wHK8hlf2jhsVuAHuWdQnYdquh6yugYgkzSK5SttI6BXVDXTUWDDGXp1w+NMNzv9LAH/ZuRUpFIqVguV6yDjpV/Z+nUXUs0jLiLIcT1Rj89oM003Lhqqv6O4GWv4kRPQuIuonn08T0UNE9JJOLk6hUCjmi+16sBu08PDbfYRFefxY2mtljOi5PBuqURhqpRblvZExNgvgJQCGALwWwIc7siqFQqFYILbLGrYdl4vytCaehe16wgg00yzk0FP8seVMO8aCf+qbAPwbY2yfdEyhUCjOKyzHE1lMSSRrFmlhKIasHvcskgcccQ0jo/sdbbuFdozFg0T0HfjG4ttE1Acg/S+hUCgUS4jlemIaXhKux4QOwcNRbsr5vmcR1TfSs6G4AeoevQJoT+B+E/xCuoOMsQoRrQLwhs4sS6FQKOYPYwy266XWTQC+F1GMaRZuWgW3FIYyRLuPxp5FN2VCAW0YC8aYR0RbAPwOETEA9zDG7ujUwhQKhWK+uB4DY2GbjrRz4nUWaTMtopoFr+BurFl0UyYU0F421CcAvAXAz+AX0r2ZiD7eqYUpFArFfOHFeHaD5oDx4Uf8WPLrMWTj2VCOB41Qp0tkYtpGt9BOGOpFAC5lQactIroNfgGdQqFQnFfYwWS7xgJ3fQV3WvaU7XhC4JY1iyRdgj++Yj0L+C3DN0v3NwF4anGXo1AoFAuHexaMpafDysOPmhXl+XUWUS/EtN3E1NiweG/lehZ9AB4nop/Cb9VxNYDdRPQNAGCM/UoH1qdQKBRtI3sUtutB1+pbeDgJmkWjorzevBE513K8xFCToakw1J91bBUKhUKxiFhO1FjkM/XGIjr8SBPHEl/PZSKsxH+bjidanMuseIGbMXY3/MZ/meD2TwE8xBi7O7ivUCgU5wWyZyHrEEcmynj75x6C6bjRsaothKGSNYuEMFSXps62kw31uwD+HcC/BIc2AvhaJxalUCgUC8GKhaE49x+axDf3nsKxyUq0gltvpd1HvWahJVRoG11alNfOp3kbgGsBzAIAY+wpAKs7sSiFQqFYCHLltpw+yw1H2XRTNIuUOgsnuTdUomehh+0+uol2jIXJGLP4HSIy4AvdCoVCcV4haxZOQkiqYrmRbCijSRjKinSd1cSxRM1CU57F3UT0JwAKRPQLAL4M4D86syyFQqGYP/FsqPjtqu1Eu85Ss0aCkmYhDzdKMAiGvsI1CwDvBTAGv4L7zQDuZIy9vyOrUigUigUQ1SxY3fGy6UazoZpqFqxOswCSW5BnVno2FIB3MMY+xRj7dcbYrzHGPkVE72r0BCL6DBGdJaJHpWN/R0RPENFeIrqDiAaD41uIqEpEe4Kff57nZ1IoFCsc20n2LHgYqmpFNYuWhh/FNAv5eTLc2+imWRZAe8bidQnHXt/kObcCuDF27C4AVzDGngHgSQDvkx47wBjbGfy8pY21KRQKhSDNs+CGo2Q6YCxsCsh/ewnGwvMYHI8l9nxqlA3VbQJ306I8Ino1gNcA2MqrtQP6AUw2ei5j7IdBp1r52Heku/cB+LVWF6tQKBStEPUmwtvciMxWbQBymmu6Z8FHs/JGghHPIsEgrOR5FvcCOAVgBMBHpOMlAHsX+P5vBPBF6f5WInoYfnrunzLGfpT0JCK6GcDNALB58+akUxQKxQqGNxIEop4FD0OVag4AtNSinD8/G6vglp8ns2LnWTDGjgA4QkQ/D6AazLW4GMAl8MXueUFE7wfgALg9OHQKwGbG2AQR/RyArxHR5cHc7/iabgFwCwDs2rVLpe8qFIoIkTCUV69flGq+Z6HX1VkkeBaB/hGflAckaxZC4O4yz6KdT/NDAHki2gDgOwBeC1+TaBsiej2AlwH4bd7ynDFmMsYmgtsPAjgA4OL5vL5CoVjZRHpDJYjds4GxaKXdB38Or7PQqXE2lEqdBYgxVgHwKgCfYIz9OoDL231DIroRwB8B+JXg9fjxUSLSg9sXAtgO4GC7r69QKBQRzcKrD0nxMJQeE62T5llwL4WHnzSNwG1EchhKpc4SET0PwG8D+M/gWH0rx+gTPg/gJwB2ENFxInoTgP8Dv935XbEU2RcA2EtEe+D3oHoLY6yhgK5QKBRJNCvKS/MsvIQZ3HHNwn9eNItKRk3KA94FP831DsbYvuDq//uNnsAYe3XC4U+nnPsVAF9pYz0KhUKRiOUmC9zCWFSjAjff/BM1i5hnAQTGxU2psxDDj7rLs2jZWDDGfghft+D3DwJ4J79PRB9jjL1jcZenUCgU7ZOWOhuGoaKps400CysmcAP1HokMNzzdVmexmKbv2kV8LYVCoZg38eFH8dtCs9CaaxZxgRsI+0M1yobqtjqL7vo0CoVCAX+D50lLSWEoHm4SjQQb1FlwwxPVLBp4FiobSqFQKJYHtuuhJ+tH2eUZFXKxHlBfM5GsWfjHkorxEhsJimwoZSzS6K5vRqFQzIsXfeQH+OIDR5d0DZbDUMj6yZrRQUhRzyHeQbZhnUVCa/KGnsVKD0MRUTHloX9c4FoUCsUyx/UYDo6VcWi80vzkDmK5HnqEsUjWL4BoxpKRYizidRb+89I1i25tJNjODO5riOgxAE8E959JRJ/gjzPGbl385SkUiuUE34zjm/Ji84GvP4q//q/H09fheMgZOnSNosYiFoaKexaNUmezRlIYKqHOQvSG6i7Pop06i48C+EUA3wAAxtgjRPSCjqxKoVAsS86VsXj42LTQJNLWkTHI1yEahKGiHWS1JmGoeoG7YZ1FlxXltWX6GGPHYofcRVyLQqFY5nB9oNPGwrS9iHAdxwqGFWV1rS4bSh5B0ZJnEXgjUc8iXbPg58nndwPtfJpjRHQNAEZEGSL6nwDS/UCFQrHiCD2LzjaDtlwvUqVd97jjGwtDrw9D9eVCj0Te7HWi5NTZRIE7PRvqwpEe/OkvXYobLlndxic6/2nHWLwFwNsAbABwAsDO4L5CoVAACGsSOu9ZuJHK7Di26yFnaMjoWsQDcTwPA8WMuC9nLDXVLBJSZ5NCTUSE/+/5F6I/n6l7bDnTTruPcfhNBBUKhSKRc6VZWK7X8D1s1x+DmtE1WJKobTke1g7kcQxVANHCOUNvljrbWlFet9LKWNWPAUj19xhj70x7TKFQrCx4+MlyOhuGMh0vsTVHuA4PGZ1g6BQtynMZBgqyZ9FKnUV6UV63idiNaCUMtRvAgwDyAK4C8FTwsxNAtnNLUygUy42wnUaHw1COV5fZJMM1i4yuRYyK43kRYxGv4G65kaCenjrbrbQyVvU2ACCitwK4jjHmBPf/GUDijGyFQrEysc5BGIoxBsvx6mom4uvIGhoMjcSaGGMJnkU0wylNs8joBKL6Cu5u6//UiHbM4hCAful+b3BMoVAoAIQjTBtt5AvFasF7sV0PWV1D1tCEEM7DSf2yZ6E39yzsIA1XRjQgpJVjLNopyvswgIeJ6Pvw+0C9AMAHO7EohUKxPBGaRQc9Cx4WktuQJ60jo/ueBV8TNy6yZ5GRwlBaSjYUD2nJKM2iAYyxfwXwHAB3wJ9o9zweokqDiD5DRGeJ6FHp2DAR3UVETwW/h4LjRET/RERPE9FeIrpqfh9JoVAsFedCs+BGImljl8/x6yy0MEMr8Hbyhi4K5uo1i6Q6C1bvWegrLxuqXXXmagDPh+9VPLuF828FcGPs2HsBfJcxth3Ad4P7APBSANuDn5sBfLLNtSkUiiXGPAdhKP4ejbKhrKDdR1YPdQi5uI43Gayrs0gZfpSNaRO60izSIaIPw5/D/Vjw804i+qtGzwlGsU7GDr8cAPdIbgPwCun4Z5nPfQAGiWhdq+tTKBRLz7mosxBhKNcDY/Wbuy9ke8jFKrjleoli0Fcqrll4Ca/n95lK1ixWkmfRjmZxE4CdjDEPAIjoNgAPA/iTNt9zDWPsVHD7NIA1we0NAOTeU8eDY6cQg4huhu99YPPmzW2+vUKh6BR8Q+6kZmFKWoXrsbqre9djYAwidVZoFlK9REF4FtE6i6rdmsCtNIvmDEq3Bxb65sy/LGjbX2WM3cIY28UY2zU6OrrQZSgUikVCaBYd7A0Vna/dYP6EoSGjk8iG4scNKQzVWp1FgmahsqEa8teoz4Z6b+OnJHKGiNYxxk4FYaazwfETADZJ520MjikUimWCdQ66zlpu2Oza9jwUoEce53qJnw2l1YWhsqmehdaGZhF4FkqzqIcx9nkAzwXwVYTZUF+cx3t+A8DrgtuvA/B16fj/CLKingtgRgpXKRSKZYDtnIMwlC01BmzgWWSN9DBUMWtA16KFdrqG1DqLeLvxULNYORXc7Qjc1wKYZYx9A35x3h8R0QVNnvN5AD8BsIOIjhPRm+DXa/wCET0F4OeD+wBwJ4CDAJ4G8CkAv9fuh1GsPMZKJmZr9lIvQxFwLgRus8GYVPlYVidkJIFbDkMVs3qdOG1oWmLKr5lYZ8HnbK8cz6KdMNQnATyTiJ4J4A8AfBrAZwG8MO0JjLFXpzz04oRzGVTLc0Wb/O5nd+PSdf3461ddudRLUeDcaBayZ5FkLMJeTrxFeTQ0ltU1FLN63Uaf1khwruZg1api5FhG1Vk0xAk29JcD+Dhj7OMA+jqzLIWiNSbLFibL5lIvQxFgiWppBq9B0VwjqpaLmp0+hNNyGwvccoqsnDorwlCGhjX9eQwVo31QDY3gJqTOlmo2+mKzKcQMbiVwJ1IiovcB+B0ALyAiDUB3TfdQLDts1+v4VDZF60Sm0nkecpre4Oxk3vH5h9CXz+Cjv7kz8XE5GyppAJKsWWT1eoHb0AhvvX4bXvOcaNq9rhHchH9LJdNBXz66VQrNQgncifwmABPAmxhjp+FnK/1dR1alULSI7bKOD9rpBhpNlVtM7CZpra1wYrqGUzPV1MdNR8qGSvQsgpnZgWfhxPpVcYF73UAh8jx/9kX09TyPYc500B8zFitRs2gnG+o0Y+wfGGM/Cu4fZYx9tnNLUyia43iNh+Ao/Cvxa//me/j8T492/L1kwz1fA2U6bsMmgdE6i8aahRG0HWeMiX8n8cwmjkb1mkXZcsAY0Bv3LJRmUQ8R3RP8LhHRbPx355eo6Aa+9ehpXPvh7y26F2A7jcdrKoDHT83izKyJY5OVBb0OYwwv+sgP8OXdx1LPsSTDPd/0WdP2Gj43EoZKyF4KNQsShkH2QNO8ASOh62yp5gBAqmZhrKDU2VaGH10X/FZitmLeHByfw4npKiqWi4HC4v0Hsz0Ge55C6kphz7FpAAtPZ7VdhoNjZTx1dq7BOQsPQ/meRbrWIbf7SBrfKmsW3DA4nhcJQyWha1qdZxEaixTNYgV5Fu0I3Ajahl8Hv0XHPYyxhzuyKkXXwatqF9sLcFzvnMXjlysPH50C0Hj+QyvUAq2gajXIVJJDRPN8v5rttRyGSvQsYqmz/rHmYShDrw9DzZl+DU9qNtQKMhbtFOX9GfwusasAjAC4lYj+tFMLU3QXnSjWcj0Gj3W2AKwb4J7FQquqa4GRqDQwFhHNYp4zLWp2Y81CFriT23OERoHXQ9ie1zQMlVRnMRt4Fr25ZM9iJQnc7XgWvw3gmYyxGiBalu8B8KFOLEzRXcQH0CzmayqBO52psoXDE75WYS7Qs6gGtQ+NaiBkY5EUImqG43pwPBbRPuLIhiTJAPLeUXz4EV+X0DLSPAuN6gwcD0OlZUMpzyKZkwDy0v0cVKM/RYvw/9T2Ik5QE5W5HZzKttzhXgUwfw2BUwsqpyuWk3qOvMnPx+MzxcjU1oryEj0L0UiQRBjKcVkkpTYJXSN4DJFiwlItOQzFs6GUZ5HMDIB9RHQXfM3iFwD8lIj+CQAYY+/swPoUXUInwlD2OZjKttx5+Ng0NALWDxYabsCtwD2LaiPPoklaazPMFhoRNm33EWkkSOK8pmGooBrbZQwa/NvNBG5NGYtE7gh+OD9Y3KUouhkhcC9mGMrr/Lzn5c6eY9O4eE0fMrq2YIGbC9uNBG7b9UTsfz6eDA9xNdQsXA85Q4OZkjYt94ASAneQOkuUHjri1diux5AJkrHmag40AorZaHbW87eP4s0vvBBbVvW09wGXMS0bC8bYbURUALCZMba/g2tSdCF8Y1/M1tU8BKHafaRzdraGzcNFTJSthYehnNYE7mJWR6nmLMiz8JivXxgJISPT9tCTM2A6Vl1dBBAvygs9CyuYeEcp/ZzCNNtoGKo3Z9Q9Z7Qvh/e99NK2P99ypp1sqF+GL2h/K7i/k4i+0amFKbqLcKbAIoahzkE77OWO6XjIZ3RkdFp46qzVPAxluQw9wXzr+fxdZPE8zbhZroeenJ76HnIjwYwkcDsuQ6ZB2IiL1m7EWDh1esVKpR2B+4MArgYwDQCMsT0ALuzAmhRdiNAXFtELiA+1UdRj2i5yhoasoUfmQMwHoVk08ywabOTNiBbcJT/fclzJIIV/+xPTVXzom49hsuyL0hGB2/PDUGmZUEDoWcjGYrZW30RwpdLOt2AzxmZi7pi6pFO0RCe8AK5V2J4HxlhqeGElYzoechm/++qCPYtAWG4ocLue2FwXolkAgOm6SGpsbTqeqHuQ/z19+9HT+P/vOQRDI2SDcJMRE7jTqreBUMuQNbA501bGIqAdz2IfEb0GgE5E24noYwDu7dC6FF0G1yoWU7PgYjljyeMwFYGxMHRkDVqwoZazoVjC3AfA9yCLCwhDteZZ+JoFEA1rHp2sQCPfi+BZUFGBm6WmzQLJnoUKQ4W0YyzeAeBy+G3KPwc/lfbd83lTItpBRHukn1kiejcRfZCITkjHb5rP6yvOPzqSOitdASYJnQq/2pnPdQ/2A6QAACAASURBVFi4Z+EbC8bSC/x8zaK1MJTnMfzbfUciYS3Zs0hbr+l4IjtJrus4NlnBxWv68D9fcjGuuWgEQDjRzgk8C6PB/AmeBiuHNUsqDCVoJxuqAuD9wU8dRPQxxtg7Wnyt/QB2Bs/T4Rf33QHgDQA+yhj7+1bXpVgeOB3QF+TXslxfyFWEeEH6qq9ZLJ6xAPyMqKTv29cs/G2lURU2AOw7OYv/9bVHMdKTxUuvXAcg5lmkGBvL8VNnMzrVeRZbR3rw9hdtF8d4V9hWwlDJnoUKQ3EWs7/utfN83osBHGCMHVnEtSjOM+xOhKGaVPKudPh37YehtAV/97IHkKZb2K6HYmBEmmW+TQTjcMspnkVaTY4VhNb4rArAb59+dLKCzcPRWdlZg2sWvuFsTbMIX1OFoULOh2bsvwXg89L9txPRXiL6DBENJT2BiG4mot1EtHtsbOzcrFKxICxRE9EpY6FyLeLwSmf/KlybdxdYTk2qAK+mtPyQ9YRmf+upiuW/lixqRzyLZIPEQ2tyOvBYyYTpeNi8KmosuGfheNyzSA9D8XO9QI8xHb9PVbyJ4EplSY0FEWUB/AqALweHPglgG/wQ1SkAH0l6HmPsFsbYLsbYrtHR0XOyVsXCCBsJzm/DYozh3gPjEWE1HoY63+Br9pZIT+HdWXMZPwy14NRZy0u8zfE8BsdjQk9olg3FU1xrkmdhphgOmTAMpYnMpaPBYKdNMc+Cp8raDms9GypY92zQFyreRHClspjGYj55iy8F8BBj7AwAMMbOMMZcxpgH4FPw6zoUXYDoEDvPjXPPsWm85lP3Y/eRqbrXBM7PMNS+k7N4zafux08OTizJ+/PNNmfoyAUCd1oWUytENYt6z4InHAjxucmFwXQzz6KBwJ0V3pL/ebixuCBuLHgFt+cFYahGnkVUs0ibkrdSadtYEFE/ESVNzfvHebz/qyGFoIhonfTYKwE8Oo/XVJyHcI9ivh4AnyswWbbC15QMz/nYH2qm6l+Zjs+ZS/L+wrMwtEhx2nyJGIsEzUKeI5HU7jsO/1vKxqJZNhT3XrKGBkMnYaCOTFRABGwYKkTOFy3KnRbqLPRonUVaE8GVSjvtPp5NRD8DsBfAo0T0CBH9HH+cMXZrO29MRD3wO9d+VTr8t0T0MyLaC+AGAL/fzmsqzl/4xj7fRoI8PCFf0co6xXxmJ3QavvHxTefcv3+oWfDpcAvJiKraLvqC+H0toYo7PqGuWRhKaBZWmmaR3lE2Z+h+GCp4j2OTFazrzyNnRDO0ROqs10IYiqKexVzK4KOVSjvfwqcB/B5j7EcAQETXAfhXAM+Yzxszxsrwp+7Jx147n9dSnP8stM6CbyJzptw7aOFT2ToJX/NSGQsRhsroEWPRk5vf61VtF8O9WZRMJ7GZoOj2GhOf05jimkWKZ5H0b4WL9vw9+DlHJyt1egUQLcpzWgxDOSIMlTzLYqXSThjK5YYCABhj9wBYmv8FimWH6A01z02dbzwVM/wnJ1+5no+dZ/nGx+c4n2vkMJQwFgsQuWu2h6FiFkBy6ix/7Yzuv9+8sqFs2Vv0b89Uwu/PdMPPZGih95KUNsvXAoRdZ5O62HL0VM1CeRZAC8aCiK4ioqsA3E1E/0JE1xPRC4noE1AzLRQtwv9TzzsMFWwcck6+HIY6XzrPWo4n1sXDQEvuWUiaxULCUDXbxXBPYCwSPYtwEp2haU2TDoRmIddZOK6oALccDyenq7jqQ3fhviBJIOJZBAapark4WzITjYWuEYjCCu6G7T70qLEIs6GUZwG0FoaKp6/+WfCb4E/MUygawhgLx6rOOwzlbyjlFM/ifMmGetNtD2DrSA/+/OVXiDXHjcW5anoob6y5eXgW8Rh/zXYbehaR1uBNelExxlI9i/5CBmXLhel4ODVTg+sxHBov47kXrpI0Cw2ZQEQ/MV0FAGwcLtS/EXzjVXM82E7jMFS8RTn/u/F26CudpsaCMXYDABBRHsCvAtgiPe/8+B+qOK+RM3DmayxEGEoSuOWQ1vkyh/vps3PQAkMQehbRMNSbbtuNzcNFfPBXLu/oWsIwlC6uqFv1LD7/06P4y/98HD9534tEzL5qu+jN+a+VpFmEQ4f81uCNDFPZcoWxj2gWjou+vIFTM75h417HdBCKsiRvydAJtsvE9ztYyCa+14bBAo5PVeB4jcNQ9ZqFg2JWb/iclUQ7wbivwZ9l8RCAWnBMGQtFU+xIuGiBYShTDkNJRmiB1cmLxUzVFpsf/z0b8yyePjt3TrrkJoWhWjHWU2ULH/6vJzBnOpiYs0JjYbnIZ3UUsnpkg+cIzyJoXNjovaakFOi4Z8Hfz3aYuDjgNRn8M/E6iznTEf8melKylraM9ODQeAWW0zgMFWoWgVZStTFYUCEoTjvGYiNj7MaOrUTRtUSF6IWFoSKeRSQbaumvW2zXQ8VyxUYqMrhixqJUsxvOhFgsxFV4pr3U2f/930+KGhHuQXge86fuGToKGT25KE/WLHRqGBrkekUho9dpFr05A7pGsFxXfE/1nkWYOlsO1hKfk83ZsqoH9x2cgMday4bin2O6YmGgmOytrETa8a/uJaIrO7YSRddiL4IQbYnU2bRsqKX3LPgGW415FiUpG4oxhjnTSbwyX2zkCu5WjcXJ6Sr+7/1HcdHqXgBA1XYir1XI6ihm9Yaps7zOolEYiusV6wbz0ToL22/lwVuq8/fh51sRz4ICA821heRr360jxcCINw5DcY+GC9vTVRtDReVZcNoxFtcBeJCI9geN/njxnELRkMUwFqbQLNKyoZbes4gbiySB23T8thONRpMuFkmps836Qx0aL8P1GF7+zPUAwu+bf6a8oaWGocLUWQqK8pobiw2DhWgFt+OKuhDZWEwH3y3/TL734r/HnAhDJXsWF6zqEbcbFeUNBoaBh8imK5Y4pmgvDPXSjq1C0dXI6bLz3dQtoVmkhKHOJ88iaLJnSqmzPAOKG45zEYaSu87yWH0zbYe3JuHdW/lmzY1DIcvDUI0Ebv/9Gn1G3kRw/UABDxyejKw5L7VUr8Y0Czm05mdDMVF7w+dyx9k6EhqLbIMwVD6joyeri7VNV2wMqjCUoJ3hR2rehGJeWIvoWcgCt9wbyj4PNAtuLITAHVwFux5DzfZQyOoijFazO2/ceMM9Imq5KG9izt+UNw75xqIa9ywyeuRzyMgV3IZOcGqNBW6NgNX9OdRsD57HoGkE03HFzHBT9iwq3LMI3kMP267z2ptCyvCr9YMFP6zVpCgPAIZ7s5iqWGCMYVoJ3BFUTpii4yxOGCqos4j1huIbxPmQDTUbNxaSQeDpnVzsPjeahSvqK1pNnZ0omzA0wrqBPABp7rYlGYuYKM2p1yzSDfhUxcJQMSvmdXMjIHsWtssiYSjGmORZ+CmttsdQNh30ZHUxFjWOrhE2BTUYjcJQADBczGKybKFkOnA9JupKFMpYKDqE5zG8/46f4bGTs7G5EwsLQ1UsV7TZtt1wdsL50BuKexa8aZ0pDQvi6bNc7K7a7oLahbeCGUyUA6S5Dk2M9XjJwnBPVoR0+GbNP0sh4wvciUV5Qbgxo1Pz1NmKhaGerPj7VSVvjHsWluMKo2Q5Hqq2KzSXrK4hKwncxSbN/ngoqlEYCgCGenzPgrcYGVCahUAZC0VHmKpYuP3+o7j7yTER+sjq2ry1BX7l6QYpnIC/8fE50OeFwC31MKrabqJnwTULN5iP3Ul4ZhHQnmexqjeHAt/EA0+O6zCFoM4iUbNw5RBR4wruybKFoWJGeIZV2xXfidAsHC/SCn26Yovuw36oK0idNcMWIWlsCUTupmGoYhYTc5YIeynPIkQZC0VH4NpC1XbFplHM6fMPQ0kbLxe5HZeJTe1cpM5+8YGj+N4TZ1If554F4LfwrtmumLLGY/xyzUWnRe5IGIpnQzUVuC2M9GaR0Qm6RnUCt19nYSS3KJfCUHwjT2O6YmOomEVeGCW3brKfLHAD/gWI3O7DCAxS2XRS02Y5WwLPolkYinsWPFtLZUOFKGOh6AihkBsai56sMf8KbskY8A3Mdv0rZ43OTW+ov/v2k/jsT9LzPGRjUbX93kajfX4/cO5RyMJwp3ULLnADaLk31ETZxKqeLIgIRSnrqSqyoTS/ziIhjBZtUd64zmKy7Ie7uGdRs11xQZCP1VnwNlozFTvsd6WHVeJly0nNhOJsFcaicRhquCeLiuXi9KzfpELVWYQoY5EAYwy3/PAAzpZqzU9WJFIW4QvJs8jq8+56Ks9m5huu7TEYPN++w5rFTMXG+JwZaVNRd041HoZyMdLLjYUd+Q0kd25dTEzHQy7YjEW7jyZdfyfmLKwK1izXU8SzoZLCaPx+RtITkuBNBIckY1G1XZE9lsvoyBi+QF6xXKwODO5UxYYVdI7VNIKhafBY0MOpSbO/Z2wcwAsvHsUzNw42PI931T00XgYADKT0m1qJKGORwMmZGv7qzidw595TS72UZQvf0Ku2K6bYFbP6vIVoy/FESIdX7NqOhywXUzs8Ke/psTkAwGSlNWNRsz3UbA8jMc+iJHsWToeNhR2GoXSNRAuNNCqWP9SIGzi5Utu0o9lQQL2xizcSTMtQ400Eh4oZFLKaeC3hWWRCz6JquVg34GcyTVctv79T8Jkyhu8lzFTtpmGovnwGt73xahGOSoNrFIfGuLFQngVnyYwFER0OqsD3ENHu4NgwEd1FRE8Fv4eWYm08Jj5TVbOd5sucVHzGrzALWX1BjQT5VR/XQxzPg6EFOf0d9iwOBMaCT3dLYqZqi7Gj/ubnYiRYM8+GimgW58KzMML/4nwDToPXWKzq9ddcyBr1YajAswCAih39/+G3NCcQkUhrTYJ7Z0PFrEhQiHgWho6c4WdDVWwH6wf9NN7pio2zJVNs4JmgpfhMxW4qcLcK/zd2cHwOvTlDGCbF0nsWNzDGdjLGdgX33wvgu4yx7QC+G9w/53BjMVtL3xgUjeHfYdVyxUbekzXmXQ9hOh6GhLEIPAuXwdApMjGtU3BjMWc6kZRYmdmqjTVBfUIt0CwKWQO9OUMYidI5FLgtKXUWQJChlP498ertkcBY+Cmy0WyofJA66x+Lrl+ef8HDUEnpwbyJYFyzqMmehRC4XQwWs8hnNExXLOw7OYPL1vcDCIcVlUxH1GsslOEe3xAdnqgoryLGUhuLOC8HcFtw+zYAr1iKRfCrKTmsoGiPOclY8BBRMWfMe6yn5bgYDkIEZUngzjaJjy8WB87Oidtp3sVM1cbaft9YzJkOLNdDPqOhN2eERXmmIwTbzgvcfs0CJ2voDbOhhGcRDOmW23rUHBdZXYOukfAG4umztsuEscjoGhhDYiv2SZFplJVSdF0R6uLzN7jAXczoGCpmcXK6hkPjZVweGAs5s6m3SRiqVXgYynI8DPUoYyGzlMaCAfgOET1IRDcHx9YwxrhQcBrAmqQnEtHNRLSbiHaPjY0t+sL4RjerjMW8kVNnuYEoZvR5txI3HU/06eGahcM9i1j9Rqlm4ycHJhay/DoOjJVFSGIyQeT2s3JcrAmMBW98lzN09OWNMBuq5gijx6/WO0U8DJUzmoShyr5nEYahwkrtquUiHxge7lnEjZ0leRZhEWD935v3eRquE7hDzyJjkGj3UczqGChkcN/BCTAGXLF+wH8PKbOpmcDdKoPFrDDmacOUVipLaSyuY4xdBb9B4duI6AXyg8z3XxN3FsbYLYyxXYyxXaOjo4u+ML4ZKc9i/vBsqFqszsL1WNuDfxhjgWbhX+lxQ2QHk88MnSLx8a88eByv/tR9ODZZWYyPAtNxcXSygp1BJs1UgsjNLyzWDvhX5TPBOfmM5huLoHK7ZDoinbbzdRb1Yagkz+7g2ByOT1UwHvMs5Ertmu0Kj4IbizrPIkg4AKTZEAlaEm/UNxzTLKKehS4MbDFnYKiYxURgpC/fUO9ZNEudbRVdI9EPStVYRFkyY8EYOxH8PgvgDgBXAzhDROsAIPh9dinWxjej+IQzRevI2VBynQXQfgEdvzodKGSgkaxZeGG+vXTFPBlU3953cHG8i6MTFbgew7O3+vkWSZ4Fv7DgYSheAZzP6OjLZyTNwp6XsXjk2HTb7UHkbCjAr39I0ox+7/aH8LbPPYyJOQs92VDAlrOharYrjvN00tMz0dRyy/WER8G9sKT3mypb0DVCX95AztBAFBQxOlHNgl9UFLO62LiHe7LiO5arsdMGH80Hro0pYxFlSYwFEfUQUR+/DeAlAB4F8A0ArwtOex2Ary/F+soqDLVgZIGbb/bzrbaWZ0n3ZA3htTgug6FRkA0VbqR8Y77/0GT9i82DpwO94tlbhgEkexbcWKwONrIpYSw09MphKNPBaJCaarZoLJ44PYuXf/zHbYfW6rKhjPpCuarl4skzJTxybBoPHZ0Sqb4AUMgYka6zPGS0bbQHq/tyuPupaAhYFrjDMa71Bm6qYmGwkIGm+ZlTXBuJeBbSugsZXYQgL1/fDwriRHKfp8XSLACIMKFq9RFl8b7h9lgD4I7gj24A+Bxj7FtE9ACALxHRmwAcAfAbS7E4LqAqYzF/ygmeRTE7vz5OYuJbRkMxp6PCw1BuUJSnRZvWzQUhn/sPLY5nwTOhrrqguWcx0puFoRFmqv45OUNHf97AbDDTYq4mhaFaTJ0dK/lawpkmRaKPnpjBnOlgVU8WF63ubSl19vHTs+B2ds+xaVy1OSxa8z0Lf901OyzwIyJcv2MU//XoaThS22/LYQnGIsGzCAryOIWMHtEschktsu5i1hBX+ZcHegUAGJp0ziIaC742lQ0VZUmMBWPsIIBnJhyfAPDic7+iKHyYCm9TrKe0PlakE2/3YWjhTIV2mwmGc5c19OQMzFlyGKq+wyl/72OTVZyYrmLDYGFBn+XwRAVr+nPoz2cwUMg0NBYDBb85nuxZ9OUzKNVs1GwPjscwWPR7LyWFoSzHQ81x0Z8PNyruKcmNCuPsP13Cyz52DwBAI+De9/r/jfgGDyCxBce+k7MAgAtWFXFkoiKqtwHfE/SYb6x9zyLcnG/YsRpf2n0cDx2dxtVbfY+L/z389yJxLM5k2RJX7/53pEc0i3xGF40PAd9oDQlj0S+OG5JnsVh1FoDyLNI431JnzwvkmQklVWsxL8rS1X/FcsWMA6B5f6I4YuCNoaEnawhjzq9qDZ0ivaHmTFeEJe5fBN1iumJjOBB9h3uyicaCe6H9hQzyWV2EqvKGjr6cAdPxRMpob95A3khu8/3HX9mL6z78PTx1piSOlUTdT7qGxsX8X71qIzzmF5UBqA9DxTyLfSdmMFjM4E3XbQUQ1lgAiFRqVywnMlzo2u0jMDTC9/eHsmLLYaiyHdEDikFbEVO6KJAznQpZHesHC9AIkXYdskFpVsHdDkqzSEYZiwTkaWyzK7yKWx440w7y+NNSzREzDoD2w1ChZ+EXhIk6Cy9MnY14FjUbz9g4gP68gfsPJusWfoZVa2Gg2ZotWo0MFTMNNYuBQgb5jCa8gFxGEzMRjk74G3p/3kA+YY71yekqvvHISczWHLzh1gdE+Il7Fo3Coryg7kWXrAYAHJ+s+u/fJHV238lZXLF+ADdduQ4ZnbB+IPTC5FkTk3OWMJj+Z8hg15YhfP+JZsYiOQw1LIehghRd0/abBmZ1DVkpi6uY1XHj5Wvx7Xe/QIx7BaIC92JlQwHAKmUsElHGIoGK5Fms9CruOx4+gef99Xdb3lg5cnfV2aotOpECCxG4/TBUfTZUtDJ5znTQn8/g6q3DeOBIsrH45t5TePaH/rsl3WC2aqO/EGbjTCYU5c1UbeQzGnKG3ztJrrO4ZK0fOvlJ4OX05ozEaXP/dt8RMMbwid++CuNzJj78X0+Iz8PfIw1uLC5d1wcAOD5VEe/PiYehbNfD/tMlXL6+HyO9OXzzHc/HGwIPAwgTEiqWg/E5S2gtnBdevBpPnC5hInjvSN+mwDOIe5FyE0FOXtIscrExsIBvLAxdw/Y1fZHX6kSdBQBRhb+6L79or9kNKGORQNl0wWWKlV5rcXCsjIlyOAymVcqmI+LMszVb9HAC5mMspDBUzu9X5HoMjPkip6Fpkd5QczUHvXkDF63uE2mvcR47NYvZmiMK0RpRqjnoE55Ftq7zrON62HNsOlL5zN8zn9Fx5YYB6BrhR0H2kDAWkmdRtVx8/qdH8ZLL1uKmK9fhGRsGcWLa3/BbaT8zVjLRnzewPtBnjk0FnkUmGoaSv/unz87Bcj3RPmPH2r5IVhFvoXFm1oTlepEQFQAxqpTXZ5SC7x0IPYt46/g50xFNBDn+d+FhzgxDXZFsqBSvoRMV3ADw0ivW4ktvfh42DRebn7yCUMYigbLliKuKlZ4Rxa9q2zEWnsdQttzILIeMsThhqJ6sjrLphIN2DELGiPaGKpkOenMGLlhVhOMxnJqp1r3mmWBeQamFWho/DCV5FhVL1DwwxvCBb+zDA4en8I4XXQQAotDMX7OGQlbHJWv78MixaQCBZpHVI5P0/vnuA5iu2HjDtVsAAP0FQ4RASy15FhZG+nLIZ/xq59CzSM+G4uK2nGEkw8NQR4LwWdyz4NlCfF3TVVsUtKV5kUkT6AoZHTXLxeOnZrF9dZ9Yq1hHJtlr4BcfGkU/50LJ6JoQ7RUhylgkUDYdrAs6Xa50z4Jvpu18D3wUJm91PVuzIwL3QsJQxawRNRaahowW9oZijKFs+p7A5uDK8GhCJffZWTPy+dLwPOaHtaQwlOX4rT1OzVTxzi/swe33H8Vbr9+G37p6M4AwfAOEhmPnpkGRotqfz6CQ0YRn8fU9J/CP330Kr3zWBrFJ9Rcy4jsPNYv0tY7NmeL7Xt2Xw3HuWUhhqLjA/djJWRQyuhgMFIevnX9/o73pxoIxhpmqHXaETQlDyU0EOYWsjpmqjX0nZrEzSN3NGlGBOwn+76kna4jaC0XnUMYigYrlYl0Qt2xVs/jWo6fxq5+8F948ex+dr/BssOkGcxzi8LAJ37xmKraYywwkV/U2QkxHMzT05vwpbdyT8AXuMBuqarvwmB+W4MYiqe0H9yxkz9FxPbz84z/GXY+Fo1NLpgPGEArcwSb3wOFJvPgjd+Pb+07jXS/ejve8ZId4jpw1xPspPWtz2G2/N2cgn/EF7hPTVbzny3vxnK3D+PCvXik2vf58Rvzbm2shDDVeMsWV/2hfTkx6k6+445rFiekKNg0XUlPDuWfBv7+RBp5FOQgNDsQ8i/j4VZ4RFtcsTs/WYLkedm4KjIXuv3cjr4G3KF/MTChFOspYJMCrbHWNWr6ivvfAOB48MpWYKbOcEWGoNjyLuZixKJmOmMsMIHXOQRry3OVizgBj4SbPPRbuWfCr8N68gXUDeegaJXoWfDPlPZsA4PhUFY8cm8aPnx4Xx+SUWCDMwf/7b++H6zHc9fsvwO//wsXQpA03GoYKPQtObz4UuPefnoXlevijGy+JeAH9hQzmTMf3bFrw7sbmTHHlv7ovB94ZJNsgG+rMrCkaHybBjQX//kYaeBZyNhgAXDjag6yu4cEjU5Hn8IuOeBiK8yzhWfCmheleAx9+tJjitiIdZSxiMObXBfTmDb/ytsXU2ZPTvtt/ttRcMF1OiEycNjQL4Vn0+RsCY4imzi7As+DaAW8qJ6ayBcaCx/d7cwYMXcOGwQKOTkY1i4rlhJPrpDDUoQl/OppsXPjVPH9ffkW87+QsXnXVBlywqj6Ew42FRmE45sKRHvTlDb+jqq4JgftMEA7jnixnoJABC0aG8s/EjUecmu2iVHOEAL1aMgB1YShpxsTZ2VrDjJ+CZCzkBnucvrxkLCpRY1HMGnjOhcOROgwg2kQwfB//38Wa/pyYiseNRVoICggruBczbVaRjjIWMUzHg+sxFLMG+guZlsNQJ6b9K9WxZWgsbr//CN7z5UcSK6tbuaqte07MswD8nHh+JbiQ3lB8M+Kpon42VNgbSngWQWhi83CxzrPgegUQDUMdHq83FtyY9Bf815Nj7a+/JkwzleFXyvmMLq6KNY2wc9MgenP++nmdBQ+HxcVjHvaardmifQk3Hpy/+dYTuPXHh8R3IcJQ0vcuZ0PxGROOx+B5DGdLJtb0R99XhmdDzVRtjPRmI94TANEMcDbBswD8Ku8DY2Ucm6zgjoeP4z1ffgQTc6Z4Xvz7etamMFSXFXpEurHghrhHeRbnBGWSY/Cr4p6gh36rmyTPuFmOnsW3Hj2NHz01jqyh4UOvuCLi9vOq4elq6+E1vmHLm1Z2ESq4cxlNFErxDTJjaEE2VBCGMqPGYtNwEd/edzryenyDBqKbr2wsPI9B0ygMQ+WjYahrtq3CjrXRvH8Ov1LOx7J43n7DRTgceC88DHVm1sRIbzaSBgrE9ADTFZ7IbM3GQNFvOXLLDw9i++pe7Az0ECFwSwYgXsEN+MZ62nTheAxrBxp4FtL64yEoeZ1yGKpfMhbX7xjFn38T+NLuY/jMPYdEhhxvIsgRSQBSXyoucKelzQJRgVvReZRnEYO3ZO7JGb7I2IKxqFiOSAnspGcxWbbw7i88vOgtSMZKJrKGhtvvP4pb7z0ceYxf1baTOsvbpcgbTEYnIUjGc++bIeosdC30LEpBGEqjIBuKgTEmNn+e7795uIjJshX5zs5If6PZSBjK9ygsxxNGnz/OjUV/wcBbr9+GP7np0tT18k02Lsw+58JV+M1nbxbn1BwPZ1JCQXzTna3amKuFc6j5pnznz07B9RieOjuHE0HmE/++I56FHIYSDf88YTAbhaF0qZ9XM2Mxm+BZbB3pwQWrivjY956G5XrYsaYPYyUzIm4DYahJ1nW4wN2o9ThPnV3MJoKKdJSxiME3up6cgf6C0ZJncXI6vFI9K3UGdVwPH/zGPhwcm0t6Wtv89NAEvrbnJPYE+fqLxfichVc9awNu2DGKv/v2fpwNNhLb9UQtQHthqCB1ti/cFDILCEPJjQTrwlCScO56THiGJkQ+PQAAHoNJREFUfUG4J8yICnUL/vlGerMRI3J4vCw8Fx6KCgVuf0MiIvzxjZfgig3JtQlAeKUc9yyi5/jzGk5MVRNDQdw4jc35BXG82I6v5z8eOQki/zPfEwjyPAzVzLPwjaH/HTQKQwHhZh0Pk3HinsWAVGxHRLhhh99+5I3XbcXf/NozAET1CgB4/kWjeM1zNgtxW15rI2PBLz56VRjqnKCMRQzeF4qPcmxlABIXt4FoGOrRk7O49d7DkVTMhcA7mSY1spsvrscwWfbTLj/wy5fDcRn+9tv7AYThJKA9Y8E37P58RlzNZhLafTDG8JHv7BfzItIwgzYSRFQfhgoEbv91WRiGkjwLIKpDnJmtIWdo2DBUFH9fy/FwfKqC6y4aiZzPNat2KoTzKZ5F0jlHJsuJGUl80z0R/NvaOFQQ6zk9U8NPD0/iV6/aCAC4OxCR+TjUUclbiFRwB9+T6XhCWG+UDQWEBXGNPIvpioWZqg2NgN5YSOjVV2/GLz1jHd5+w0XYuWkQ7/nFHXjVVRsi52xeVcRfvfLKOjEeiIbC4mgaQddIaCuKzqKMRYyyFPOOh6G+/8RZ/McjJ+uew/WKDYOFSBhqz1E/bXCxQlM8LXdibvGMxWTZgsf8K8ctIz1443Vb8e8PHsfPjs+IjZeozTCU6YDIN7i8ziCqWfhhqBPTVXzse0/jKw8db/h6puMip/OrSAO6RpKxkOo3PE+smYueSbUWPGW0P28Iz+LYVAUeA667aAREobEo1RyRWdUqIgzVYKPjoZea7UWylzhc4OYXIrzB30zVxjf3ngRjwFuv34b+vIGTMzX05w2x2fbnDbHZypXQsmaRJqynrTPe6oPjexYOZoL+WXERfMfaPnz8NVeJzKm33XCRKF5sRCueBQC85urNwntRdBZlLGLwJoI8G8p0PNRsF4wx/OnXHsVH73qy7jknpmsgAq7cMBA1FkG4iG9sC4X3JFpMz4KvjV85vvX6bQCAHz41JuL/a/rybWdD8apavtkYWpg6y7OuDo/7G/KBJp6F5XjiCpnIT+Hk/YgMjcS8Z8f1NYusromNc6CYQX/eqPMs1vbn/dnYwWfk4vb2NX1YP1AQxmW2GnacbRX+mfMNPAv5inltgrHozRnQCEKPCMNQDu4/NIkLR3uwbbRX9HWSN30iwuq+XDBFMCEM5XqpwnocftXeKAzFs6EWc1gQvwBoJHADwF+84gpct31k0d5XkY4yFjF4GKonp4ciY83GQ0encWK6irGEjf/UdBVr+vJYN5iPGIuHhbFYnM2dh6EmOmgsBgoZ9OUMjJVMcZW+YaiA2Zqd2JAvibLpiCt7vilmjPpGgryu4UATTcef+BZurgOFDMZL0Wwo/rpzpi1CUJytIz2R9zhbMrE6GGbEPYtDgbHYOtKDTcOFSBiqL9/eJlhoQbOQjUWSbkBE6C9khB62diAPjfz1PHmmhEvX+UaC93WKh4lW9+XqwmAZSeBuVmMRX2e81Qenv5AJjE9tUY0Fv7BYzNnaioWxVDO4NxHR94noMSLaR0TvCo5/kIhOENGe4Oemc702WeAW6YsVW4SfSjWnfg7BTBXrBvNY3ZfHnOmgYjmYLFuiAdtihaF49etkC51SW4WvTb5yHO3LYXzOFBvpxqFCkOPfmndRNl3RgoFvmFldEx4AD0Pxq/kjE5WGorcptb4GfG+BF6r5vaEkYxGEjWQuXtOHJ4NhQowxnJmtYU3gWfCiy8MTZfTnDQwVM5HajNmqI8TtVmlJs8jKxiJ50x4oZEQYqi/ve7qnZmo4OlnBjqBdN58cF2/FMdqXi3xnQCwMVao1FbcBKQyV4llwDenYZGVRjQUR4UWXrMauC4aan6w4JyyVZ+EA+EPG2GUAngvgbUR0WfDYRxljO4OfO8/1woRnkTVw0WgviICPfOdJ/OfPTomrnXhY6eR0DesHC2LDHSuZ2HPM1yu2jfYsWhhqsqNhqDAmPdKbi3gWm4b8uH+rusWcGW7Y/Mowo5M/p0CqtubGwvGYMKxJWI4b2XjlSmLeGwrww1DylDzOjrV9GJ+zMD5nBsbcxZr+HPryGTEj/PB4BVtHekBE2DxcxFjJRNVyIx1nW0WEoRplQ0me0uqUTbs/HxpFfvHy0JEpMOYbQCD0LOJX/tdsG6nrnBoXuJuJ20D492skcAPAqdlapMZiMfjM65+Nl165blFfUzF/lsRYMMZOMcYeCm6XADwOYEPjZ50bKpYvzuYzGi5b34/333QpvrXvNMZKJl72TP8frhxWYozh5HQV6wfyWB0Yi7MlE3uOTkMjf3LZZMVqOHf6+0+cjQwLSoNv1ospcI+VTOQMLbLBjvRlA8/CXxPPxJF1i2/uPYlP33MIX/jp0brPVg40CyDcOMMJaiTOPzRRFvOxG4Wi6jwLaVOKd7NNCkPx4rknz5QiWUBci5irOTg8URatO/gcg2NTFZRqTtubIBf185kGmkXwvegaiTkYcWSPpi9IuDgYGFj+mbaN9uDiNb2RtFMAeN01W/Avr90VOca/w6rlYnzOTBTWk9ZpJLT64PC/BWNIPUfRHSy5ZkFEWwA8C8D9waG3E9FeIvoMESX6oER0MxHtJqLdY2Nji7oeWZwFgDddtxW/+/yt2LKqiN/YtQkARLwc8K/yTcfPg+dXiGdnTTx8bBo71vZj83ARjKV7A6dnanjDrQ/gCz892nRtIhtqUT0LfwKaXLU90pvD+JwljMWGwFjwZoJ7jk3j7Z97GH/xzcfw3q/+TOT5c+ZMR4ShhGYRbOj+CFQGx/VwbLIixoA2Sp81bS/qWRTl+g0Ks6GC1Nk6zyK4Cn/ydEnMtt44VBBaxETZwsnpKi4IRnbylt1PnZmLjFRtlbAor7lmwRtWJiEbxd68IYxHPqOJLC9D1/Cd338hXr6z+bUWv5j55t5TYKx5jQXgf3fP3DRYl+WUtMbFDEMpzj+W1FgQUS+ArwB4N2NsFsAnAWwDsBPAKQAfSXoeY+wWxtguxtiu0dHRRV1TxXQjvWaICO//pcvwvT+8XlxxymGlUzO+ALluoCBCAU+fncMDhyfx7C1DYWgqJRTFhdUDY+XI8dMzNfEY4NdDzFRt0Qm33cK2NMalOQic0d4cZqo2Jst+Hx/e5I5rJv/640PozRn47z94IQBg/+lS7DUtMQktnwnDUP5vv5ndyekabJfhig39WNufb+hZWG69wM0xdE00lHO8ZM1itC+HoWIG+8+U8IP9Y+jLG3jGxkHRn2j/6RI8FnoUl6ztR9bQ8PDRqchI1VYJBe7m2VCNNmwe/tLIP59/7u2r+1INTCM2DRfxqqs24I6HT/jv3YLA/eYXbsNX3npN6uPKWKwclsxYEFEGvqG4nTH2VQBgjJ1hjLmMMQ/ApwBcfa7XVbacxF4zmkZikLssWPOJZBsGCxgqZmFohFvvPYSa7eG3nr1ZbMTjcxaqliuEVg5P0Tw8HjUW/+vrj+LN/7Zb3J+t2vAYcEGwobXbCv3oRKVuHCj/LHFjwcXMwxMV9OYMcSU/W7VxZraG/9x7Cr++ayMuWt2Ltf157Jc+09lSDeNzJi4JsnXinkVWJ9iOJzKhtqzqwbbVPXXGUsZ03EgYarAoh6Eolg3l1IWhiAgXr+nDE6dL+P7+s3jB9lFkdE14FvtOzgAIazKyhobL1/fjxwcm4DG0rVnkW9Esgv5RjUJBfPPtzfmeLl/HxWuSe1K1wh/feInQIVrRLJqhjMXKYamyoQjApwE8zhj7B+m4rGa9EsCj53ptZdNJ7Y+fz+joyxsRz+JQUCtwwUgRmkYY7cthqmLjOVuHcdn6frERj5VM3PLDg3jZx+5BVRoIw7NueIM5zhOnZ3FgrCxaXXDjsG11L4D2Re7f+fT9+Ms7H687Pj5n1uXQ8zUfHi+jV8oKm67YuP2+I3AZw+uv2QIAuHhtX8SzCEd1BsYirlkYGhyPCeO4ZcSvFzhwdk60zo5jOdEwVESzCCblAX4YqlRz0JdQbb1jbR/2HJvG2ZKJ63f43igP6/A1b5ZmLj9r0xAeP+Uf72szDFXM6BgqZkRtRBIteRbB5+RGjX/uS1IaGLbCmv483vni7cjqmtCiFoKcVqyMRXezVJ7FtQBeC+BFsTTZvyWinxHRXgA3APj9c72wsuU27GLpp5WGG/WRiTJW9WTFVR/feN9w7dbI/fE5P0PKcjwcmQwNAzcWp2ZqwojUbBfHp6pwPYajwbm8xmLbaGAs2hC550wHRycrYvPj+K0+LIzGqnP5mo9MVNCXN5DRNfRkdZwp1XD7/Ufx4ktWCzH4krV9eOrsnKjBeCzYeHmxmFxnAfhFdJbr4dB4GcWsjtV9OVy0uhdzppPasdd04ppFPBsqEG5tF6bjJbbm2LG2TwwEeiE3FpJnkdW1yJW23AG13TCUoWu4+49uEBpXEoWMjv68n3GXBtdK+Ofh67h4AcYCAN78ggvxk/e9qK6h33yQ240rY9HdLElTFcbYPQCSgq7nPFU2TsVyGhYr8bRSzqHxMrZIM4wvWNWDmaqNX7hsDQA/5bGQ0TFeMsUV7OHxMi5Z62+mcmXxkUn/+KHxstjYnj5bxkWr+0QIaduo/17tiNy8QvrA2Jxove2/hgmP1efQ8zRay/XERjBYzOLre06iVHOEIQT8kIjleDg8Uca20V7sOzmDzcNFsRGHdRahZmEH51+wyk9V5Rvm46dmE0Mjph3Phoo1KAxem2sq8TAUEIrcV2zoF39f/tnG5yxcONIT0QGeJXVAbTcM1cpzDF3Dd//w+ojhq3sNHoYK1rl+MI+MTrh03cKMBRFhVUoq7HwYKGTmlTWmWF4seTbU+YLleHj67BymK3bDqtHR3lwkDHV4oowt0rS0D738Cvz7W66JbDwjfVk8dmpWXDnz0BXgaxZXBh1MeWhGzgziwi8PQ100jzAUf42a7eHkTNj0kLf5jufoyxoGv6rlG8KONX24Ztsq8bicaQQAj56YxRUb+sXj8TBUNpg98dSZOVwYGNlnbBqErhEeODyZuP5GArecOjsVTGFLmsl88do+ZHUNP3/pmrrPBoTiNmfjUEEYzXaL8lpltC/XsN0G33z55/nlZ6zHd//g+pYqr88l/O+hPIvuRhkL+GGaV37ix/j5f7gbx6eqkfnAcUb7ciKzqWI5ODNrYutIuNEMFDN1GsBobw67D4eziLlRmDMdTJQtvOBiv7cNNyIHxuZABKzqyQqvgNdY+IVjbXoWY7LxCUNgoiAvtl6uzQBAbyxe/vprt0TSbC9a7Rcu7j9TwmzNxtHJiigUA8IwlCHqLDTsP13CiekqnhcYnd6cgSs3DOD+g1FjUbNdOK4H03YTw1BEfhjEiHkWSZpFfz6D/3zndXjLC7eJY0YQXgMg0mY5RISdweS2+XgWiwH/zvnnMXQNm2PrPB8QxqKBl6RY/qz43r6O6+Fttz+EJ06X8GcvuwyjfbnIlXMcfwaC3/KDN8KTw1DJz8mJ6XA71vSJTCCeCXXZugGM9GaFETkwVsbGoQIuGA57Gk1WLBgaYaCQwWAhE2n5wecyp+XCHzhbxnBPFpNlC0+fncMLL/Zj9jycllSdO9qb88XifNhIbrCYwSti+fyFrI4tq3qw/3SpTq8AQmMRhqEIJ4N0Yy40A8BzLhzGZ+45hKrlwtAJt993BB/976dw6bo+1FKK8jKSAQIgXjetl9P2hCyivnwGZcuNiNucXVuG8L0nzjS8eOgk3Ei10x59KRgoZBLbkyu6ixXvWXz6nkO4+8kxfOgVV+CN123FLz9zfcN4bpgKa4oMJjkMlQT3NC5YVcSVGwciPZEAPwtny6oe8XoHzs5h22gvto36KaWMMUxXLAwWsyAisfEDfgX5yz52D9771b1178uzi54em8PPXTCEwWIm4mU8FXgtSR1F+efkV7Xvfekl+OLNzxNhJZmL1/Ri/+kS9h73GydeLhmLfF0Ftyaes3Eo3KCfu3UVbJfhoaNT+J9ffgQf/I/HcMGqIu47OAnXY5EwVEb3K855FhTvDXXXY2eQz2h11cyN4MYwHoYCgNdfswVfuPl5S3bFPBDTLM5XRnpzWNWbS71YUXQHK9pYMMbw7w8ex9VbhvHqFnrsA3J2kyWK5lrxLAB/E9060oOzJRNl0xGexebhIi4IjIXnMRwcn8NFo70iS+jMrImpsi0K3Vb15ETLj73HZ/DYqVl8afdxPHgkDHX9w11P4oa//wGmyhaOTJRx0epeXBSkqAJ+0d9t9x7GjZevTbxy5Z+TP7Z+sJA6c3rH2n4cHC/jr+58Aqv7cpGYejGTbCziMwh2bRmCRsAnfvA0vr7nJH7v+m34+tuuxXt+cYf/OjEjNVDIiNAWD0PNVG28+NI1iZpFGtxYJHkW+Yxe11/pXPL/2rv3KKvKMo7j3x9zA+bGghnkJjCAKAxiApEGigkh4AUrSiszs9VNLc2VZdnFqLVK7bKyWmqtLDNNu7nCLqRS2d1EboKBIFIidzBkGLnO0x/7PYczhzmzh2E8Z0/n+ax11ux5z97nPPOuPec57/vu/b41vaK1O5I+jcY1547iu5dPit/RdWvJ/sryKluzdQ9rtzXxhYvHdfiYzPsmNuzYS311RWw3QWpMoHFQbboVsmHnXv6zq5manqXU9i6joa43P1+yn3Xbm9h3sIWR/avSN+A9t72Jl5oPpLtD+laWp1sIDy/fRFmJ6NO7nPkPr+Khq6bw4OIXuH3RWgC++ugaDh42RtZXsavpAItWR6v23bJwNYfNuOn8tteSTg3uduRb7eVnDqO6opRDLcZpJ7ZebjR79tnUlUvTTm595311zzLGDa7lr+t2MrhPLz587klI4qpzRjK8XyVnjGj9oV0b1hqJXvPId54Lxw+KjTdTahC5rZZFoVWUlvD9K17bqqWWRCfU9OySG/xcshV1sliwbBMlPcSccQM6fExddetuqOEdGHBMzckzdlBNenvDjmb+s6s5PWDZUBdd5fTl364GovspUoOuqWSRSjT9qsp5csMBWlqMX63YzLTR/Zlz6gCu/8lyxn5uIfsOtjBtdD179h3k/ieiOadG9a9i1979PLj4AAuWb+KhpS9y9RtG5vyQTHdDdWBwt66qgvedPaLN5yY39OXWeeOZGKaarigtoaqilEnDjv7G/rqGvqzYuJsbZ5+S7u6SxPnjj555tE/vsvQVYqkEVF1R2mocpCNqe5XRr7I8seMCZ4/u2ulsnOusZP6H5IGZ8fCKTUwZVXdM15ynpvzYsWc/z+9o5g0d+HCaNrqemy8cy1mj6tLfhp/ZvJvVW15Of4hOH9OfWY0DWLhqCxDdT9G3spzqilKeWL+Ll5oPMmFo9N79qirY1XyArz36LFte3senzh/DBacOpCl0bdX0LOM9Uxv445ptXHP/UgBG1FeyM1z9dN0DSxkzsIarzhmVM+a6rG6ozirpoVY3p31g2gjedPrgo9ZaALhyagPD+lVyQRvJIVuf3mXp10i1LM4bN6DdKTba8qFzRnLx6YmY8Ni5RCvaZLHshf/ywq5XuHb66GM6rme48/auP62naf+h2PGK1DFXhBvZSkt60L+6gjsfX4+Zcclrh6b3ufNdE/nbuh1s2NmcTmCXnTmMO/74HHBkttW3ThzC71Zu4Vt/WEevshJmjOlPjx7i8jOHt3rf8xoHMLC2J4dbjJqeZel7NOqrK7j7iknt9u3Xp1sWXXuKNA6qpTFHT9HA2l5cdsawDr3OB6eNZEu4+qmyopT5cxs7tRbzKQNq0jdIOudyK9pkUVbSg1mNA5jZeEL8zlk+MfsUlvz7v5SViItOO7Y+cogGxLft2c/8uY3py1hTXj+qjtdnfOG/YebJPL99LwtXbUkPcJ/Ytze//shUfvbURnqVl6TXSc5WVtKDW+eNT185NbRvbz46YzSzTx3AwNr25wWaMqqOj0w/iUnDk7lS2fghfRg/5Mjv2YnSOde1lGvytu5i0qRJtnjx4vgdE+SxZ7ay8aXmdGsjzisHDvP1x57l7ZOHptdacM654yHpKTPr8GVsRduyKKQZY4+tNdOrvIRPzWn7qiXnnMuHor7PwjnnXMd4snDOORfLk4VzzrlYniycc87F8mThnHMulicL55xzsTxZOOeci+XJwjnnXKxufwe3pO3Avzt5eB2wowvDyQePOX+6Y9wec378P8Q8zMw6PK1xt08Wx0PS4mO53T0JPOb86Y5xe8z5UYwxezeUc865WJ4snHPOxSr2ZPGdQgfQCR5z/nTHuD3m/Ci6mIt6zMI551zHFHvLwjnnXAd4snDOOReraJOFpFmS1khaJ+nGQsfTFkknSvqDpGckrZJ0bSi/WdKLkpaFx5xCx5pJ0gZJT4fYFoeyvpIelbQ2/EzMeq2STs6oy2WSXpZ0XdLqWdLdkrZJWplR1ma9KnJ7OL9XSJqQoJhvk7Q6xPWQpD6hfLikVzLq+84ExZzzXJD0yVDPaySdl6CYH8yId4OkZaG8c/VsZkX3AEqA54ARQDmwHBhb6LjaiHMgMCFsVwPPAmOBm4GPFTq+duLeANRlld0K3Bi2bwRuKXSc7ZwbW4BhSatn4GxgArAyrl6BOcBvAQFnAE8kKOaZQGnYviUj5uGZ+yWsnts8F8L/43KgAmgInyslSYg56/mvAp89nnou1pbFZGCdma03swPAA8DcAsd0FDPbbGZLwvYe4F/A4MJG1WlzgXvC9j3AxQWMpT3TgefMrLOzArxqzOxPwK6s4lz1Ohf4oUX+AfSRNDA/kR7RVsxm9oiZHQq//gMYku+42pOjnnOZCzxgZvvN7HlgHdHnS161F7MkAW8Dfnw871GsyWIw8ELG7xtJ+IewpOHA6cAToeia0Iy/O0ldOoEBj0h6StL7Q9kJZrY5bG8Bjm0h8vy5lNb/VEmuZ8hdr93lHL+SqAWU0iBpqaTHJZ1VqKByaOtc6A71fBaw1czWZpQdcz0Xa7LoViRVAT8HrjOzl4E7gJHAa4DNRE3MJJlqZhOA2cDVks7OfNKitnDirtmWVA5cBPw0FCW9nltJar3mIukm4BBwXyjaDAw1s9OB64H7JdUUKr4s3epcyPJ2Wn8B6lQ9F2uyeBE4MeP3IaEscSSVESWK+8zsFwBmttXMDptZC/BdCtDsbY+ZvRh+bgMeIopva6obJPzcVrgIc5oNLDGzrZD8eg5y1Wuiz3FJVwAXAO8MSY7QlbMzbD9F1P8/umBBZmjnXEh6PZcCbwYeTJV1tp6LNVk8CZwkqSF8m7wUWFDgmI4S+hq/B/zLzL6WUZ7Z9/wmYGX2sYUiqVJSdWqbaDBzJVH9vjvs9m7gl4WJsF2tvoEluZ4z5KrXBcDl4aqoM4DdGd1VBSVpFvBx4CIza84or5dUErZHACcB6wsTZWvtnAsLgEslVUhqIIr5n/mOrx0zgNVmtjFV0Ol6zveofVIeRFeLPEuUVW8qdDw5YpxK1K2wAlgWHnOAe4GnQ/kCYGChY82IeQTR1SHLgVWpugX6AYuAtcBjQN9Cx5oVdyWwE6jNKEtUPRMlss3AQaK+8ffmqleiq6C+Hc7vp4FJCYp5HVE/f+qcvjPs+5ZwziwDlgAXJijmnOcCcFOo5zXA7KTEHMp/AHwwa99O1bNP9+Gccy5WsXZDOeecOwaeLJxzzsXyZOGccy6WJwvnnHOxPFk455yL5cnCuU6QNF/SjC54naauiMe5V5tfOutcAUlqMrOqQsfhXBxvWTgXSLpM0j/DHP93SSqR1CTp64rWE1kkqT7s+wNJ88L2lxWtObJC0ldC2XBJvw9liyQNDeUNkv6uaL2PL2a9/w2SngzHfD6UVUr6taTlklZKuiS/teJcxJOFc4CkMcAlwBQzew1wGHgn0Z3di82sEXgc+FzWcf2Ipn9oNLPxQCoBfBO4J5TdB9weyr8B3GFmpxLdcZt6nZlE0y5MJpqsbmKYgHEWsMnMTjOzccDCLv/jnesATxbORaYDE4Enw4pi04mmLmnhyCRsPyKagiXTbmAf8D1JbwZScx2dCdwftu/NOG4KR+afujfjdWaGx1KiKRhOIUoeTwNvlHSLpLPMbPdx/p3OdUppoQNwLiFE1BL4ZKtC6TNZ+7Ua5DOzQ5ImEyWXecA1wLkx79XWQKGAL5nZXUc9ES2JOgf4oqRFZjY/5vWd63LesnAusgiYJ6k/pNe2Hkb0PzIv7PMO4C+ZB4W1RmrN7DfAR4HTwlN/I5rNGKLurD+H7b9mlaf8DrgyvB6SBkvqL2kQ0GxmPwJuI1o607m885aFc4CZPSPp00Qr/PUgmr3zamAvMDk8t41oXCNTNfBLST2JWgfXh/IPA9+XdAOwHXhPKL+WaLGZT5AxTbuZPRLGTf4ezUxPE3AZMAq4TVJLiOlDXfuXO9cxfumsc+3wS1udi3g3lHPOuVjesnDOORfLWxbOOediebJwzjkXy5OFc865WJ4snHPOxfJk4ZxzLtb/AOwRF0UJ3Th3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa34a8beed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLyN6v7X-Zui"
      },
      "source": [
        "## Double Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W4CWiJ3e-ZPn",
        "outputId": "76e80606-ae8c-4c39-976b-e0095a902bab"
      },
      "source": [
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# define the policy (how we select the actions)\r\n",
        "policy = BoltzmannQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               enable_double_dqn=True,          # a boolean which enables a target network as a second network proposed by van Hasselt et al. to decrease overfitting\r\n",
        "               enable_dueling_network=True,     # a boolean which enables duelling architecture proposed by Mnih et al.\r\n",
        "               dueling_type='avg',              # if enable_duelling_dqn is set to true, a type of duellin garchitecture must be chosen. 'avg' is recommended\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   13/10000: episode: 1, duration: 0.708s, episode steps:  13, steps per second:  18, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.917992, mae: 0.814913, mean_q: 0.041791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   42/10000: episode: 2, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.626052, mae: 0.634501, mean_q: 0.082130\n",
            "   56/10000: episode: 3, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.480196, mae: 0.546651, mean_q: 0.152525\n",
            "   74/10000: episode: 4, duration: 0.152s, episode steps:  18, steps per second: 118, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.413478, mae: 0.521755, mean_q: 0.212122\n",
            "   84/10000: episode: 5, duration: 0.085s, episode steps:  10, steps per second: 117, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.368436, mae: 0.528378, mean_q: 0.276493\n",
            "   99/10000: episode: 6, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.318498, mae: 0.535283, mean_q: 0.365938\n",
            "  139/10000: episode: 7, duration: 0.329s, episode steps:  40, steps per second: 122, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.250134, mae: 0.562104, mean_q: 0.560053\n",
            "  149/10000: episode: 8, duration: 0.080s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.216246, mae: 0.606019, mean_q: 0.697019\n",
            "  226/10000: episode: 9, duration: 0.564s, episode steps:  77, steps per second: 136, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.169300, mae: 0.708255, mean_q: 0.956997\n",
            "  242/10000: episode: 10, duration: 0.118s, episode steps:  16, steps per second: 136, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.141301, mae: 0.839005, mean_q: 1.251799\n",
            "  255/10000: episode: 11, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.131732, mae: 0.875692, mean_q: 1.367318\n",
            "  282/10000: episode: 12, duration: 0.199s, episode steps:  27, steps per second: 136, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.120492, mae: 0.942885, mean_q: 1.533810\n",
            "  293/10000: episode: 13, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.112279, mae: 0.993702, mean_q: 1.672433\n",
            "  305/10000: episode: 14, duration: 0.109s, episode steps:  12, steps per second: 110, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.133952, mae: 1.069576, mean_q: 1.826881\n",
            "  325/10000: episode: 15, duration: 0.184s, episode steps:  20, steps per second: 109, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.147690, mae: 1.127017, mean_q: 1.901826\n",
            "  340/10000: episode: 16, duration: 0.114s, episode steps:  15, steps per second: 131, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.165350, mae: 1.212308, mean_q: 2.045430\n",
            "  351/10000: episode: 17, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.161245, mae: 1.242202, mean_q: 2.132883\n",
            "  372/10000: episode: 18, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.201194, mae: 1.321317, mean_q: 2.281497\n",
            "  407/10000: episode: 19, duration: 0.271s, episode steps:  35, steps per second: 129, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.186143, mae: 1.400483, mean_q: 2.467401\n",
            "  418/10000: episode: 20, duration: 0.082s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.290402, mae: 1.527539, mean_q: 2.723891\n",
            "  432/10000: episode: 21, duration: 0.117s, episode steps:  14, steps per second: 120, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.189479, mae: 1.544187, mean_q: 2.766972\n",
            "  460/10000: episode: 22, duration: 0.240s, episode steps:  28, steps per second: 117, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.321 [0.000, 1.000],  loss: 0.244699, mae: 1.640189, mean_q: 2.942378\n",
            "  504/10000: episode: 23, duration: 0.319s, episode steps:  44, steps per second: 138, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.221154, mae: 1.760996, mean_q: 3.246451\n",
            "  527/10000: episode: 24, duration: 0.176s, episode steps:  23, steps per second: 130, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.307876, mae: 1.910692, mean_q: 3.564155\n",
            "  543/10000: episode: 25, duration: 0.130s, episode steps:  16, steps per second: 123, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.264027, mae: 1.964340, mean_q: 3.661549\n",
            "  556/10000: episode: 26, duration: 0.092s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.277918, mae: 2.021143, mean_q: 3.796915\n",
            "  589/10000: episode: 27, duration: 0.257s, episode steps:  33, steps per second: 128, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.358441, mae: 2.142334, mean_q: 3.939632\n",
            "  608/10000: episode: 28, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.228178, mae: 2.209279, mean_q: 4.166021\n",
            "  626/10000: episode: 29, duration: 0.150s, episode steps:  18, steps per second: 120, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.464156, mae: 2.339322, mean_q: 4.381361\n",
            "  649/10000: episode: 30, duration: 0.187s, episode steps:  23, steps per second: 123, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.401987, mae: 2.391407, mean_q: 4.397553\n",
            "  662/10000: episode: 31, duration: 0.122s, episode steps:  13, steps per second: 107, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.395121, mae: 2.471432, mean_q: 4.576924\n",
            "  717/10000: episode: 32, duration: 0.422s, episode steps:  55, steps per second: 130, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.366931, mae: 2.577874, mean_q: 4.828915\n",
            "  735/10000: episode: 33, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.499071, mae: 2.738926, mean_q: 5.087242\n",
            "  764/10000: episode: 34, duration: 0.213s, episode steps:  29, steps per second: 136, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.650169, mae: 2.851495, mean_q: 5.215827\n",
            "  782/10000: episode: 35, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.512240, mae: 2.894100, mean_q: 5.375744\n",
            "  798/10000: episode: 36, duration: 0.134s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.335415, mae: 2.931417, mean_q: 5.560134\n",
            "  812/10000: episode: 37, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.436635, mae: 3.010283, mean_q: 5.689063\n",
            "  823/10000: episode: 38, duration: 0.085s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.544878, mae: 3.064301, mean_q: 5.788363\n",
            "  843/10000: episode: 39, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.556795, mae: 3.128651, mean_q: 5.865174\n",
            "  856/10000: episode: 40, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.903800, mae: 3.243784, mean_q: 5.999634\n",
            "  872/10000: episode: 41, duration: 0.130s, episode steps:  16, steps per second: 123, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.612030, mae: 3.243597, mean_q: 6.054274\n",
            "  892/10000: episode: 42, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.718914, mae: 3.321275, mean_q: 6.246188\n",
            "  914/10000: episode: 43, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.595344, mae: 3.384113, mean_q: 6.367006\n",
            "  931/10000: episode: 44, duration: 0.141s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.654224, mae: 3.457117, mean_q: 6.528709\n",
            "  952/10000: episode: 45, duration: 0.161s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.670963, mae: 3.523985, mean_q: 6.679377\n",
            "  965/10000: episode: 46, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.606599, mae: 3.583814, mean_q: 6.810776\n",
            " 1003/10000: episode: 47, duration: 0.272s, episode steps:  38, steps per second: 140, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.821953, mae: 3.700627, mean_q: 6.894377\n",
            " 1023/10000: episode: 48, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.514090, mae: 3.747447, mean_q: 7.112623\n",
            " 1037/10000: episode: 49, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.663197, mae: 3.832803, mean_q: 7.310805\n",
            " 1051/10000: episode: 50, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.767982, mae: 3.893032, mean_q: 7.341092\n",
            " 1067/10000: episode: 51, duration: 0.130s, episode steps:  16, steps per second: 123, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.821216, mae: 3.946623, mean_q: 7.374059\n",
            " 1091/10000: episode: 52, duration: 0.174s, episode steps:  24, steps per second: 138, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.854666, mae: 3.998414, mean_q: 7.465658\n",
            " 1102/10000: episode: 53, duration: 0.096s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.074148, mae: 4.087769, mean_q: 7.566226\n",
            " 1114/10000: episode: 54, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.857501, mae: 4.090934, mean_q: 7.643296\n",
            " 1150/10000: episode: 55, duration: 0.259s, episode steps:  36, steps per second: 139, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.892424, mae: 4.154570, mean_q: 7.808924\n",
            " 1171/10000: episode: 56, duration: 0.174s, episode steps:  21, steps per second: 121, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.132891, mae: 4.279503, mean_q: 8.050735\n",
            " 1192/10000: episode: 57, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.810423, mae: 4.308396, mean_q: 8.136434\n",
            " 1204/10000: episode: 58, duration: 0.087s, episode steps:  12, steps per second: 138, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.927448, mae: 4.367064, mean_q: 8.234905\n",
            " 1231/10000: episode: 59, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.014067, mae: 4.440403, mean_q: 8.378521\n",
            " 1258/10000: episode: 60, duration: 0.214s, episode steps:  27, steps per second: 126, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.128062, mae: 4.537953, mean_q: 8.531642\n",
            " 1293/10000: episode: 61, duration: 0.248s, episode steps:  35, steps per second: 141, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.151463, mae: 4.647499, mean_q: 8.709942\n",
            " 1314/10000: episode: 62, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.304885, mae: 4.741380, mean_q: 8.853585\n",
            " 1341/10000: episode: 63, duration: 0.196s, episode steps:  27, steps per second: 137, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.038487, mae: 4.776356, mean_q: 9.025831\n",
            " 1357/10000: episode: 64, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.173699, mae: 4.847388, mean_q: 9.133219\n",
            " 1373/10000: episode: 65, duration: 0.135s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.235663, mae: 4.891099, mean_q: 9.221494\n",
            " 1398/10000: episode: 66, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1.249847, mae: 4.972429, mean_q: 9.385491\n",
            " 1422/10000: episode: 67, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.556925, mae: 5.050457, mean_q: 9.367511\n",
            " 1457/10000: episode: 68, duration: 0.248s, episode steps:  35, steps per second: 141, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.272925, mae: 5.111236, mean_q: 9.658902\n",
            " 1471/10000: episode: 69, duration: 0.106s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.703296, mae: 5.216021, mean_q: 9.637126\n",
            " 1541/10000: episode: 70, duration: 0.500s, episode steps:  70, steps per second: 140, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.274158, mae: 5.278612, mean_q: 9.970959\n",
            " 1553/10000: episode: 71, duration: 0.091s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.700706, mae: 5.452921, mean_q: 10.258335\n",
            " 1576/10000: episode: 72, duration: 0.171s, episode steps:  23, steps per second: 134, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.628593, mae: 5.486588, mean_q: 10.325541\n",
            " 1605/10000: episode: 73, duration: 0.198s, episode steps:  29, steps per second: 147, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.545620, mae: 5.528626, mean_q: 10.446900\n",
            " 1621/10000: episode: 74, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.472977, mae: 5.583940, mean_q: 10.569466\n",
            " 1643/10000: episode: 75, duration: 0.165s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.425520, mae: 5.661082, mean_q: 10.771894\n",
            " 1710/10000: episode: 76, duration: 0.471s, episode steps:  67, steps per second: 142, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.460324, mae: 5.772555, mean_q: 10.961368\n",
            " 1758/10000: episode: 77, duration: 0.347s, episode steps:  48, steps per second: 138, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.727072, mae: 5.939240, mean_q: 11.204727\n",
            " 1794/10000: episode: 78, duration: 0.273s, episode steps:  36, steps per second: 132, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.128771, mae: 6.081883, mean_q: 11.378065\n",
            " 1814/10000: episode: 79, duration: 0.169s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.612088, mae: 6.084235, mean_q: 11.457819\n",
            " 1828/10000: episode: 80, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.915565, mae: 6.218478, mean_q: 11.527855\n",
            " 1844/10000: episode: 81, duration: 0.140s, episode steps:  16, steps per second: 114, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.431805, mae: 6.125843, mean_q: 11.563309\n",
            " 1862/10000: episode: 82, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 2.294695, mae: 6.255750, mean_q: 11.711773\n",
            " 1874/10000: episode: 83, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.850037, mae: 6.180370, mean_q: 11.589917\n",
            " 1895/10000: episode: 84, duration: 0.166s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.258001, mae: 6.244318, mean_q: 11.953901\n",
            " 1953/10000: episode: 85, duration: 0.428s, episode steps:  58, steps per second: 136, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.893022, mae: 6.393950, mean_q: 12.155290\n",
            " 1978/10000: episode: 86, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.099577, mae: 6.517728, mean_q: 12.419361\n",
            " 2009/10000: episode: 87, duration: 0.233s, episode steps:  31, steps per second: 133, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.483873, mae: 6.528469, mean_q: 12.480483\n",
            " 2043/10000: episode: 88, duration: 0.262s, episode steps:  34, steps per second: 130, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.890863, mae: 6.666950, mean_q: 12.707197\n",
            " 2058/10000: episode: 89, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.898780, mae: 6.731167, mean_q: 12.911585\n",
            " 2086/10000: episode: 90, duration: 0.212s, episode steps:  28, steps per second: 132, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.743587, mae: 6.836954, mean_q: 12.866632\n",
            " 2194/10000: episode: 91, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.564039, mae: 6.935910, mean_q: 13.117371\n",
            " 2209/10000: episode: 92, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.037246, mae: 7.089735, mean_q: 13.560180\n",
            " 2231/10000: episode: 93, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.949506, mae: 7.076099, mean_q: 13.595200\n",
            " 2254/10000: episode: 94, duration: 0.166s, episode steps:  23, steps per second: 138, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 2.317508, mae: 7.191843, mean_q: 13.757744\n",
            " 2302/10000: episode: 95, duration: 0.339s, episode steps:  48, steps per second: 141, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.333333, mae: 7.272786, mean_q: 13.859876\n",
            " 2317/10000: episode: 96, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.707425, mae: 7.410615, mean_q: 14.126761\n",
            " 2339/10000: episode: 97, duration: 0.184s, episode steps:  22, steps per second: 120, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.230999, mae: 7.401451, mean_q: 14.124564\n",
            " 2368/10000: episode: 98, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.251563, mae: 7.546544, mean_q: 14.467861\n",
            " 2388/10000: episode: 99, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.085053, mae: 7.559712, mean_q: 14.523488\n",
            " 2411/10000: episode: 100, duration: 0.179s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.833430, mae: 7.672317, mean_q: 14.565851\n",
            " 2479/10000: episode: 101, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.748907, mae: 7.727945, mean_q: 14.691849\n",
            " 2499/10000: episode: 102, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.601191, mae: 7.774539, mean_q: 14.814287\n",
            " 2544/10000: episode: 103, duration: 0.323s, episode steps:  45, steps per second: 139, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.189399, mae: 7.916090, mean_q: 15.025269\n",
            " 2584/10000: episode: 104, duration: 0.310s, episode steps:  40, steps per second: 129, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.272618, mae: 7.983687, mean_q: 15.193250\n",
            " 2618/10000: episode: 105, duration: 0.277s, episode steps:  34, steps per second: 123, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 2.818216, mae: 8.035419, mean_q: 15.324886\n",
            " 2641/10000: episode: 106, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.519647, mae: 8.149355, mean_q: 15.519774\n",
            " 2663/10000: episode: 107, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.773415, mae: 8.131059, mean_q: 15.530475\n",
            " 2697/10000: episode: 108, duration: 0.269s, episode steps:  34, steps per second: 126, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.405300, mae: 8.213366, mean_q: 15.833484\n",
            " 2781/10000: episode: 109, duration: 0.636s, episode steps:  84, steps per second: 132, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.395167, mae: 8.386061, mean_q: 15.991926\n",
            " 2806/10000: episode: 110, duration: 0.193s, episode steps:  25, steps per second: 129, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.949242, mae: 8.552982, mean_q: 16.432951\n",
            " 2843/10000: episode: 111, duration: 0.279s, episode steps:  37, steps per second: 133, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.395314, mae: 8.566670, mean_q: 16.367825\n",
            " 2861/10000: episode: 112, duration: 0.151s, episode steps:  18, steps per second: 119, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.950865, mae: 8.667149, mean_q: 16.713020\n",
            " 2885/10000: episode: 113, duration: 0.185s, episode steps:  24, steps per second: 130, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.729721, mae: 8.731452, mean_q: 16.691874\n",
            " 2935/10000: episode: 114, duration: 0.386s, episode steps:  50, steps per second: 129, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.266950, mae: 8.794989, mean_q: 16.904491\n",
            " 2951/10000: episode: 115, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.962614, mae: 8.884296, mean_q: 17.083656\n",
            " 3017/10000: episode: 116, duration: 0.531s, episode steps:  66, steps per second: 124, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.430198, mae: 8.974142, mean_q: 17.246029\n",
            " 3035/10000: episode: 117, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.953859, mae: 9.085231, mean_q: 17.191299\n",
            " 3061/10000: episode: 118, duration: 0.211s, episode steps:  26, steps per second: 123, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.450600, mae: 9.065533, mean_q: 17.371983\n",
            " 3090/10000: episode: 119, duration: 0.232s, episode steps:  29, steps per second: 125, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.523521, mae: 9.207992, mean_q: 17.693035\n",
            " 3120/10000: episode: 120, duration: 0.227s, episode steps:  30, steps per second: 132, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.297244, mae: 9.262618, mean_q: 17.820517\n",
            " 3161/10000: episode: 121, duration: 0.347s, episode steps:  41, steps per second: 118, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.729906, mae: 9.348959, mean_q: 17.966644\n",
            " 3187/10000: episode: 122, duration: 0.227s, episode steps:  26, steps per second: 115, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.349511, mae: 9.446813, mean_q: 18.193300\n",
            " 3284/10000: episode: 123, duration: 0.714s, episode steps:  97, steps per second: 136, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.999660, mae: 9.506186, mean_q: 18.273491\n",
            " 3333/10000: episode: 124, duration: 0.365s, episode steps:  49, steps per second: 134, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.917768, mae: 9.651316, mean_q: 18.524500\n",
            " 3363/10000: episode: 125, duration: 0.228s, episode steps:  30, steps per second: 132, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.998860, mae: 9.821760, mean_q: 18.965532\n",
            " 3394/10000: episode: 126, duration: 0.248s, episode steps:  31, steps per second: 125, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.125855, mae: 9.894231, mean_q: 19.014051\n",
            " 3414/10000: episode: 127, duration: 0.156s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.290582, mae: 9.853028, mean_q: 19.089893\n",
            " 3452/10000: episode: 128, duration: 0.309s, episode steps:  38, steps per second: 123, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 4.527010, mae: 9.966885, mean_q: 19.155462\n",
            " 3467/10000: episode: 129, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.292939, mae: 10.099099, mean_q: 19.302120\n",
            " 3534/10000: episode: 130, duration: 0.520s, episode steps:  67, steps per second: 129, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.866094, mae: 10.120298, mean_q: 19.574657\n",
            " 3581/10000: episode: 131, duration: 0.341s, episode steps:  47, steps per second: 138, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.543321, mae: 10.287180, mean_q: 19.957615\n",
            " 3693/10000: episode: 132, duration: 0.827s, episode steps: 112, steps per second: 135, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.919359, mae: 10.470162, mean_q: 20.127611\n",
            " 3750/10000: episode: 133, duration: 0.404s, episode steps:  57, steps per second: 141, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4.982389, mae: 10.629358, mean_q: 20.423370\n",
            " 3792/10000: episode: 134, duration: 0.302s, episode steps:  42, steps per second: 139, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.835467, mae: 10.706499, mean_q: 20.558176\n",
            " 3894/10000: episode: 135, duration: 0.721s, episode steps: 102, steps per second: 142, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.329850, mae: 10.830407, mean_q: 20.803024\n",
            " 3940/10000: episode: 136, duration: 0.340s, episode steps:  46, steps per second: 135, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.858802, mae: 10.892591, mean_q: 20.964149\n",
            " 3964/10000: episode: 137, duration: 0.195s, episode steps:  24, steps per second: 123, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 5.059352, mae: 11.004811, mean_q: 21.241499\n",
            " 4015/10000: episode: 138, duration: 0.371s, episode steps:  51, steps per second: 137, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.952527, mae: 11.014129, mean_q: 21.395060\n",
            " 4069/10000: episode: 139, duration: 0.393s, episode steps:  54, steps per second: 137, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.996316, mae: 11.186479, mean_q: 21.774843\n",
            " 4109/10000: episode: 140, duration: 0.300s, episode steps:  40, steps per second: 133, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.624286, mae: 11.333524, mean_q: 21.896473\n",
            " 4135/10000: episode: 141, duration: 0.188s, episode steps:  26, steps per second: 138, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.564476, mae: 11.353780, mean_q: 22.125702\n",
            " 4187/10000: episode: 142, duration: 0.405s, episode steps:  52, steps per second: 128, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.063346, mae: 11.458253, mean_q: 22.215816\n",
            " 4219/10000: episode: 143, duration: 0.260s, episode steps:  32, steps per second: 123, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.268621, mae: 11.532456, mean_q: 22.346783\n",
            " 4306/10000: episode: 144, duration: 0.636s, episode steps:  87, steps per second: 137, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 4.375692, mae: 11.691553, mean_q: 22.771046\n",
            " 4347/10000: episode: 145, duration: 0.340s, episode steps:  41, steps per second: 121, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.691548, mae: 11.827211, mean_q: 22.959135\n",
            " 4411/10000: episode: 146, duration: 0.469s, episode steps:  64, steps per second: 136, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.938451, mae: 11.965700, mean_q: 23.297474\n",
            " 4469/10000: episode: 147, duration: 0.461s, episode steps:  58, steps per second: 126, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.006470, mae: 12.094557, mean_q: 23.544243\n",
            " 4488/10000: episode: 148, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.007804, mae: 12.248820, mean_q: 23.898325\n",
            " 4515/10000: episode: 149, duration: 0.204s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.512403, mae: 12.260957, mean_q: 23.705862\n",
            " 4555/10000: episode: 150, duration: 0.325s, episode steps:  40, steps per second: 123, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.716295, mae: 12.285620, mean_q: 23.702068\n",
            " 4609/10000: episode: 151, duration: 0.421s, episode steps:  54, steps per second: 128, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 6.369361, mae: 12.387291, mean_q: 23.948275\n",
            " 4662/10000: episode: 152, duration: 0.402s, episode steps:  53, steps per second: 132, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.979122, mae: 12.415411, mean_q: 24.168039\n",
            " 4712/10000: episode: 153, duration: 0.388s, episode steps:  50, steps per second: 129, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 6.832522, mae: 12.572523, mean_q: 24.303135\n",
            " 4770/10000: episode: 154, duration: 0.437s, episode steps:  58, steps per second: 133, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 5.930170, mae: 12.639393, mean_q: 24.520943\n",
            " 4794/10000: episode: 155, duration: 0.181s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 6.758272, mae: 12.651248, mean_q: 24.517408\n",
            " 4831/10000: episode: 156, duration: 0.282s, episode steps:  37, steps per second: 131, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 5.114974, mae: 12.686693, mean_q: 24.695047\n",
            " 4956/10000: episode: 157, duration: 0.920s, episode steps: 125, steps per second: 136, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 6.180032, mae: 12.925237, mean_q: 25.082029\n",
            " 5038/10000: episode: 158, duration: 0.599s, episode steps:  82, steps per second: 137, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.449680, mae: 13.041696, mean_q: 25.314585\n",
            " 5184/10000: episode: 159, duration: 1.085s, episode steps: 146, steps per second: 135, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 6.194456, mae: 13.284040, mean_q: 25.865091\n",
            " 5245/10000: episode: 160, duration: 0.427s, episode steps:  61, steps per second: 143, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.386804, mae: 13.385613, mean_q: 26.235701\n",
            " 5288/10000: episode: 161, duration: 0.337s, episode steps:  43, steps per second: 128, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.488040, mae: 13.485855, mean_q: 26.386055\n",
            " 5431/10000: episode: 162, duration: 1.057s, episode steps: 143, steps per second: 135, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.904742, mae: 13.739702, mean_q: 26.694168\n",
            " 5529/10000: episode: 163, duration: 0.710s, episode steps:  98, steps per second: 138, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.940915, mae: 13.899724, mean_q: 27.109488\n",
            " 5588/10000: episode: 164, duration: 0.422s, episode steps:  59, steps per second: 140, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 6.908687, mae: 14.058561, mean_q: 27.362919\n",
            " 5647/10000: episode: 165, duration: 0.451s, episode steps:  59, steps per second: 131, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 5.275276, mae: 14.128561, mean_q: 27.693722\n",
            " 5756/10000: episode: 166, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.646979, mae: 14.306990, mean_q: 28.071434\n",
            " 5859/10000: episode: 167, duration: 0.740s, episode steps: 103, steps per second: 139, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.613586, mae: 14.522296, mean_q: 28.490475\n",
            " 5941/10000: episode: 168, duration: 0.593s, episode steps:  82, steps per second: 138, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 7.506866, mae: 14.737071, mean_q: 28.676632\n",
            " 6020/10000: episode: 169, duration: 0.565s, episode steps:  79, steps per second: 140, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 7.292219, mae: 14.841004, mean_q: 29.007532\n",
            " 6183/10000: episode: 170, duration: 1.189s, episode steps: 163, steps per second: 137, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.731957, mae: 14.989171, mean_q: 29.344837\n",
            " 6281/10000: episode: 171, duration: 0.700s, episode steps:  98, steps per second: 140, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.661515, mae: 15.296986, mean_q: 30.022943\n",
            " 6349/10000: episode: 172, duration: 0.485s, episode steps:  68, steps per second: 140, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 6.964725, mae: 15.414256, mean_q: 30.197380\n",
            " 6540/10000: episode: 173, duration: 1.399s, episode steps: 191, steps per second: 136, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 7.030703, mae: 15.634404, mean_q: 30.700876\n",
            " 6604/10000: episode: 174, duration: 0.464s, episode steps:  64, steps per second: 138, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 7.589293, mae: 15.817930, mean_q: 31.058479\n",
            " 6700/10000: episode: 175, duration: 0.718s, episode steps:  96, steps per second: 134, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 6.339380, mae: 16.018671, mean_q: 31.575104\n",
            " 6820/10000: episode: 176, duration: 0.932s, episode steps: 120, steps per second: 129, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 7.704497, mae: 16.230947, mean_q: 31.899067\n",
            " 6891/10000: episode: 177, duration: 0.548s, episode steps:  71, steps per second: 129, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 8.071437, mae: 16.344110, mean_q: 32.034073\n",
            " 6999/10000: episode: 178, duration: 0.800s, episode steps: 108, steps per second: 135, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 7.676645, mae: 16.427200, mean_q: 32.306187\n",
            " 7129/10000: episode: 179, duration: 0.958s, episode steps: 130, steps per second: 136, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.971151, mae: 16.695766, mean_q: 32.883713\n",
            " 7277/10000: episode: 180, duration: 1.102s, episode steps: 148, steps per second: 134, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 7.350532, mae: 16.878994, mean_q: 33.321056\n",
            " 7355/10000: episode: 181, duration: 0.594s, episode steps:  78, steps per second: 131, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.646672, mae: 17.068306, mean_q: 33.610672\n",
            " 7555/10000: episode: 182, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.156238, mae: 17.356627, mean_q: 34.402740\n",
            " 7755/10000: episode: 183, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 7.106580, mae: 17.797552, mean_q: 35.336838\n",
            " 7955/10000: episode: 184, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.307076, mae: 18.257504, mean_q: 36.293518\n",
            " 8130/10000: episode: 185, duration: 1.348s, episode steps: 175, steps per second: 130, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 8.389958, mae: 18.615858, mean_q: 37.022495\n",
            " 8330/10000: episode: 186, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.047607, mae: 19.055674, mean_q: 37.879208\n",
            " 8530/10000: episode: 187, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.851126, mae: 19.428822, mean_q: 38.742470\n",
            " 8730/10000: episode: 188, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.674944, mae: 19.915730, mean_q: 39.851429\n",
            " 8930/10000: episode: 189, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.919180, mae: 20.445133, mean_q: 40.916630\n",
            " 9130/10000: episode: 190, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.762260, mae: 20.964361, mean_q: 41.943016\n",
            " 9328/10000: episode: 191, duration: 1.420s, episode steps: 198, steps per second: 139, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 9.615380, mae: 21.417593, mean_q: 42.830471\n",
            " 9528/10000: episode: 192, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.448620, mae: 21.794632, mean_q: 43.580097\n",
            " 9728/10000: episode: 193, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 9.135553, mae: 22.253935, mean_q: 44.640690\n",
            " 9928/10000: episode: 194, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 10.878740, mae: 22.707232, mean_q: 45.438633\n",
            "done, took 75.523 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efbb1cf7990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}