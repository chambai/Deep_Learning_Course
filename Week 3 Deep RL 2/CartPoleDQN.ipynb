{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeTVdfcYumMOh0VN62GJ3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\r\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\r\n",
        "!pip install keras-rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0"
      },
      "source": [
        "# load the gym module\r\n",
        "import gym\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# import the usual Keras modules for creating deep neural networks\r\n",
        "from keras import Sequential\r\n",
        "from keras.layers import Input, Flatten, Dense\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "ENV_NAME = 'CartPole-v0'\r\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D"
      },
      "source": [
        "import rl\r\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\r\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\r\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\r\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\r\n",
        "# memory buffer\r\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# define the policy (how we select the actions)\r\n",
        "policy = BoltzmannQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,???)))\r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(???, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='???'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=???,                  # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy instead of BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c"
      },
      "source": [
        "from rl.policy import BoltzmannGumbelQPolicy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\r\n",
        "# based on the paper (https://arxiv.org/pdf/1705.10257.pdf).\r\n",
        "policy = BoltzmannGumbelQPolicy()\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCvKlzqw5I1K"
      },
      "source": [
        "## Implement DQN with BoltzmannQPolicy and LinearAnnaeledPolicy, changing the tau parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bezkVlP45YGv"
      },
      "source": [
        "from rl.policy import BoltzmannQPolicy\r\n",
        "\r\n",
        "# setup experience replay buffer\r\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
        "\r\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\r\n",
        "policy =  LinearAnnealedPolicy(inner_policy=BoltzmannQPolicy(),   # policy used to select actions\r\n",
        "                               attr='tau',                        # attribute in the inner policy to vary             \r\n",
        "                               value_max=1,                       # maximum value of attribute that is varying\r\n",
        "                               value_min=.1,                      # minimum value of attribute that is varying\r\n",
        "                               value_test=.05,                    # test if the value selected is < 0.05\r\n",
        "                               nb_steps=10000)                    # the number of steps between value_max and value_min\r\n",
        "\r\n",
        "# Q-Network\r\n",
        "model = Sequential()\r\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "# add extra layers here\r\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# define the agent\r\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\r\n",
        "               nb_actions=env.action_space.n,   # number of actions\r\n",
        "               memory=memory,                   # experience replay memory\r\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\r\n",
        "               target_model_update=1e-2,        # how often the target network is updated\r\n",
        "               policy=policy)                   # the action selection policy\r\n",
        "\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\n",
        "\r\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\r\n",
        "\r\n",
        "# summarize the history for number  of episode steps\r\n",
        "plt.plot(history.history['nb_episode_steps'])\r\n",
        "plt.ylabel('nb_episode_steps')\r\n",
        "plt.xlabel('episodes')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjzJMsq9-vB"
      },
      "source": [
        "## Double DQN Network\r\n",
        "Implement a double DQN Network with BoltzmannQPolicy\r\n",
        "Add layers to the Q-Network and analyse the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdWVWj_-DDm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyw0K86m-LGO"
      },
      "source": [
        "## Dueling DQN Network\r\n",
        "Implement a dueling double DQN Network with BoltzmannQPolicy\r\n",
        "Add layers to the Q-Network and analyse the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNY4Tur2-SAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLyN6v7X-Zui"
      },
      "source": [
        "## Double Dueling DQN\r\n",
        "Implement a double dueling DQN Network with BoltzmannQPolicy\r\n",
        "Add layers to the Q-Network and analyse the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4CWiJ3e-ZPn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}