{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IrisLR_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmUzvq3fXQLHS/Xu2+CYVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%201%20DL%201/IrisLR_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWeV4Bkj4kLa"
      },
      "source": [
        "# Iris Learning Rate Exercise\r\n",
        "Exercise to adjust the learning rate of the optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJlfSy8vYmq6"
      },
      "source": [
        "Load the libraries and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edV8wRk_4ihz"
      },
      "source": [
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras import utils\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# get the data\r\n",
        "data = load_iris()\r\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\r\n",
        "\r\n",
        "# Normalise the data\r\n",
        "df_norm = normalize(df)\r\n",
        "\r\n",
        "# 2. split the data into train and test\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df_norm, data.target, random_state=0)\r\n",
        "\r\n",
        "# one-hot encode the data\r\n",
        "Y_train_cat = utils.to_categorical(Y_train)\r\n",
        "Y_test_cat = utils.to_categorical(Y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkPkH62YvWJ"
      },
      "source": [
        "Create, compile, fit the model and plot the accuracy and loss\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwPTlgr05Z5G"
      },
      "source": [
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "history = model.fit(\r\n",
        "    X_train,    # input training data\r\n",
        "    Y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(X_test, Y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73MCCqwBY7fn"
      },
      "source": [
        "## Adjust the learning rate of the optimiser\r\n",
        "Instead of specifying the optimiser as a string, you can specify the optimiser as an object.  Specifying the optimiser as a string means that it uses it's default parameters.  By specifying it as an object, you have more flexibility and can change it's parameters.  Implement the optimiser as an object so you can change the learning rate (hint: see [keras optimisers](https://keras.io/api/optimizers)).  As we are using Keras from within tensorflow, you will also need to import tensorflow and reference keras from that.  Then you will be able to access the `optimizers` module as in the Keras documentation.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "import tensorflow as tf\r\n",
        "tf.keras.optimizers...\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fir-8I0GaxDe"
      },
      "source": [
        "Set the learning rate to the following values.  You will need to re-create, re-compile and re-fit the model and re-plot the accuracy and loss to try out each learning rate.\r\n",
        "\r\n",
        "\r\n",
        "*   0.0001\r\n",
        "*   0.001\r\n",
        "*   0.01\r\n",
        "*   1\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxUJSCbomX_i"
      },
      "source": [
        "# adjusting the learning rate to 0.0001\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "# create an optimiser object and set the learning rate\r\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\r\n",
        "# Compile the model with the optimiser object\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "history = model.fit(\r\n",
        "    X_train,    # input training data\r\n",
        "    Y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(X_test, Y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuInIhqcovX_"
      },
      "source": [
        "Re-run the above code after editing the `learning_rate` parameter with the other values"
      ]
    }
  ]
}