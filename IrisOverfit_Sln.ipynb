{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IrisOverfit_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfin0ZL4eUflzhvrJZKIK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/IrisOverfit_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWeV4Bkj4kLa"
      },
      "source": [
        "# Overfitting reduction with Dropout and Regularizers for the Iris DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJlfSy8vYmq6"
      },
      "source": [
        "Load the libraries and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edV8wRk_4ihz"
      },
      "source": [
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras import utils\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# get the data\r\n",
        "data = load_iris()\r\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\r\n",
        "\r\n",
        "# Normalise the data\r\n",
        "df_norm = normalize(df)\r\n",
        "\r\n",
        "# 2. split the data into train and test\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_norm, data.target, random_state=0)\r\n",
        "\r\n",
        "# one-hot encode the data\r\n",
        "y_train_cat = utils.to_categorical(y_train)\r\n",
        "y_test_cat = utils.to_categorical(y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkPkH62YvWJ"
      },
      "source": [
        "Create, compile, fit the model and plot the accuracy and loss\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwPTlgr05Z5G"
      },
      "source": [
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='relu'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73MCCqwBY7fn"
      },
      "source": [
        "## Add a dropout layer to reduce the overfitting\r\n",
        "You will need to import the Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Nk0t4pr0eY"
      },
      "source": [
        "# import the Dropout layer from keras\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "\r\n",
        "# create the model with a dropout layer\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu'))\r\n",
        "# add the dropout layer here\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(units=10,  activation='relu'))\r\n",
        "# add the dropout layer here\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StqLbeNIy7fQ"
      },
      "source": [
        "## It is common just to add dropout after the last hidden layer\r\n",
        "Create a neural network with 3 hidden layers, teh first with 20 units, and the second and third layers with 100 units.\r\n",
        "\r\n",
        "Add a dropout layer after the second layer with a dropout that decreases tha hidden units by half."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M5y_Gu_zjqW"
      },
      "source": [
        "# import the Dropout layer from keras\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "\r\n",
        "# create the model with a dropout layer\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=50, input_dim=4, activation='relu'))\r\n",
        "model.add(Dense(units=100,  activation='relu'))\r\n",
        "model.add(Dense(units=100,  activation='relu'))\r\n",
        "# dropout layer to reduce the units by 0.5 in the layer before\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fir-8I0GaxDe"
      },
      "source": [
        "## Add L1 regularizer\r\n",
        "You will need to add a kernel_regularizer parameter to the hidden layers of the neural network (https://keras.io/api/layers/regularizers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppC22R-Kui4C"
      },
      "source": [
        "# import the Dropout layer from keras\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "\r\n",
        "# create the model with # create the model with the kernel_regularizer parameter set to l1 in each hidden layer\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu', kernel_regularizer='l1'))\r\n",
        "model.add(Dense(units=10,  activation='relu', kernel_regularizer='l1'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAqv98F3v0TE"
      },
      "source": [
        "## Add L2 Regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwtwSu2pv6nD"
      },
      "source": [
        "# import the Dropout layer from keras\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "\r\n",
        "# create the model with the kernel_regularizer parameter set to l2 in each hidden layer\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu', kernel_regularizer='l2'))\r\n",
        "model.add(Dense(units=10,  activation='relu', kernel_regularizer='l2'))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7HlVLKNv8E-"
      },
      "source": [
        "## Add L1 and L2 regularizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDNVA2f3wtMp"
      },
      "source": [
        "# import the Dropout layer from keras\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "\r\n",
        "# create a regularizer object\r\n",
        "from tensorflow.keras import regularizers # import the regularizers module\r\n",
        "reg = regularizers.l1_l2(l1=0.01, l2=0.01)\r\n",
        "\r\n",
        "# create the model with the regularizer object in each hidden layer\r\n",
        "model = Sequential() \r\n",
        "model.add(Dense(units=10, input_dim=4, activation='relu', kernel_regularizer=reg))\r\n",
        "model.add(Dense(units=10,  activation='relu', kernel_regularizer=reg))\r\n",
        "model.add(Dense(units=3, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 1. Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model (replace the question marks)\r\n",
        "history = model.fit(\r\n",
        "    x_train,    # input training data\r\n",
        "    y_train_cat,    # output training data\r\n",
        "    batch_size=10,   # mini-batch gradient descent size\r\n",
        "    epochs=125,       # number of iterations over the entire training data\r\n",
        "    verbose=1,        # what type of information is printed during training\r\n",
        "    validation_data=(x_test, y_test_cat))  # input test data, output test data\r\n",
        "\r\n",
        "# summarize the history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# summarize the history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}