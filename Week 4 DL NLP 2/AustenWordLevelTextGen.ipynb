{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AustenWordLevelTextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVUFcXh6YNbLelKz9sJc9w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%202/AustenWordLevelTextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "# Text Generation in the style of Jane Austen\n",
        "Adapted from: https://stackabuse.com/python-for-nlp-deep-learning-text-generation-with-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96aupv4YwguN"
      },
      "source": [
        "Use the Natural Language Tool Kit (NLTK) library to download the dataset.  We are using the **Gutenberg Dataset** which contains 3036 english books written by 142 authors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EMj3jAXikwM",
        "outputId": "9fb9049f-8245-41e2-81e7-b7f36fa0b488"
      },
      "source": [
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lw30yi_xmU2"
      },
      "source": [
        "The file austen-sense.txt contains raw text for the novel Sense and Sensibility by Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKhgnXyinuh"
      },
      "source": [
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64TVaIxxirEy",
        "outputId": "16c4a7b0-29b8-4afc-f2a9-f59f4c482f42"
      },
      "source": [
        "# print the first 500 characters of the text so we can look at it\n",
        "print(book_text[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Sense and Sensibility by Jane Austen 1811]\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "\n",
            "The family of Dashwood had long been settled in Sussex.\n",
            "Their estate was large, and their residence was at Norland Park,\n",
            "in the centre of their property, where, for many generations,\n",
            "they had lived in so respectable a manner as to engage\n",
            "the general good opinion of their surrounding acquaintance.\n",
            "The late owner of this estate was a single man, who lived\n",
            "to a very advanced age, and who for many years of his life,\n",
            "had a constant companion an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ew_mLKyHSO"
      },
      "source": [
        "The text may contain many special characters and numbers.  This text isn't actually too bad but it still needs cleaning to remove special characters (such as whitespaces), numbers and punctuation.\n",
        "## Data preprocessing\n",
        "To remove the puctuations and special characters, we will define a function called `preprocess_text()`  This uses regular expressions to search for and replace words.  The python library \"`re`\" does this. There are many tutorials for this on the web i.e. [w3schools regex tutorial](https://www.w3schools.com/python/python_regex.asp).  \n",
        "\n",
        "The `preprocess_text()` function accepts a text string as a parameter and returns a cleaned text string in lower case.\n",
        "\n",
        "Stop word removal needs to be done with an NLP library like NLTK i.e. https://stackabuse.com/removing-stop-words-from-strings-in-python/  as the Keras tokenizer is limited in its capabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLwmLpK1iuIk"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCuGZo4o0c38"
      },
      "source": [
        "Call the `preprocess_text()` function to clean the data and display the first 500 characters of the cleaned text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je0_iA8jiwU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "20c32481-c943-44da-fa63-3db0129dd963"
      },
      "source": [
        "book_text = preprocess_text(book_text)\n",
        "book_text[:500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' sense and sensibility by jane austen chapter the family of dashwood had long been settled in sussex their estate was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister b'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhmYGD3kQd68",
        "outputId": "38692b36-cd07-4bc0-9bd6-d28600484e74"
      },
      "source": [
        "# limit the text to 5000 characters, just so this example runs faster, do not do this in a real model!\n",
        "print(len(book_text))\n",
        "book_text = book_text[:5000]\n",
        "print(len(book_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "636106\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75IomB5y0326"
      },
      "source": [
        "## Convert words to numbers\n",
        "We are using a simple approach to convert words into single integers.  Before we do this we need to tokenize the text into individual words.  To do this we can use the `word_tokenize()` method from the `nltk.tokenize` module.\n",
        "\n",
        "The following code tokenizes the text in the dataset and prints out the total number of words in the dataset, as well as the total number of unique words in that dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqaVSLGTiyl2",
        "outputId": "7130fa69-6469-471f-fd0f-bb96fb9f7c0a"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# punkt is a sentence tokenizer that nltk requires. \n",
        "# It divides a text into a list of sentences, by using an unsupervised algorithm \n",
        "# to build a model for abbreviation words, collocations, and words that start sentences\n",
        "nltk.download('punkt')\n",
        "\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "print('Total Words: %d' % n_words)\n",
        "print('Unique Words: %d' % unique_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Total Words: 3662\n",
            "Unique Words: 900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veF60V822i2d"
      },
      "source": [
        "To convert tokenized words to numbers, we use the `Tokenizer` class from the `keras.preprocessing.text` module.  Then use the fit_on_texts method and pass in the list of words.  A dictionary will be created where the keys will represent words, and integers will represent the corresponding values of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF_bjp9di1L_"
      },
      "source": [
        "# convert words to numbers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfIQ6sUmi27A"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6g9_W3di432",
        "outputId": "ca1e7064-b30e-440f-efab-f7715fb1f543"
      },
      "source": [
        "# just for exploration, let's print the 1000th word in the dictionary and it's index\n",
        "print(book_text_words[1000])\n",
        "print(word_2_index[book_text_words[500]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mrs\n",
            "125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRoIdE1RK2pY"
      },
      "source": [
        "## Creating the input sequences\n",
        "The following code splits the text into smaller sequences of text, each consisting of 100 words.\n",
        "\n",
        "The first iteration of the loop:\n",
        "> The first 100 words from the begining of the text are added to the input_sequence list\n",
        "> The 101st word is appened to the output_words list\n",
        "\n",
        "In the second iteration of the loop:\n",
        "> The first 100 words starting from the second word of the text are added to the input_sequence list\n",
        "> The 102nd word is appened to the output_words list\n",
        "\n",
        "And so on..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtQr_IEi7cF"
      },
      "source": [
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "\n",
        "# form the input sequence list and the output words list\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaldWv3Gi-rv",
        "outputId": "f1a7e8dc-a950-4e5c-a1cf-75ed43ab2f6a"
      },
      "source": [
        "# print the first sequence to see what it looks like - a list of 100 integers that represent the first observation of words\n",
        "print(len(input_sequence))      # print the number of input sequences\n",
        "print(input_sequence[0])        # print the first input sequence\n",
        "print(len(input_sequence[0]))   # print the length of the first input sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3562\n",
            "[177, 3, 236, 19, 389, 390, 178, 4, 116, 1, 25, 22, 237, 54, 391, 10, 392, 15, 87, 6, 88, 3, 15, 393, 6, 30, 64, 394, 10, 4, 395, 1, 15, 238, 239, 9, 102, 396, 23, 22, 240, 10, 32, 179, 397, 13, 2, 398, 4, 180, 139, 241, 1, 15, 399, 400, 4, 242, 401, 1, 69, 87, 6, 402, 140, 103, 240, 2, 45, 243, 181, 3, 103, 9, 102, 70, 1, 5, 117, 22, 244, 403, 3, 404, 10, 5, 141, 16, 8, 245, 36, 246, 118, 70, 247, 5, 49, 248, 89, 405]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJUhLJDijBUu"
      },
      "source": [
        "# reshape the input sequences to be 3-dimensional\n",
        "#X = np.reshape(input_sequence, (3562, 100, 1))    # number of input sequences, length of each sequence\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYdNZw_4jEEp",
        "outputId": "ee70d1b3-569c-4f71-e00a-e0f642943657"
      },
      "source": [
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (3562, 100, 1)\n",
            "y shape: (3562, 901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZXtiF0DV1rS"
      },
      "source": [
        "## Create, compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3fd1SEpjHEV",
        "outputId": "ded52fe2-35ae-4cb4-e150-59086a34f500"
      },
      "source": [
        "model = Sequential()\n",
        "# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(800, return_sequences=True))\n",
        "model.add(LSTM(800))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# the output word can be one of any of the unique words in the vocabulary\n",
        "# This means it is a multi-class calssification problem and we use the categorical crossentropy loss function\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 800)          2566400   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 800)          5123200   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 800)               5123200   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 901)               721701    \n",
            "=================================================================\n",
            "Total params: 13,534,501\n",
            "Trainable params: 13,534,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-qIiMvjJMX",
        "outputId": "67f96119-0325-4c0d-9d9c-fe40186fa066"
      },
      "source": [
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 19s 185ms/step - loss: 6.1375\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 10s 187ms/step - loss: 5.8178\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 10s 187ms/step - loss: 5.7844\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 11s 189ms/step - loss: 5.7751\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 11s 190ms/step - loss: 5.7677\n",
            "Epoch 6/20\n",
            "56/56 [==============================] - 11s 191ms/step - loss: 5.7625\n",
            "Epoch 7/20\n",
            "56/56 [==============================] - 11s 191ms/step - loss: 5.7594\n",
            "Epoch 8/20\n",
            "56/56 [==============================] - 11s 192ms/step - loss: 5.7589\n",
            "Epoch 9/20\n",
            "56/56 [==============================] - 11s 192ms/step - loss: 5.7529\n",
            "Epoch 10/20\n",
            "56/56 [==============================] - 11s 192ms/step - loss: 5.7514\n",
            "Epoch 11/20\n",
            "56/56 [==============================] - 11s 194ms/step - loss: 5.7529\n",
            "Epoch 12/20\n",
            "56/56 [==============================] - 11s 194ms/step - loss: 5.7506\n",
            "Epoch 13/20\n",
            "56/56 [==============================] - 11s 195ms/step - loss: 5.7483\n",
            "Epoch 14/20\n",
            "56/56 [==============================] - 11s 197ms/step - loss: 5.7477\n",
            "Epoch 15/20\n",
            "56/56 [==============================] - 11s 198ms/step - loss: 5.7473\n",
            "Epoch 16/20\n",
            "56/56 [==============================] - 11s 199ms/step - loss: 5.7459\n",
            "Epoch 17/20\n",
            "56/56 [==============================] - 11s 199ms/step - loss: 5.7458\n",
            "Epoch 18/20\n",
            "56/56 [==============================] - 11s 197ms/step - loss: 5.7460\n",
            "Epoch 19/20\n",
            "56/56 [==============================] - 11s 197ms/step - loss: 5.7415\n",
            "Epoch 20/20\n",
            "56/56 [==============================] - 11s 197ms/step - loss: 5.7418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7dc80be510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky-c-XWOWG8S"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGbsIGYbjLq5",
        "outputId": "665daf0c-c812-466d-f1e5-013177552382"
      },
      "source": [
        "# Making a prediction requires selecting an input sequence\n",
        "# randomly select a sequence of integers from the input sequences\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "# convert the integer sequence to its words\n",
        "# word_2_index contains a dictionary of the format word : index (word being the key and index being the value)\n",
        "# the next line of code reverses this to index: word (index now being the key and word is now the value)\n",
        "# this reversed dictionary can now be used by supplying an index to it, and the word will be returned\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # swaps keys with values\n",
        "# loop round using a list iteration to get the list of words that correspond to the integers in the randomly picked sequence\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it will be gone for ever if indeed it could be restored to our poor little boy why to be sure said her husband very gravely that would make great difference the time may come when harry will regret that so large sum was parted with if he should have numerous family for instance it would be very convenient addition to be sure it would perhaps then it would be better for all parties if the sum were diminished one half five hundred pounds would be prodigious increase to their fortunes oh beyond anything great what brother on earth would\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrVqz51jObB"
      },
      "source": [
        "# this code predicts the next 100 words that follow the randomly picked sequence above\n",
        "# we loop round, making 100 predictions\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "   \n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    \n",
        "    # for info, let's print out the first prediction\n",
        "    if i==0:\n",
        "      print('%s\\n%s'%(i,predicted_word_index))  # print the array that was output in the prediction\n",
        "      print(predicted_word_id)                  # print out the prediction index\n",
        "\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "    # random_seq is the new text to be presented to the network for the next word prediction\n",
        "    # i.e. if the first seed sentence is 'hello my name is peter'\n",
        "    # if the next predicted word is 'and'\n",
        "    # the next sequence to be presented to the model predict method will be 'my name is peter and'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOcmRFS4jQp3",
        "outputId": "85308669-b57d-4027-8233-3bd483c2e158"
      },
      "source": [
        "# print out the list of predicted words as a string so we can see what was predicted\n",
        "print(' '.join(word_sequence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqfzQ-lBdd3L"
      },
      "source": [
        "The model has produced repeated words, so this is not a very good model yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiMh37DpkzUH"
      },
      "source": [
        "## Metrics - The Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eJdZDSqgQGV",
        "outputId": "a338a355-1524-4fe3-db00-67ba52f057e4"
      },
      "source": [
        "# BLEU score\n",
        "# Make sure you are comparing like with like\n",
        "# input sequences contain the words in lists\n",
        "# join each sequence into a string so it can be compared with the final output which is a string\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of predicted words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed word sequence: it will be gone for ever if indeed it could be restored to our poor little boy why to be sure said her husband very gravely that would make great difference the time may come when harry will regret that so large sum was parted with if he should have numerous family for instance it would be very convenient addition to be sure it would perhaps then it would be better for all parties if the sum were diminished one half five hundred pounds would be prodigious increase to their fortunes oh beyond anything great what brother on earth would\n",
            "Predicted words:  of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n",
            "BLEU Score for predicted words: 0.06226716290146055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrNx1r4jwgH"
      },
      "source": [
        "The Bleu score is a value between 0 and 1.  **The closer the Bleu score is to 1 the better**.\n",
        "Repeat words mean the model has not learned very well.  If it is repeating 1 word then it has not learnt at all and can only predict one class (vocab word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAazRz6AjTIM"
      },
      "source": [
        "## **Exercises** \n",
        "1.   Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\n",
        "2.   Try adding dropout after the LSTM layers and Dense layers.\n",
        "3.   Normalisation does not always provide the best results.  Remove normalisation and see if this improves the results (this will probably mean the model hyper-parameters also need changing).\n",
        "4.   Add an Embedding Layer into the DNN to see if this improves the model.\n",
        "\n",
        "\n"
      ]
    }
  ]
}