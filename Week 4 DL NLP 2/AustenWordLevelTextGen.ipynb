{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AustenWordLevelTextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%202/AustenWordLevelTextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "# Text Generation in the style of Jane Austen\r\n",
        "Adapted from: https://stackabuse.com/python-for-nlp-deep-learning-text-generation-with-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6"
      },
      "source": [
        "# import python libraries\r\n",
        "import numpy as np\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\r\n",
        "from keras.utils import to_categorical\r\n",
        "from random import randint\r\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96aupv4YwguN"
      },
      "source": [
        "Use the Natural Language Tool Kit (NLTK) library to download the dataset.  We are using the **Gutenberg Dataset** which contains 3036 english books written by 142 authors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EMj3jAXikwM",
        "outputId": "1a19f7dc-7b9e-4437-b351-c064f09a3aae"
      },
      "source": [
        "import nltk   # natural language tool kit library\r\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\r\n",
        "\r\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\r\n",
        "print(gut.fileids())    # prints the name of the files in the dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lw30yi_xmU2"
      },
      "source": [
        "The file austen-sense.txt contains raw text for the novel Sense and Sensibility by Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKhgnXyinuh"
      },
      "source": [
        "# get the book text\r\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64TVaIxxirEy",
        "outputId": "2dd39b94-013f-4834-e8fd-a25f319b2337"
      },
      "source": [
        "# print the first 500 characters of the text so we can look at it\r\n",
        "print(book_text[:500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Sense and Sensibility by Jane Austen 1811]\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "\n",
            "The family of Dashwood had long been settled in Sussex.\n",
            "Their estate was large, and their residence was at Norland Park,\n",
            "in the centre of their property, where, for many generations,\n",
            "they had lived in so respectable a manner as to engage\n",
            "the general good opinion of their surrounding acquaintance.\n",
            "The late owner of this estate was a single man, who lived\n",
            "to a very advanced age, and who for many years of his life,\n",
            "had a constant companion an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ew_mLKyHSO"
      },
      "source": [
        "The text may contain many special characters and numbers.  This text isn't actually too bad but it still needs cleaning to remove special characters (such as whitespaces), numbers and punctuation.\r\n",
        "## Data preprocessing\r\n",
        "To remove the puctuations and special characters, we will define a function called `preprocess_text()`  This uses regular expressions to search for and replace words.  The python library \"`re`\" does this. There are many tutorials for this on the web i.e. [w3schools regex tutorial](https://www.w3schools.com/python/python_regex.asp).  \r\n",
        "\r\n",
        "The `preprocess_text()` function accepts a text string as a parameter and returns a cleaned text string in lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLwmLpK1iuIk"
      },
      "source": [
        "def preprocess_text(sen):\r\n",
        "    # Remove punctuations and numbers\r\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\r\n",
        "\r\n",
        "    # Single character removal\r\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\r\n",
        "\r\n",
        "    # Removing multiple spaces\r\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\r\n",
        "\r\n",
        "    return sentence.lower()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCuGZo4o0c38"
      },
      "source": [
        "Call the `preprocess_text()` function to clean the data and display the first 500 characters of the cleaned text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Je0_iA8jiwU1",
        "outputId": "80ea8dae-c818-4f8a-dd4b-7a087636e19a"
      },
      "source": [
        "book_text = preprocess_text(book_text)\r\n",
        "book_text[:500]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' sense and sensibility by jane austen chapter the family of dashwood had long been settled in sussex their estate was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister b'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhmYGD3kQd68",
        "outputId": "96e504e4-d8a4-4a95-bed6-0590a3b5a901"
      },
      "source": [
        "# limit the text to 20,000 characters\r\n",
        "print(len(book_text))\r\n",
        "book_text = book_text[:20000]\r\n",
        "print(len(book_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "636106\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75IomB5y0326"
      },
      "source": [
        "## Convert words to numbers\r\n",
        "We are using a simple approach to convert words into single integers.  Before we do this we need to tokenize the text into individual words.  To do this we can use the `word_tokenize()` method from the `nltk.tokenize` module.\r\n",
        "\r\n",
        "The following code tokenizes the text in the dataset and prints out the total number of words in the dataset, as well as the total number of unique words in that dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqaVSLGTiyl2",
        "outputId": "7191cf7d-9f04-4289-b696-9fc35f8c338f"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "# punkt is a sentence tokenizer that nltk requires. \r\n",
        "# It divides a text into a list of sentences, by using an unsupervised algorithm \r\n",
        "# to build a model for abbreviation words, collocations, and words that start sentences\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "book_text_words = (word_tokenize(book_text))\r\n",
        "n_words = len(book_text_words)\r\n",
        "unique_words = len(set(book_text_words))\r\n",
        "\r\n",
        "print('Total Words: %d' % n_words)\r\n",
        "print('Unique Words: %d' % unique_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Total Words: 3662\n",
            "Unique Words: 900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veF60V822i2d"
      },
      "source": [
        "To convert tokenized words to numbers, we use the `Tokenizer` class from the `keras.preprocessing.text` module.  Then use the fit_on_texts method and pass in the list of words.  A dictionary will be created where the keys will represent words, and integers will represent the corresponding values of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF_bjp9di1L_"
      },
      "source": [
        "# convert words to numbers\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "tokenizer = Tokenizer(num_words=900)\r\n",
        "tokenizer.fit_on_texts(book_text_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfIQ6sUmi27A"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\r\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6g9_W3di432",
        "outputId": "cf69257e-3654-45b0-f8ba-f55ac3751267"
      },
      "source": [
        "# just for exploration, let's print the 500th word in the dictionary and it's index\r\n",
        "print(book_text_words[500])\r\n",
        "print(word_2_index[book_text_words[500]])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "far\n",
            "125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWf2Ufuj42Kb"
      },
      "source": [
        "Here, the word 'far' is assigned the integer 125"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRoIdE1RK2pY"
      },
      "source": [
        "## Creating the input sequences\r\n",
        "The following code splits the text into smaller sequences of text, each consisting of 100 words.\r\n",
        "\r\n",
        "The first iteration of the loop:\r\n",
        "> The first 100 words from the begining of the text are added to the input_sequence list\r\n",
        "> The 101st word is appened to the output_words list\r\n",
        "\r\n",
        "In the second iteration of the loop:\r\n",
        "> The first 100 words starting from the second word of the text are added to the input_sequence list\r\n",
        "> The 102nd word is appened to the output_words list\r\n",
        "\r\n",
        "And so on..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtQr_IEi7cF"
      },
      "source": [
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\r\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\r\n",
        "output_words = []     # empty list to hold the output words\r\n",
        "input_seq_length = 100  # length of the input sequence\r\n",
        "\r\n",
        "# form the input sequence list and the output words list\r\n",
        "for i in range(0, n_words - input_seq_length , 1):\r\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\r\n",
        "    input_sequence_words.append(in_seq)\r\n",
        "    out_seq = book_text_words[i + input_seq_length]\r\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\r\n",
        "    output_words.append(word_2_index[out_seq])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaldWv3Gi-rv",
        "outputId": "10eafcaf-3231-4a74-c4d4-ec959f00f100"
      },
      "source": [
        "# print the first sequence to see what it looks like - a list of 100 integers that represent the first observation of words\r\n",
        "print(len(input_sequence))      # print the number of input sequences\r\n",
        "print(input_sequence[0])        # print the first input sequence\r\n",
        "print(len(input_sequence[0]))   # print the length of the first input sequence"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3562\n",
            "[177, 3, 236, 19, 389, 390, 178, 4, 116, 1, 25, 22, 237, 54, 391, 10, 392, 15, 87, 6, 88, 3, 15, 393, 6, 30, 64, 394, 10, 4, 395, 1, 15, 238, 239, 9, 102, 396, 23, 22, 240, 10, 32, 179, 397, 13, 2, 398, 4, 180, 139, 241, 1, 15, 399, 400, 4, 242, 401, 1, 69, 87, 6, 402, 140, 103, 240, 2, 45, 243, 181, 3, 103, 9, 102, 70, 1, 5, 117, 22, 244, 403, 3, 404, 10, 5, 141, 16, 8, 245, 36, 246, 118, 70, 247, 5, 49, 248, 89, 405]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJUhLJDijBUu"
      },
      "source": [
        "# reshape the input sequences to be 3-dimensional\r\n",
        "# X = np.reshape(input_sequence, (3562, 100, 1))    # number of input sequences, length of each sequence\r\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\r\n",
        "\r\n",
        "# Normalize the data by dividing by the max number of unique words (the vocab size)\r\n",
        "X = X / float(vocab_size)\r\n",
        "\r\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\r\n",
        "y = to_categorical(output_words)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYdNZw_4jEEp",
        "outputId": "45106eda-2531-4ac7-9963-bc73322d2231"
      },
      "source": [
        "print(\"X shape:\", X.shape)\r\n",
        "print(\"y shape:\", y.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (3562, 100, 1)\n",
            "y shape: (3562, 901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZXtiF0DV1rS"
      },
      "source": [
        "## Create, compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3fd1SEpjHEV",
        "outputId": "49343f0e-64fd-4158-de2a-c0c2cbb05d87"
      },
      "source": [
        "model = Sequential()\r\n",
        "# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\r\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\r\n",
        "model.add(LSTM(800, return_sequences=True))\r\n",
        "model.add(LSTM(800))\r\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# the output word can be one of any of the unique words in the vocabulary\r\n",
        "# This means it is a multi-class calssification problem and we use the categorical crossentropy loss function\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 800)          2566400   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 800)          5123200   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 800)               5123200   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 901)               721701    \n",
            "=================================================================\n",
            "Total params: 13,534,501\n",
            "Trainable params: 13,534,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-qIiMvjJMX",
        "outputId": "dbe3c73d-f994-4d3b-98b1-13342f142883"
      },
      "source": [
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 19s 342ms/step - loss: 5.7830\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 19s 343ms/step - loss: 5.7722\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 19s 344ms/step - loss: 5.7643\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 19s 344ms/step - loss: 5.7606\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 19s 344ms/step - loss: 5.7606\n",
            "Epoch 6/20\n",
            " 5/56 [=>............................] - ETA: 17s - loss: 5.7872"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky-c-XWOWG8S"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGbsIGYbjLq5",
        "outputId": "b0f260d2-41fd-47f0-e9e6-d4ed5b063117"
      },
      "source": [
        "# randomly select a sequence of integers from the input sequences\r\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)\r\n",
        "random_seq = input_sequence[random_seq_index]\r\n",
        "\r\n",
        "# convert the integer sequence to its words\r\n",
        "# word_2_index contains a dictionary of the format word : index (word being the key and index being the value)\r\n",
        "# the next line of code reverses this to index: word (index now being the key and word is now the value)\r\n",
        "# this reversed dictionary can now be used by supplying an index to it, and the word will be returned\r\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # swaps keys with values\r\n",
        "# loop round using a list iteration to get the list of words that correspond to the integers in the randomly picked sequence\r\n",
        "word_sequence = [index_2_word[value] for value in random_seq]\r\n",
        "\r\n",
        "# join the words in the list and print the sequence of words\r\n",
        "print(' '.join(word_sequence))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "think have not done enough for them even themselves they can hardly expect more there is no knowing what they may expect said the lady but we are not to think of their expectations the question is what you can afford to do certainly and think may afford to give them five hundred pounds piece as it is without any addition of mine they will each have about three thousand pounds on their mother death very comfortable fortune for any young woman to be sure it is and indeed it strikes me that they can want no addition at all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrVqz51jObB"
      },
      "source": [
        "# this code predicts the next 100 words that follow the randomly picked sequence above\r\n",
        "# we loop round, making 100 predictions\r\n",
        "word_sequence = []\r\n",
        "for i in range(100):\r\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\r\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise\r\n",
        "\r\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict\r\n",
        "\r\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so )\r\n",
        "    seq_in = [index_2_word[index] for index in random_seq]          # loop round integers in the random sequence and pick out the words for the random sequence - don't think we need this\r\n",
        "\r\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index\r\n",
        "\r\n",
        "    random_seq.append(predicted_word_id)                            # append so we get a list of predicted words\r\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # do not use the first element of random_seq"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOcmRFS4jQp3",
        "outputId": "c4fcd466-cd39-4db3-bb38-34baf2870318"
      },
      "source": [
        "# loop round the list of predicted words and print them out for our final prediction of the next 100 words\r\n",
        "final_output = \"\"\r\n",
        "for word in word_sequence:\r\n",
        "    final_output = final_output + \" \" + word\r\n",
        "\r\n",
        "print(final_output)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqfzQ-lBdd3L"
      },
      "source": [
        "The model has repeated words at the end, so this is not a very good model yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiMh37DpkzUH"
      },
      "source": [
        "## Metrics - The Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eJdZDSqgQGV",
        "outputId": "17addcf2-9941-461f-a4ed-d3cea93ee4ac"
      },
      "source": [
        "# Bleu score\r\n",
        "print(input_sequence_words[:2])       # print out the first 2 elements of the input sequence words list\r\n",
        "print(word_sequence)                  # print out the words of our randomly picked sequence\r\n",
        "\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "reference = input_sequence_words\r\n",
        "candidate = word_sequence\r\n",
        "\r\n",
        "score = sentence_bleu(reference, candidate)\r\n",
        "print(score)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['sense', 'and', 'sensibility', 'by', 'jane', 'austen', 'chapter', 'the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex', 'their', 'estate', 'was', 'large', 'and', 'their', 'residence', 'was', 'at', 'norland', 'park', 'in', 'the', 'centre', 'of', 'their', 'property', 'where', 'for', 'many', 'generations', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', 'the', 'late', 'owner', 'of', 'this', 'estate', 'was', 'single', 'man', 'who', 'lived', 'to', 'very', 'advanced', 'age', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', 'had', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', 'but', 'her', 'death', 'which', 'happened', 'ten', 'years', 'before', 'his', 'own', 'produced', 'great', 'alteration'], ['and', 'sensibility', 'by', 'jane', 'austen', 'chapter', 'the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex', 'their', 'estate', 'was', 'large', 'and', 'their', 'residence', 'was', 'at', 'norland', 'park', 'in', 'the', 'centre', 'of', 'their', 'property', 'where', 'for', 'many', 'generations', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', 'the', 'late', 'owner', 'of', 'this', 'estate', 'was', 'single', 'man', 'who', 'lived', 'to', 'very', 'advanced', 'age', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', 'had', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', 'but', 'her', 'death', 'which', 'happened', 'ten', 'years', 'before', 'his', 'own', 'produced', 'great', 'alteration', 'in']]\n",
            "['with', 'only', 'common', 'feelings', 'must', 'have', 'been', 'highly', 'unpleasing', 'but', 'in', 'her', 'mind', 'there', 'was', 'sense', 'of', 'honor', 'so', 'keen', 'generosity', 'so', 'romantic', 'that', 'any', 'offence', 'of', 'the', 'kind', 'by', 'whomsoever', 'given', 'or', 'received', 'was', 'to', 'her', 'source', 'of', 'immoveable', 'disgust', 'mrs', 'john', 'dashwood', 'had', 'never', 'been', 'favourite', 'with', 'any', 'of', 'her', 'husband', 'family', 'but', 'she', 'had', 'had', 'no', 'opportunity', 'till', 'the', 'present', 'of', 'shewing', 'them', 'with', 'how', 'little', 'attention', 'to', 'the', 'comfort', 'of', 'other', 'people', 'she', 'could', 'act', 'when', 'occasion', 'required', 'it', 'so', 'acutely', 'did', 'mrs', 'dashwood', 'feel', 'this', 'ungracious', 'behaviour', 'and', 'so', 'earnestly', 'did', 'she', 'despise', 'her', 'daughter', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
            "0.3355930100258299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrNx1r4jwgH"
      },
      "source": [
        "The Bleu score will vary as the input sentence supplied to the predict method in the model is random. In this example, the BLEU scores will be around 0.3 to 0.5, not great!  **The closer to 1 the BLEU score the better**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s8r325pENMs"
      },
      "source": [
        "Just as an experiment to see a change in the BLEU score, the following cell removes a repeated word.  Update the word `'affliction'` with a repeated word in your generated text.  This will remove your repeated word from the generated text and improve the BLEU score.\r\n",
        "\r\n",
        "##**This is just a test to see the improvement in the BLEU score, you would not normally do this!  You would normally make improvements to your model to improve the BLEU score.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF8rLTQljV5j",
        "outputId": "9f17334a-6b68-4582-a3c3-c37f3e76c8df"
      },
      "source": [
        "# Bleu score\r\n",
        "print(input_sequence_words[:2])       # print out the first 2 elements of the input sequence words list\r\n",
        "print(word_sequence)                  # print out the words of our randomly picked sequence\r\n",
        "\r\n",
        "# remove the words 'of' as a littel exercise on the Bleu score   \r\n",
        "word_sequence_no_of = [word for word in word_sequence if word != 'affliction']  # loop round the predicted words and remove the word 'of'\r\n",
        "print(word_sequence_no_of)                                              # print the modified output\r\n",
        "\r\n",
        "# set the reference words and the candidate words that are to be compared to the reference\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "reference = input_sequence_words\r\n",
        "candidate = word_sequence_no_of\r\n",
        "\r\n",
        "# calculate the Bleu score\r\n",
        "score = sentence_bleu(reference, candidate)\r\n",
        "print(score)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['sense', 'and', 'sensibility', 'by', 'jane', 'austen', 'chapter', 'the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex', 'their', 'estate', 'was', 'large', 'and', 'their', 'residence', 'was', 'at', 'norland', 'park', 'in', 'the', 'centre', 'of', 'their', 'property', 'where', 'for', 'many', 'generations', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', 'the', 'late', 'owner', 'of', 'this', 'estate', 'was', 'single', 'man', 'who', 'lived', 'to', 'very', 'advanced', 'age', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', 'had', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', 'but', 'her', 'death', 'which', 'happened', 'ten', 'years', 'before', 'his', 'own', 'produced', 'great', 'alteration'], ['and', 'sensibility', 'by', 'jane', 'austen', 'chapter', 'the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex', 'their', 'estate', 'was', 'large', 'and', 'their', 'residence', 'was', 'at', 'norland', 'park', 'in', 'the', 'centre', 'of', 'their', 'property', 'where', 'for', 'many', 'generations', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', 'the', 'late', 'owner', 'of', 'this', 'estate', 'was', 'single', 'man', 'who', 'lived', 'to', 'very', 'advanced', 'age', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', 'had', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', 'but', 'her', 'death', 'which', 'happened', 'ten', 'years', 'before', 'his', 'own', 'produced', 'great', 'alteration', 'in']]\n",
            "['with', 'only', 'common', 'feelings', 'must', 'have', 'been', 'highly', 'unpleasing', 'but', 'in', 'her', 'mind', 'there', 'was', 'sense', 'of', 'honor', 'so', 'keen', 'generosity', 'so', 'romantic', 'that', 'any', 'offence', 'of', 'the', 'kind', 'by', 'whomsoever', 'given', 'or', 'received', 'was', 'to', 'her', 'source', 'of', 'immoveable', 'disgust', 'mrs', 'john', 'dashwood', 'had', 'never', 'been', 'favourite', 'with', 'any', 'of', 'her', 'husband', 'family', 'but', 'she', 'had', 'had', 'no', 'opportunity', 'till', 'the', 'present', 'of', 'shewing', 'them', 'with', 'how', 'little', 'attention', 'to', 'the', 'comfort', 'of', 'other', 'people', 'she', 'could', 'act', 'when', 'occasion', 'required', 'it', 'so', 'acutely', 'did', 'mrs', 'dashwood', 'feel', 'this', 'ungracious', 'behaviour', 'and', 'so', 'earnestly', 'did', 'she', 'despise', 'her', 'daughter', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'affliction', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
            "['with', 'only', 'common', 'feelings', 'must', 'have', 'been', 'highly', 'unpleasing', 'but', 'in', 'her', 'mind', 'there', 'was', 'sense', 'of', 'honor', 'so', 'keen', 'generosity', 'so', 'romantic', 'that', 'any', 'offence', 'of', 'the', 'kind', 'by', 'whomsoever', 'given', 'or', 'received', 'was', 'to', 'her', 'source', 'of', 'immoveable', 'disgust', 'mrs', 'john', 'dashwood', 'had', 'never', 'been', 'favourite', 'with', 'any', 'of', 'her', 'husband', 'family', 'but', 'she', 'had', 'had', 'no', 'opportunity', 'till', 'the', 'present', 'of', 'shewing', 'them', 'with', 'how', 'little', 'attention', 'to', 'the', 'comfort', 'of', 'other', 'people', 'she', 'could', 'act', 'when', 'occasion', 'required', 'it', 'so', 'acutely', 'did', 'mrs', 'dashwood', 'feel', 'this', 'ungracious', 'behaviour', 'and', 'so', 'earnestly', 'did', 'she', 'despise', 'her', 'daughter', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
            "0.5034786920803475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RmiPcJgkj8n"
      },
      "source": [
        "The Bleu score is higher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAazRz6AjTIM"
      },
      "source": [
        "## **Exercises**\r\n",
        "1. We limited the amount of data in this exercise.  Increase the amount of data and see if it improves the score.   \r\n",
        "2. Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\r\n",
        "3. Try adding dropout after the LSTM layers and Dense layers.\r\n",
        "\r\n"
      ]
    }
  ]
}