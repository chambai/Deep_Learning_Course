{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EncoderDecoderLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOH1Q8VGeC3RYvV5ijUu31V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%202/EncoderDecoderLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr9UP7SWeCOr"
      },
      "source": [
        "#Outline Code for an Encoder-Decoder LSTM Model\r\n",
        "\r\n",
        "This code will not run! Its' skeleton code to explain the architecture of an Encoder-Decoder LSTM in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5OCPNQP7AKb"
      },
      "source": [
        "\r\n",
        "model = Sequential()\r\n",
        "# encoder\r\n",
        "# learns the relationship between the steps in the input sequence\r\n",
        "# and develops an internal representation of these relationships\r\n",
        "# one or more LSTM layers can be used\r\n",
        "# the output is a fixed size vector that represents the ineternal \r\n",
        "# representation  of the input sequence\r\n",
        "# The number of memory cells in this layer is the size of the vector\r\n",
        "model.add(LSTM(..., input_shape=(...)))  # units = size of the output vector\r\n",
        "\r\n",
        "# decoder\r\n",
        "# transforms the vector that is output from the encoder int othe correct\r\n",
        "# output sequence\r\n",
        "#one or more LSTM layers can be used\r\n",
        "# dense layer is used as the output for the network\r\n",
        "# The same weights can be used to output each time step in the output sequence\r\n",
        "# by wrapping the Dense layer in a TimeDistributed wrapper\r\n",
        "model.add(LSTM(..., return_sequences=True))\r\n",
        "model.add(TimeDistributed(Dense(...)))\r\n",
        "\r\n",
        "# We must connect the encoder to the decoder and they do not fit!\r\n",
        "# the encoder wil produce a 2D matrix with a lengthe of the number of memory cells in the layer\r\n",
        "# the decoder is an LSTM layer that expects 3D input of [samples, time steps, features]\r\n",
        "# solve this with a RepeatVector layer\r\n",
        "# this repleats the 2D input multiple times to create a 3D output\r\n",
        "model.add(RepeatVector(...))\r\n",
        "\r\n",
        "# putting this together, we have:\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(..., input_shape=(...)))   # encoder\r\n",
        "model.add(RepeatVector(...))              # repeat vector to join the encoder and decoder\r\n",
        "model.add(LSTM(..., return_sequences=True)) # decoder\r\n",
        "model.add(TimeDistributed(Dense(...)))      # use the same weights to output each time step in the output sequence\r\n",
        "\r\n",
        "# summary: RepeatVector is used as an adapter to fit the fixed-size 2D output \r\n",
        "# of the encoder to the differening length and 3D input expected by the decoder.\r\n",
        "# The TimeDistributed wrapper allows the same output layer to be resued for each\r\n",
        "# element in the output sequence.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}