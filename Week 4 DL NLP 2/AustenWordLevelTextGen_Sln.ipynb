{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AustenWordLevelTextGen_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORcbQrp/PFNmI1PsH43ZIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%202/AustenWordLevelTextGen_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "#Text Generation in the style of Jane Austen -  Exercise Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_0F75kzfNuV"
      },
      "source": [
        "## 1. Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\n",
        "\n",
        "Smplify the network to one LSTM layer and increase the epochs to 50 improves Bleu score to 0.95. More epochs will improve this further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=50, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKUH5aycmUr",
        "outputId": "0239886f-1f01-499d-a858-35e228b21312"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the\n",
            "Seed word sequence: not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the\n",
            "Predicted words: was his was one for his his son lady of proceeded the his from of the bequest of the the wished of urgency more illness could command of interest the reasonably hope to live many years and by the present law of its deal of the produce of the discharge of the ordinary duties in he married more make woman comfortable by considerable sum is and his ill disposed the any of all occasional visits for his father and mother john was tardy have been the was only and young was had strong disposed he man unless of the rather\n",
            "BLEU Score for predicted words: 0.9588026591174506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SDqaoFhB8C"
      },
      "source": [
        "## 2. Try adding dropout after the LSTM layers and Dense layers.\n",
        "\n",
        "In this particular model, Dropout prevents the model from learning at 50 epochs, but it does learn by 85 epochs. Dropout slows learning down in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdc-iC_hK1C",
        "outputId": "febf4b41-9146-4da3-f062-17362914ce54"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_11 (LSTM)               (None, 800)               2566400   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 344)               275544    \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 2s 44ms/step - loss: 5.5199\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.2430\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 5.1837\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.1171\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 5.1062\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 5.0900\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.1021\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0991\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 5.0768\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 5.0800\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0769\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0618\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0754\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0707\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0737\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0820\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0855\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0686\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0676\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0474\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0525\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0618\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0508\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0719\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0693\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0683\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0573\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0637\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0504\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0465\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0433\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0471\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0516\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0463\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0510\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0502\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0348\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0518\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0523\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0483\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0335\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0314\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0176\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 5.0269\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0169\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0032\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0055\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.9973\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.9933\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.9664\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.9551\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.9380\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.9114\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.9085\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.8784\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.8300\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.7953\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.7877\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.7140\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.6893\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.6334\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.6033\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.4479\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 4.4141\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.3023\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.1519\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.0301\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 3.8784\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 3.6299\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 3.5101\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 3.2711\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 3.0197\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.7163\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.4633\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.2436\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.0822\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.7651\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.5339\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.2720\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 1.0041\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.7702\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.6002\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.4504\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.3435\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.2593\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.2141\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.1632\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.1398\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.1165\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0909\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0804\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.0769\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0629\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0572\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.0521\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0433\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0398\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0347\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.0622\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 6.5035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78e6493ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvYRKc7hNgF",
        "outputId": "a6be0e43-69b7-4340-b171-7adc8dc5e9fe"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him\n",
            "Seed word sequence: death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him\n",
            "Predicted words: every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters\n",
            "BLEU Score for predicted words: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdtLURYijpa"
      },
      "source": [
        "## 3. Normalisation does not always provide the best results. Remove normalisation and see if this improves the results (this will probably mean the model hyper-parameters also need changing).\n",
        "\n",
        "The model has learnt by 20 epochs, so not having normalisation has sped up the model training.  The number of neurons in the model can also be reduced but this increases the number of epochs it takes to train.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FGGjud3imPQ",
        "outputId": "1411fea6-0ea0-4312-ef71-14a6a1827d28"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "# X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_14 (LSTM)               (None, 800)               2566400   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 344)               275544    \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "13/13 [==============================] - 2s 44ms/step - loss: 5.5429\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 4.9509\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 4.7872\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 4.6438\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 4.4829\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 4.3036\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 4.1100\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 3.8682\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 3.5879\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 3.2873\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 2.9628\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 2.6182\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 2.2386\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 1.8729\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 1.5398\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 1.2353\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.9874\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.7730\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 0.6210\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.4853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7874c59690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccz-a4g8imvh",
        "outputId": "6fb0347f-ae3b-4ea7-913c-d2d23ab1d4e0"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "promised to do every thing in his power to make them comfortable his father was rendered easy by such an assurance and mr john dashwood had then leisure to consider how much there might prudently be in his power to do for them he was not an ill disposed young man unless to be rather cold hearted and rather selfish is to be ill disposed but he was in general well respected for he conducted himself with propriety in the discharge of his ordinary duties had he married more amiable woman he might have been made still more respectable than\n",
            "Seed word sequence: promised to do every thing in his power to make them comfortable his father was rendered easy by such an assurance and mr john dashwood had then leisure to consider how much there might prudently be in his power to do for them he was not an ill disposed young man unless to be rather cold hearted and rather selfish is to be ill disposed but he was in general well respected for he conducted himself with propriety in the discharge of his ordinary duties had he married more amiable woman he might have been made still more respectable than\n",
            "Predicted words: he was he might even have been made amiable himself for he was very young when he married and very fond of his wife but mrs john dashwood was strong caricature of himself more narrow minded and selfish when he gave his promise to his father he meditated within himself to increase of fortunes of his sisters by the present of thousand pounds piece he then really thought himself equal to it the prospect of four thousand year in addition to his pr of his ordinary of his three girls he left them thousand pounds piece mr dashwood disappointment was\n",
            "BLEU Score for predicted words: 0.9802228716555276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwKDdQEkLv0"
      },
      "source": [
        "## 4. Add an Embedding Layer into the DNN to see if this improves the model.\n",
        "\n",
        "Adding an embedding layer and removing normalisation allows the model to acheive good results after 35 epochs.\n",
        "\n",
        "The dimension of the dense embedding layer was arbitarily selected to be 50.  This is a parameter that can be experimented with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znTg1sDZkNPy"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "#X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0y1mDYkNiG",
        "outputId": "a3b3c9c4-284d-49e0-91be-d732a80055a4"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the value of the bequest mr dashwood had wished for it more for the sake of his wife and daughters than for himself or his son but to his son and his son son child of four years old it was secured in such way as to leave to himself no power of providing for those who were most dear to him and who most needed provision by any charge on the estate or by any sale of its valuable woods the whole was tied up for the benefit of this child who in occasional visits with his father and\n",
            "Seed word sequence: the value of the bequest mr dashwood had wished for it more for the sake of his wife and daughters than for himself or his son but to his son and his son son child of four years old it was secured in such way as to leave to himself no power of providing for those who were most dear to him and who most needed provision by any charge on the estate or by any sale of its valuable woods the whole was tied up for the benefit of this child who in occasional visits with his father and\n",
            "Predicted words: mother at norland had so far gained on the affections of his uncle by such attractions as are by no means unusual in children of two or three years old an imperfect articulation an earnest desire of having his own way many cunning tricks and great deal of noise as to outweigh all the value of all the attention which for years he had received from his niece and her daughters he meant not to be unkind however and as mark of his affection for the three girls he left them thousand pounds piece mr dashwood disappointment was at first\n",
            "BLEU Score for predicted words: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}