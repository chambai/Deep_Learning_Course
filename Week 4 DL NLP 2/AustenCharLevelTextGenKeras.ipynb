{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AustenCharLevelTextGenKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMk63cismdHvewm0qAsd2Jv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%202/AustenCharLevelTextGenKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "# Character level text generation in the style of Jane Austen\r\n",
        "Adapted from: https://stackabuse.com/python-for-nlp-deep-learning-text-generation-with-keras/ and https://keras.io/examples/generative/lstm_character_level_text_generation/\r\n",
        "\r\n",
        "Uses LSTM model to generate text character by character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6"
      },
      "source": [
        "# import python libraries\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\r\n",
        "from keras.utils import to_categorical\r\n",
        "from random import randint\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96aupv4YwguN"
      },
      "source": [
        "Use the Natural Language Tool Kit (NLTK) library to download the dataset.  We are using the **Gutenberg Dataset** which contains 3036 english books written by 142 authors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EMj3jAXikwM"
      },
      "source": [
        "import nltk   # natural language tool kit library\r\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\r\n",
        "\r\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\r\n",
        "print(gut.fileids())    # prints the name of the files in the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lw30yi_xmU2"
      },
      "source": [
        "The file austen-sense.txt contains raw text for the novel Sense and Sensibility by Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKhgnXyinuh"
      },
      "source": [
        "# get the book text\r\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgQoGYT1Y5tm"
      },
      "source": [
        "# KERAS - DO NOT RUN\r\n",
        "text = book_text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\r\n",
        "# text is a list of the characters in the text (each element of the list is a character)\r\n",
        "print(\"First 10 elements of the corpus: \",text[:10])  # print the first 10 elements of the list\r\n",
        "print(\"Corpus length:\", len(text))  # pritns the number of characters in teh text\r\n",
        "\r\n",
        "# the list of characters is stored in a dictionary where the key is each character\r\n",
        "# chars is a unique sorted list of the list of the characters in the text\r\n",
        "chars = sorted(list(set(text)))  \r\n",
        "print(\"unique charaters: \",chars)\r\n",
        "print(\"Total chars:\", len(chars))\r\n",
        "# create a dictionary with the characters as the keys\r\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
        "print(\"dictionary with characters as the keys: \",char_indices)\r\n",
        "# create a dictionary with the index as the keys\r\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "print(\"dictionary with indexes as the keys: \", indices_char)\r\n",
        "\r\n",
        "# cut the text in semi-redundant 'sentences' of maxlen characters\r\n",
        "maxlen = 40\r\n",
        "step = 3\r\n",
        "sentences = []\r\n",
        "next_chars = []\r\n",
        "for i in range(0, len(text) - maxlen, step):\r\n",
        "    sentences.append(text[i : i + maxlen])\r\n",
        "    next_chars.append(text[i + maxlen])\r\n",
        "print(\"Number of sentences:\", len(sentences))\r\n",
        "print(\"sentence 1: \", sentences[0])\r\n",
        "print(\"sentence 2: \", sentences[1])\r\n",
        "print(\"sentence 3: \", sentences[2])\r\n",
        "# each element of the sentences has been shifted by maxlen\r\n",
        "print(\"next character for sentence 1: \", next_chars[0])\r\n",
        "print(\"next character for sentence 1: \", next_chars[1])\r\n",
        "print(\"next character for sentence 1: \", next_chars[2])\r\n",
        "# each element of the sentences has been shifted by maxlen\r\n",
        "# next_chars contains the next character in each sentence\r\n",
        "\r\n",
        "# specify the character data as booleans so that the network can use it\r\n",
        "# creates 3D shapes for each character in the sentence and sets that co-ordiate to True (x data)\r\n",
        "# creates 2D shapes for the next character in the sentence and sets that co-ordiate to True  (y data)\r\n",
        "\r\n",
        "# create 3D shape of (num sentences, max length of sentence, number of unique chars in corpus)\r\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\r\n",
        "# create 2D shape of (num sentences, number of unique chars in corpus)\r\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\r\n",
        "for i, sentence in enumerate(sentences):    # for each sentence\r\n",
        "    for t, char in enumerate(sentence):     # for each character in each sentence\r\n",
        "        x[i, t, char_indices[char]] = 1     # [index of sentence, index of character in sentence, unique index of the character for this corpus] set to true\r\n",
        "    y[i, char_indices[next_chars[i]]] = 1   # [index of sentence, unique index of next character in the sentence]\r\n",
        "\r\n",
        "print(\"boolean representation of sentence 0: \", x[0])\r\n",
        "print(\"boolean representation of the next character after sentence 0: \", y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX_OuP3TZE1-"
      },
      "source": [
        "# build the model\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import io\r\n",
        "\r\n",
        "model = keras.Sequential()\r\n",
        "model.add(Input(shape=(maxlen, len(chars))))\r\n",
        "model.add(LSTM(128))\r\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\r\n",
        "\r\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bpzYsqvZP6U"
      },
      "source": [
        "def sample(preds):\r\n",
        "    # helper function to sample an index from a probability array\r\n",
        "    preds = np.asarray(preds).astype(\"float64\")\r\n",
        "    preds = np.log(preds)\r\n",
        "    exp_preds = np.exp(preds)\r\n",
        "    preds = exp_preds / np.sum(exp_preds)\r\n",
        "    probas = np.random.multinomial(1, preds, 1)\r\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjvItSVeZYyH"
      },
      "source": [
        "# train\r\n",
        "epochs = 25\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1)\r\n",
        "    print()\r\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\r\n",
        "\r\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\r\n",
        "    generated = \"\"\r\n",
        "    sentence = text[start_index : start_index + maxlen]\r\n",
        "    print('...Generating with seed: \"' + sentence + '\"')\r\n",
        "\r\n",
        "    for i in range(400):\r\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\r\n",
        "        for t, char in enumerate(sentence):\r\n",
        "            x_pred[0, t, char_indices[char]] = 1.0\r\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\r\n",
        "        next_index = sample(preds)\r\n",
        "        next_char = indices_char[next_index]\r\n",
        "        sentence = sentence[1:] + next_char\r\n",
        "        generated += next_char\r\n",
        "\r\n",
        "    print(\"...Generated: \", generated)\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}