{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HorseHumanCnn_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%202%20DL%20Image%20Analysis%201/HorseHumanCnn_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQdkXj0Ins8S"
      },
      "source": [
        "# Classification of Images of Horses and Humans Using a CNN\n",
        "(adapted from https://colab.research.google.com/github/mohnabil2020/machine_learning/blob/master/horse_or_human_classifier.ipynb#scrollTo=AwQbx0_vsDli)\n",
        "\n",
        "##Uses images from a zip file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMcRCp75TzKk"
      },
      "source": [
        "Download the zip files containg the images.  The training data ad the test data are stored in 2 separate zip files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObcznUETmygU"
      },
      "source": [
        "# get the training data zip file from the google API and store it in directory /tmp\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqnJ0DX0UwGt"
      },
      "source": [
        "# get the validation data zip file from the google API and store it in directory /tmp\r\n",
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\r\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDaqvR_nsNmD"
      },
      "source": [
        "# load the libraries\n",
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffsbYFlOWIPa"
      },
      "source": [
        "Extract the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1aRdJWKs9JR"
      },
      "source": [
        "# extract the training data into a directory\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "\n",
        "# extract the validation data into a directory\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Zwnn5ztUhK"
      },
      "source": [
        "# Directory with our training horse pictures for training\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures for training\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures for validation\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures for validation\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OQ-W08lWN15"
      },
      "source": [
        "Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQLv--Mfvnxa"
      },
      "source": [
        "# print out the names of the first 10 files to give a nice feeling that the files are arranged as expected\n",
        "# i.e. images of horses in the horses folder and images of humans in the human folder\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])\n",
        "\n",
        "validation_horse_hames = os.listdir(validation_horse_dir)\n",
        "print(validation_horse_hames[:10])\n",
        "\n",
        "validation_human_names = os.listdir(validation_human_dir)\n",
        "print(validation_human_names[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFQIKsm7tuUL"
      },
      "source": [
        "# print out the number of images in the directories\n",
        "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
        "print('total training human images:', len(os.listdir(train_human_dir)))\n",
        "print('total validation horse images:', len(os.listdir(validation_horse_dir)))\n",
        "print('total validation human images:', len(os.listdir(validation_human_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL21whyat1rG"
      },
      "source": [
        "# display images from the dataset\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "pic_index = 0\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "# pick the first 8 horse pictures\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "# pick the first 8 human pictures\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "# add the horse and human pictures together in a list and plot in\n",
        "# the specified number of rows and columns\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') \n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSE-M4PYcVLM"
      },
      "source": [
        "Data Preparation.  Normalise the images and re-size them to the same size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-Z7RXAA_sp"
      },
      "source": [
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDG2U4jTuibZ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0WlOPLe_-Ti"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBeG-BDaAthT"
      },
      "source": [
        "history=model.fit( train_generator,  \n",
        "                    validation_data = validation_generator,\n",
        "                    epochs=15\n",
        "                  )             \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg0ZGhodFNp4"
      },
      "source": [
        "# summarize the history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize the history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEThIN2YoXvV"
      },
      "source": [
        "## You do not need to do this but for a bit of fun let's upload an image from your computer and have it calssified as a horse or human\n",
        "\n",
        "***hint :*** ( try to use image > 300*300 pixels because this is the minimum pixels for the model ) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzffGLjNFMrQ"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6laaouZqaws"
      },
      "source": [
        "## You do not need to do this either but for a bit more fun, let's visualise what is happening with the image in the hidden layers of the network\n",
        "\n",
        "**You do not need to understand the code below.**\n",
        "\n",
        "A visualisation model is created from our original model that makes the hidden layers of our model avaialble.\n",
        "It takes a random image from the training set, resizes it, normalizes it.  Applies it to the model using the `predict` method.\n",
        "\n",
        "Loops round the hidden layers of the visualisation model and displays the activations as images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FQ6BP_PnWQG"
      },
      "source": [
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Now let's display our representations\n",
        "# loops round all of the internal hidden layers\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLWNkyz4lh-9"
      },
      "source": [
        "## Reduce overfitting on the model"
      ]
    }
  ]
}