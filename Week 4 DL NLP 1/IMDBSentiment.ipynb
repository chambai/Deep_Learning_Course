{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDBSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMovaOvusBbcvuB8IUtnQyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%201/IMDBSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsIs52Q9C2B"
      },
      "source": [
        "# Sentiment Analysis on IMDB Movie Reviews\r\n",
        "\r\n",
        "Adapted from https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B32Q4oOWXoP_"
      },
      "source": [
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\r\n",
        "\r\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper [PDF] where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\r\n",
        "\r\n",
        "The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLonDcDLXzXM"
      },
      "source": [
        "The IMDB dataset is avaialble in Keras and is ready for use in deep neural networks.  The words have been replace by integers that indicate the popularity of the word in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no731YgMYlSL"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxEZJh9iXq79"
      },
      "source": [
        "# load the dataset and explore the data\r\n",
        "import numpy\r\n",
        "from keras.datasets import imdb\r\n",
        "from matplotlib import pyplot\r\n",
        "# load the dataset\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\r\n",
        "X = numpy.concatenate((X_train, X_test), axis=0)\r\n",
        "y = numpy.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KoLW8lZMf3"
      },
      "source": [
        "## Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poa5n_N-Y2X1"
      },
      "source": [
        "# display the shape of the dataset\r\n",
        "print(\"Training data: \")\r\n",
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oYTWydwY9Bm"
      },
      "source": [
        "# Find number of classes\r\n",
        "print(\"Classes: \")\r\n",
        "print(numpy.unique(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyNjyHEbZV3h"
      },
      "source": [
        "# get the number of words\r\n",
        "print(\"Number of words: \", len(numpy.unique(numpy.hstack(X))))   # numpy.hstack Stack arrays in sequence horizontally (column wise)\r\n",
        "print(\"Review 1: \", X[0])\r\n",
        "print(\"Review 2: \", X[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9bx1y8ZhzI"
      },
      "source": [
        "# get the average review length\r\n",
        "print(\"Review length: \")\r\n",
        "result = [len(x) for x in X]\r\n",
        "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\r\n",
        "# plot review length\r\n",
        "pyplot.boxplot(result)\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI2D_zz5ZtKH"
      },
      "source": [
        "The average review length is just under 300 and the standard deviation (dispersion of the dataset relative to its mean) is just over 200 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThv3rc5YjhI"
      },
      "source": [
        "Limit the dataset for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6eH4DvRwd6M"
      },
      "source": [
        "# limit the vocabulary to the 5000 most used words\r\n",
        "imdb.load_data(num_words=5000)\r\n",
        "\r\n",
        "# we don't need to change the text into numeric sequences as the data \r\n",
        "# is already stored as numbers in the Keras IMDB dataset\r\n",
        "# truncate or pad each review to 500 words each and form our sequences\r\n",
        "from keras.preprocessing import sequence\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=500)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=500)\r\n",
        "print(X_train)\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7K080Dr2pV1"
      },
      "source": [
        "# Simple Multi Layer Perceptron for Sentiment Classification\r\n",
        "##**Exercise:** Implement a feed forward (MLP) network and experiment with how the following affects the accuracy:\r\n",
        "\r\n",
        "1.   Number of words in the vocabulary \r\n",
        "2.   Output dimension of the embedding layer\r\n",
        "\r\n",
        "With a one-layer MLP, about 87% accuracy can be acheived (precise accuracy will vary due to stochastic nature of the model)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsT3Ldfv2xxP"
      },
      "source": [
        "# MLP for the IMDB problem\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# decide on the maximum amount of words for a review\r\n",
        "max_words = 500\r\n",
        "# create sequences of the words\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# decide the output dimension of the embedding layer (an arbitary number that can be tuned)\r\n",
        "output_dim = 32\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBUL1AP0dEuJ"
      },
      "source": [
        "The more words used in the vocabulary, the higher the accuracy that can be obtained.\r\n",
        "\r\n",
        "In this particular model, the output dimension of the embedding layer does not have much of an impact. It depends on the particular problem as to whether this has an effect.\r\n",
        "\r\n",
        "Adding 2 more hidden layers with the same number of units reduces the accuracy by approx 2%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh8T8dCF53ho"
      },
      "source": [
        "# One Dimensional Convolutional Neural Network\r\n",
        "\r\n",
        "##**Exercise:**  Implement a 1D Convolutional neural network\r\n",
        "\r\n",
        "Does it acheive a better accuracy than the MLP model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yqQb-Fw6F08"
      },
      "source": [
        "# CNN for the IMDB problem\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQXX9cSrBtOA"
      },
      "source": [
        "##**Exercise:** Create a network with one RNN layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of memory cells in the layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH8vGrBJB0XM"
      },
      "source": [
        "# CNN for the IMDB problem\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, SimpleRNN, Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5mmYoYVN0Mu"
      },
      "source": [
        "##**Exercise:** Create a network with multiple RNN layers to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of layers and number of memory cells in the layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O02-86XTNzhH"
      },
      "source": [
        "# CNN for the IMDB problem\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, SimpleRNN, Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQfeYOLBCie3"
      },
      "source": [
        "##**Exercise:** Create a network with one GRU layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQVvjGY4CjGh"
      },
      "source": [
        "# CNN for the IMDB problem\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, GRU, Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bst-wp8OHuN"
      },
      "source": [
        "##**Exercise:** Create a network with multiple GRU layers to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of layers and number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfN9-wUPOMFz"
      },
      "source": [
        "# CNN for the IMDB problem\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, GRU, Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12NK54rhi9NO"
      },
      "source": [
        "##**Exercise:** Create a network with one LSTM layer to perform sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70EUjRe0jOHA"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2niLHAyCvq2p"
      },
      "source": [
        "##**Exercise:** Create a network with more than one LSTM layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of LSTM layers and the number of memory cells in a layer to see if you can improve performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqNzqRuLv28H"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}