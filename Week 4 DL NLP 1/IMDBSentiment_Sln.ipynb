{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDBSentiment_Sln.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzeFk4jwGI3rU9WOqMe42B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chambai/Deep_Learning_Course/blob/main/Week%204%20DL%20NLP%201/IMDBSentiment_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsIs52Q9C2B"
      },
      "source": [
        "# Sentiment Analysis on IMDB Movie Reviews\r\n",
        "\r\n",
        "Adapted from https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B32Q4oOWXoP_"
      },
      "source": [
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\r\n",
        "\r\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper [PDF] where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\r\n",
        "\r\n",
        "The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLonDcDLXzXM"
      },
      "source": [
        "The IMDB dataset is avaialble in Keras and is ready for use in deep neural networks.  The words have been replace by integers that indicate the popularity of the word in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no731YgMYlSL"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxEZJh9iXq79",
        "outputId": "d83ff8ba-b947-4ad4-e0dd-42f5d266d6cb"
      },
      "source": [
        "# load the dataset and explore the data\r\n",
        "import numpy\r\n",
        "from keras.datasets import imdb\r\n",
        "from matplotlib import pyplot\r\n",
        "# load the dataset\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\r\n",
        "X = numpy.concatenate((X_train, X_test), axis=0)\r\n",
        "y = numpy.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KoLW8lZMf3"
      },
      "source": [
        "## Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poa5n_N-Y2X1",
        "outputId": "360de254-da1a-4094-803f-0199a3822b57"
      },
      "source": [
        "# display the shape of the dataset\r\n",
        "print(\"Training data: \")\r\n",
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data: \n",
            "(50000,)\n",
            "(50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oYTWydwY9Bm",
        "outputId": "d12f0aa8-f768-4d23-8fc9-fe3aaf1fc005"
      },
      "source": [
        "# Find number of classes\r\n",
        "print(\"Classes: \")\r\n",
        "print(numpy.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes: \n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyNjyHEbZV3h",
        "outputId": "a9e488e6-8982-42cd-a20d-114cb36a53ea"
      },
      "source": [
        "# get the number of words\r\n",
        "print(\"Number of words: \", len(numpy.unique(numpy.hstack(X))))   # numpy.hstack Stack arrays in sequence horizontally (column wise)\r\n",
        "print(\"Review 1: \", X[0])\r\n",
        "print(\"Review 2: \", X[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words:  88585\n",
            "Review 1:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Review 2:  [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "cc9bx1y8ZhzI",
        "outputId": "22a65aa4-52f3-43fa-c31a-2fb25f1db386"
      },
      "source": [
        "# get the average review length\r\n",
        "print(\"Review length: \")\r\n",
        "result = [len(x) for x in X]\r\n",
        "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\r\n",
        "# plot review length\r\n",
        "pyplot.boxplot(result)\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review length: \n",
            "Mean 234.76 words (172.911495)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI2D_zz5ZtKH"
      },
      "source": [
        "The average review length is just under 300 and the standard deviation (dispersion of the dataset relative to its mean) is just over 200 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThv3rc5YjhI"
      },
      "source": [
        "Limit the dataset for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6eH4DvRwd6M",
        "outputId": "a535bdf2-b853-46f2-c0cb-10737a67fabb"
      },
      "source": [
        "# limit the vocabulary to the 5000 most used words\r\n",
        "imdb.load_data(num_words=5000)\r\n",
        "\r\n",
        "# we don't need to change the text into numeric sequences as the data \r\n",
        "# is already stored as numbers in the Keras IMDB dataset\r\n",
        "# truncate or pad each review to 500 words each and form our sequences\r\n",
        "from keras.preprocessing import sequence\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=500)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=500)\r\n",
        "print(X_train)\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0 ...    19   178    32]\n",
            " [    0     0     0 ...    16   145    95]\n",
            " [    0     0     0 ...     7   129   113]\n",
            " ...\n",
            " [    0     0     0 ...     4  3586 22459]\n",
            " [    0     0     0 ...    12     9    23]\n",
            " [    0     0     0 ...   204   131     9]]\n",
            "(25000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7K080Dr2pV1"
      },
      "source": [
        "# Simple Multi Layer Perceptron for Sentiment Classification\r\n",
        "##**Exercise:** Implement a feed forward (MLP) network and experiment with how the following affects the accuracy:\r\n",
        "\r\n",
        "1.   Number of words in the vocabulary \r\n",
        "2.   Output dimension of the embedding layer\r\n",
        "\r\n",
        "With a one-layer MLP, about 87% accuracy can be acheived (precise accuracy will vary due to stochastic nature of the model)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsT3Ldfv2xxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da566933-75b6-4d1d-c114-52d7d4fd765d"
      },
      "source": [
        "# MLP for the IMDB problem\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# decide on the maximum amount of words for a review\r\n",
        "max_words = 500\r\n",
        "# create sequences of the words\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# decide the output dimension of the embedding layer (an arbitary number that can be tuned)\r\n",
        "output_dim = 32\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, output_dim, input_length=max_words))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 500, 128)          6400000   \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 250)               16000250  \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 250)               62750     \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 22,463,251\n",
            "Trainable params: 22,463,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "196/196 - 17s - loss: 0.4735 - accuracy: 0.7383 - val_loss: 0.2971 - val_accuracy: 0.8768\n",
            "Epoch 2/2\n",
            "196/196 - 16s - loss: 0.0945 - accuracy: 0.9676 - val_loss: 0.3965 - val_accuracy: 0.8516\n",
            "Accuracy: 85.16%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBUL1AP0dEuJ"
      },
      "source": [
        "The more words used in the vocabulary, the higher the accuracy that can be obtained.\r\n",
        "\r\n",
        "In this particular model, the output dimension of the embedding layer does not have much of an impact. It depends on the particular problem as to whether this has an effect.\r\n",
        "\r\n",
        "Adding 2 more hidden layers with the same number of units reduces the accuracy by approx 2%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh8T8dCF53ho"
      },
      "source": [
        "# One Dimensional Convolutional Neural Network\r\n",
        "\r\n",
        "##**Exercise:**  Implement a 1D Convolutional neural network\r\n",
        "\r\n",
        "Does it acheive a better accuracy than the MLP model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yqQb-Fw6F08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11624a73-0b42-489d-d900-7b418c9a8ba4"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(Conv1D(32, 3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_35 (Embedding)     (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 8000)              0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 250)               2000250   \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,163,605\n",
            "Trainable params: 2,163,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "196/196 - 3s - loss: 0.4281 - accuracy: 0.7778 - val_loss: 0.2709 - val_accuracy: 0.8868\n",
            "Epoch 2/2\n",
            "196/196 - 3s - loss: 0.2103 - accuracy: 0.9173 - val_loss: 0.2759 - val_accuracy: 0.8858\n",
            "Accuracy: 88.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYeDZgKl7SUY"
      },
      "source": [
        "In the summary of the network structure, you can see that the convolutional layer preserves the dimenionality of the embedding input layer of 32 dimensional input with a max of 500 words.  The pooling layer compresses this representation by halving it.\r\n",
        "\r\n",
        "A small improvement over the MLP network is acheived"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siDNkgXJDVdd"
      },
      "source": [
        "##**Exercise:** Create a network with one RNN layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXTBKSI7DWDT",
        "outputId": "aa2847fc-d907-4d7a-f050-075c703dff89"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(SimpleRNN(64))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 80s - loss: 0.6204 - accuracy: 0.6280 - val_loss: 0.4048 - val_accuracy: 0.8220\n",
            "Epoch 2/2\n",
            "196/196 - 79s - loss: 0.3573 - accuracy: 0.8493 - val_loss: 0.4426 - val_accuracy: 0.7956\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)     (None, 64)                6208      \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 250)               16250     \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 182,709\n",
            "Trainable params: 182,709\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 79.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHbAbx8cOaJk"
      },
      "source": [
        "##**Exercise:** Create a network with mutliple RNN layers to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of layers and number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXlonDIiOfrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791f5493-7d2e-4d3d-ee49-5fa2df1a8937"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(SimpleRNN(64,return_sequences=True))\r\n",
        "model.add(SimpleRNN(64,return_sequences=True))\r\n",
        "model.add(SimpleRNN(64))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 218s - loss: 0.7031 - accuracy: 0.5051 - val_loss: 0.7566 - val_accuracy: 0.4992\n",
            "Epoch 2/2\n",
            "196/196 - 212s - loss: 0.7062 - accuracy: 0.4984 - val_loss: 0.6968 - val_accuracy: 0.5000\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 500, 64)           6208      \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 500, 64)           8256      \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               16250     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 199,221\n",
            "Trainable params: 199,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 50.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxBDa2gGBwy"
      },
      "source": [
        "Takes longer to train than the Conv1D network. Lower accuracy than the Conv1D.  Has probably 'forgotten' some of the words at the begining of the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNBBv2ZvDWfm"
      },
      "source": [
        "##**Exercise:** Create a network with one GRU layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gzmB_V6DW9s",
        "outputId": "8a9c0dff-4928-4243-9032-2ddf787067ea"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Embedding, GRU\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(GRU(64))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 8s - loss: 0.5352 - accuracy: 0.7122 - val_loss: 0.3804 - val_accuracy: 0.8268\n",
            "Epoch 2/2\n",
            "196/196 - 6s - loss: 0.3017 - accuracy: 0.8729 - val_loss: 0.3219 - val_accuracy: 0.8643\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_38 (Embedding)     (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 64)                18816     \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 250)               16250     \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 195,317\n",
            "Trainable params: 195,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 86.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XnfPSh8PE5D"
      },
      "source": [
        "##**Exercise:** Create a network with multiple GRU layers to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of layers and number of memory cells in a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nE67IGoPK_q"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Embedding, GRU\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "# add your model here\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(GRU(64,return_sequences=True))\r\n",
        "model.add(GRU(64,return_sequences=True))\r\n",
        "model.add(GRU(64))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT3anZ5eGdz_"
      },
      "source": [
        "Faster to train than the RNN. Higher accuracy than the RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12NK54rhi9NO"
      },
      "source": [
        "##**Exercise:** Create a network with one LSTM layer to perform sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70EUjRe0jOHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473e8219-3b6c-4457-9591-acb4ef1eb522"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Embedding, LSTM\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
        "model.add(LSTM(64))\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_39 (Embedding)     (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 201,729\n",
            "Trainable params: 201,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "196/196 - 10s - loss: 0.4618 - accuracy: 0.7725 - val_loss: 0.3375 - val_accuracy: 0.8556\n",
            "Epoch 2/2\n",
            "196/196 - 8s - loss: 0.2671 - accuracy: 0.8927 - val_loss: 0.2881 - val_accuracy: 0.8812\n",
            "Accuracy: 88.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjmi4Mut56ZW"
      },
      "source": [
        "Acheived about the same as the 1D convolutional network in this example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2niLHAyCvq2p"
      },
      "source": [
        "##**Exercise:** Create a network with more than one LSTM layer to perform sentiment analysis.\r\n",
        "\r\n",
        "Experiment with the number of LSTM layers and the number of memory cells in a layer to see if you can improve performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqNzqRuLv28H",
        "outputId": "bc4e3fd5-a67b-4b79-f6fc-9136e7bc2b86"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "# pad dataset to a maximum review length in words\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n",
        "\r\n",
        "# create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words)) # learns a representation of the words into a fixed size of 32\r\n",
        "model.add(LSTM(16, return_sequences=True))\r\n",
        "model.add(LSTM(16, return_sequences=True))\r\n",
        "model.add(LSTM(16, return_sequences=True))\r\n",
        "model.add(LSTM(16))\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
        "\r\n",
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_32 (Embedding)     (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 500, 16)           3136      \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 500, 16)           2112      \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 500, 16)           2112      \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 256)               4352      \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 174,081\n",
            "Trainable params: 174,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "196/196 - 23s - loss: 0.4558 - accuracy: 0.7702 - val_loss: 0.3317 - val_accuracy: 0.8610\n",
            "Epoch 2/2\n",
            "196/196 - 17s - loss: 0.2645 - accuracy: 0.8953 - val_loss: 0.3112 - val_accuracy: 0.8724\n",
            "Accuracy: 87.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vO1idXf7N40"
      },
      "source": [
        "This network configuration acheives about the same as the single layer LSTM above, but has not been tuned, so there could be a better option!"
      ]
    }
  ]
}